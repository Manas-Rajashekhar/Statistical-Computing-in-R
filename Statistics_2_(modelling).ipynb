{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Statistics - 2 (modelling).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltiJ4WB2jjY0"
      },
      "source": [
        "### Learning Outcomes of Modelling for Data Analysis\n",
        "\n",
        "1. perform exploratory data analysis with descriptive statistics on given datasets;\n",
        "\n",
        "2. construct models for inferential statistical analysis;\n",
        "\n",
        "3. produce models for predictive statistical analysis\n",
        "\n",
        "4. perform fundamental random sampling, simulation and hypothesis testing for required scenarios;\n",
        "\n",
        "5. implement a model for data analysis through programming and scripting;\n",
        "\n",
        "6. interpret results for a variety of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tdq_UZtjjY_"
      },
      "source": [
        "<h2>Question 1 - Confidence Intervals and Hypothesis Testing (20 marks)</h2>\n",
        "\n",
        "Previously, we worked with a company who was making phone cases using a new 3D printing method. Because 3D printing is not a perfect process, management wanted to know how many samples they should produce to get a good approximation for the failure probability of the process.\n",
        "\n",
        "This was several weeks ago, and the engineers have now begun testing the new 3D printing process. We are no longer concerned with whether a print is a \"success\" or \"failure\" - we are going to be more precise now. Although they will do more tests later, the engineers have performed an initial test run of 20 successful units (that is, they keep producing units until they have 20 \"successful\" prints).\n",
        "\n",
        "For this question, we are going to assume that the 3D print thickness is normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "logXL1aljjZA"
      },
      "source": [
        "test.20.units <- read.csv(\"test20units.csv\")$x\n",
        "head(test.20.units)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "666df79a274a234ef49dd128d7147963",
          "grade": false,
          "grade_id": "cell-a0ce5b3aae506e53",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "HT16EgGTkBL7",
        "outputId": "01477025-8568-4b13-e502-af81d3059864"
      },
      "source": [
        "test.20.units <- read.csv(\"test20units.csv\")$x\n",
        "head(test.20.units)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<ol class=list-inline>\n",
              "\t<li>1.15709584471467</li>\n",
              "\t<li>0.963530182860391</li>\n",
              "\t<li>1.05631284113373</li>\n",
              "\t<li>1.0832862604961</li>\n",
              "\t<li>1.0604268323141</li>\n",
              "\t<li>1.00938754839085</li>\n",
              "</ol>\n"
            ],
            "text/latex": "\\begin{enumerate*}\n\\item 1.15709584471467\n\\item 0.963530182860391\n\\item 1.05631284113373\n\\item 1.0832862604961\n\\item 1.0604268323141\n\\item 1.00938754839085\n\\end{enumerate*}\n",
            "text/markdown": "1. 1.15709584471467\n2. 0.963530182860391\n3. 1.05631284113373\n4. 1.0832862604961\n5. 1.0604268323141\n6. 1.00938754839085\n\n\n",
            "text/plain": [
              "[1] 1.1570958 0.9635302 1.0563128 1.0832863 1.0604268 1.0093875"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b78647aecc5cc1bea3b4d50b6ee51234",
          "grade": false,
          "grade_id": "cell-fbda8f3df9a1b68b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Z8U006ddj11Y"
      },
      "source": [
        "**bold text**<h2>Question 1.a (2 marks)</h2>\n",
        "We have decided that, for the 3D printing process to be considered a success, we are going to require the following facts to be true (with a confidence level $\\alpha = 0.01$):\n",
        "\n",
        " - The mean print thickness must be 1mm\n",
        " - The confidence interval for the mean must be $\\leq$ 0.1mm wide\n",
        " \n",
        "We will test these facts by determining whether the corresponding values are within the 99% confidence intervals for our estimates, and also whether the wideness of our confidence intervals fits. That is, we will calculate a two-sided 99% confidence interval for the mean of the print thickness and see if 1mm is within that range, and whether that range has a total width $\\leq$ 0.1mm wide.\n",
        "\n",
        "Using R code, write a function conf.interval.mean.n, which takes two arguments (data and alpha) and returns a list containing two values:\n",
        "\n",
        " - list\\$lower: the <b>lower</b> end of the two-sided confidence interval of the mean of $data$ with confidence level $alpha$\n",
        " \n",
        " - list\\$upper: the <b>upper</b> end of the two-sided confidence interval of the mean of $data$ with confidence level $alpha$\n",
        " \n",
        "<b>For this question, since we both have a small sample size and also don't know the standard deviation of the population (we are using the estimate from the sample)</b>\n",
        "    \n",
        "The function should look something like this:\n",
        "\n",
        "    conf.interval.mean.t <- function(data, alpha) {\n",
        "        # Your code here\n",
        "    }\n",
        "    \n",
        "<b>Note: we are going to use hypothesis testing, rather than this confidence-interval based approach, later in this question. Hypothesis testing is a better way to determine whether the 3D printing process meets the requirements or not</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "048207952c5faf6f8eb2b0a250f4732a",
          "grade": true,
          "grade_id": "cell-6fc55ddefc67def4",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "4HKPNqCvj11Z"
      },
      "source": [
        "conf.interval.mean.t <- function(data, alpha)\n",
        "{\n",
        "    avrg <- mean(data)\n",
        "    Std_dv <- sd(data)\n",
        "    n <- length(data)\n",
        "    std_err <- Std_dv/sqrt(n)\n",
        "    t_stat=qt(alpha/2, n-1, lower.tail=FALSE)*std_err\n",
        "    conf_intv_lower = avrg - t_stat\n",
        "    conf_intv_upper = avrg + t_stat\n",
        "    return(list(conf_intv_lower,conf_intv_upper))\n",
        "    \n",
        "    \n",
        "\n",
        "} \n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sIyx9efj11a"
      },
      "source": [
        "t_stat=qt(0.001/2, 20-1, lower.tail=FALSE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c3adef937993f88bd32beb63e6b055b3",
          "grade": false,
          "grade_id": "cell-c7b43bf6a6473980",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "MPw2NqBAj11a",
        "outputId": "bcca2197-cb46-4834-bed5-4ac996095b1e"
      },
      "source": [
        "ci.mean.20 <- conf.interval.mean.t(test.20.units, 0.01)\n",
        "ci.mean.20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li>0.955219955546044</li>\n",
              "\t<li>1.1231640483385</li>\n",
              "</ol>\n"
            ],
            "text/latex": "\\begin{enumerate}\n\\item 0.955219955546044\n\\item 1.1231640483385\n\\end{enumerate}\n",
            "text/markdown": "1. 0.955219955546044\n2. 1.1231640483385\n\n\n",
            "text/plain": [
              "[[1]]\n",
              "[1] 0.95522\n",
              "\n",
              "[[2]]\n",
              "[1] 1.123164\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "04799f9be68e7183d50f1779f5495347",
          "grade": false,
          "grade_id": "cell-6f6cb83843ff1040",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "hv4S1KnOj11b"
      },
      "source": [
        "<h2>Question 1.b  (2 marks)</h2>\n",
        "Are the two criteria established in Question 1.a met? That is, is the desired mean of 1mm thickness within the confidence interval, and is the confidence interval less than 0.1mm wide from the lower to upper bound?\n",
        "\n",
        "Using this question as an example, why is it important to consider both whether the value is within the confidence interval <b>and</b> how wide the confidence interval is (relative to how wide an error we are allowed)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHzWbn2Mj11b",
        "outputId": "e887cfc5-5232-4e45-861e-bb5334ac231b"
      },
      "source": [
        "a=1.1231640483385-0.955219955546044\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.167944092792456"
            ],
            "text/latex": "0.167944092792456",
            "text/markdown": "0.167944092792456",
            "text/plain": [
              "[1] 0.1679441"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "12b2c3024a5a9b76c71d34f96316d680",
          "grade": true,
          "grade_id": "cell-c15c8d94d9b5176f",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "lBZJ9DH3j11c"
      },
      "source": [
        "Since the confidence inteerwal width = 0.167 which is > 0.1 . However the mean lies within the confidence intervale \n",
        "Therefore criteria 1 is met whereas criteria 2 is not met"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "52a12bf2fcc1a128fa540116f0dda995",
          "grade": false,
          "grade_id": "cell-cd75560a3b20eff3",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "8kufQxfCj11c"
      },
      "source": [
        "<h2>Question 1.c (6 marks)</h2>\n",
        "The engineers have developed two alternative methods for 3D printing the phone cases. The engineers are now trying to minimize \"warping\" (which is where phone units are not printed flat and are instead warped slightly - this warping is measured in millimeters).\n",
        "\n",
        "We have two datasets (one for each of the two new printing methods), where each dataset contains warping measurements from 50 prints produced using that dataset's 3D printing method. From our many tests with the original method, we know that the mean warping measurement for the original printing method is very close to 0.1mm (so we will use that as our estimate for the mean of the original population).\n",
        "\n",
        "The engineers have asked us to determine whether either of their new 3D printing methods are superior to the original method. To do this, we could just calculate the mean of each dataset's measurements and compare that to the 0.1mm mean of the original method, but this does not take into account the variance introduced by sampling from the population. Instead, we will use hypothesis testing.\n",
        "\n",
        "For each of the two new printing methods, each with datasets method1.50.units and method2.50.units respectively, we need to test the hypothesis that the mean of that method's dataset is <b>less</b> than the mean of the original population (i.e., less than 0.1mm). So, for each of the two printing methods, do the following (there are four cells below - the first cell imports the data, and the next three cells correspond to the following three points):\n",
        "\n",
        "- Create a hypothesis test (defining the null hypothesis and alternative hypothesis). Note that this should be a <b>one-sided</b> test.\n",
        "- Calculate a p-value using a t.test\n",
        "- Interpret this p-value, using a confidence level $\\alpha = 0.01$\n",
        "\n",
        "When interpreting the p-value, keep in mind that the goal is to determine whether each of the new methods is better than the original method (in terms of warping).\n",
        "\n",
        "<b>Do not use the inbuilt t.test function in R to answer this question. Use the formulas in the lecture notes to calculate the values from scratch. You may write your own version of a t-test function if you like, but you don't have to do this.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f1437a67df56dd38894a106da9fff237",
          "grade": false,
          "grade_id": "cell-4c01ad9eb8d74efd",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "zXmlOYXjj11d"
      },
      "source": [
        "method1.50.units <- read.csv(\"method1_50units.csv\")$x\n",
        "method2.50.units <- read.csv(\"method2_50units.csv\")$x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI3NorOEj11e",
        "outputId": "d8f895e8-f6c8-4304-b958-4b758542f8c1"
      },
      "source": [
        "sd(method1.50.units)           # calculate standar deviation of data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.0317329248067579"
            ],
            "text/latex": "0.0317329248067579",
            "text/markdown": "0.0317329248067579",
            "text/plain": [
              "[1] 0.03173292"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLNAbELOj11f",
        "outputId": "c8e11533-3912-4f13-a70b-93b20c3e639c"
      },
      "source": [
        "sd(method2.50.units)        # calculate standar deviation of data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.0445994135763026"
            ],
            "text/latex": "0.0445994135763026",
            "text/markdown": "0.0445994135763026",
            "text/plain": [
              "[1] 0.04459941"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "a30ea8a2ee74606857d48ba8dd3794ad",
          "grade": true,
          "grade_id": "cell-f75742ba68d8e9d4",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "Ea61yUxZj11f"
      },
      "source": [
        "Null Hypothesis         H0 :$\\mu$(method1.50.units) >= 0.1\n",
        "\n",
        "Alternate Hypothesis    H1 :$\\mu$( (method1.50.units) < 0.1                \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "12daf3ac848c3973b08abe9900a37079",
          "grade": true,
          "grade_id": "cell-49e036e0fa3ef481",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "8cbarqD2j11g",
        "outputId": "42cc618d-45d4-41ca-c831-90ce11896dbc"
      },
      "source": [
        "t_test_fn<-function(df){\n",
        "n<-length(df)                            # calculate length of dataframe\n",
        "mu<-0.1                                     # set population mean = 0.1    \n",
        "t<- (mean(df) - mu)/(sd(df)/sqrt(n))                        # calculate t - statistic\n",
        "prob<-pt(t,n-1)                       # get probability  value from pt() function\n",
        " print(list(t,prob))                \n",
        " return(prob)    \n",
        "}\n",
        "hypo_tst_1<-t_test_fn(method1.50.units)\n",
        "hypo_tst_2<-t_test_fn(method2.50.units)\n",
        "                                          # your code here\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]]\n",
            "[1] -2.748737\n",
            "\n",
            "[[2]]\n",
            "[1] 0.004176281\n",
            "\n",
            "[[1]]\n",
            "[1] 0.4665702\n",
            "\n",
            "[[2]]\n",
            "[1] 0.6785624\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "0ed94f278cbb35b2cf8bdce7be179a75",
          "grade": true,
          "grade_id": "cell-ef8fa9b7a02ecbc1",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "mSMfcbZXj11g"
      },
      "source": [
        "Based on p values ,\n",
        "pval for method_1 is 0.004176 which is < 0.01 , Hence reject null hypothesis\n",
        "pval for method_2 is 0.6785 which is > 0.01 , Hence we do not reject null hypothesis\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b0f80ff1083b18ab423ffba621a73fae",
          "grade": false,
          "grade_id": "cell-e867dc64589601dd",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Qx1XSlPhj11h"
      },
      "source": [
        "<h2>Question 1.d (3 marks)</h2>\n",
        "When we use $\\alpha = 0.01$ for a single hypothesis test, we expect to find a \"false positive\" (that is, we reject the null hypothesis when we should not have) in around 1 in 100 experiments.\n",
        "\n",
        "However, in Question 1.c, we did two hypothesis tests (one for each of our two methods). We will consider the implications of this below.\n",
        "\n",
        "Please do the following:\n",
        "- Assuming that the probability of finding a false positive is 1% for each experiment, calculate the probability of finding at least 1 false positive overall (for two hypothesis tests).\n",
        "- Calculate the probability of finding at least 1 false positive overall, if we tested 100 methods instead of just two.\n",
        "- Briefly discuss something you think data scientists could do to reduce the risk of false positives when performing multiple hypothesis tests at once. Justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "c5b7e1c3e0f818664e9ced19a490c6b7",
          "grade": true,
          "grade_id": "cell-50783078a14a7076",
          "locked": false,
          "points": 3,
          "schema_version": 1,
          "solution": true
        },
        "id": "7kvywnfGj11h"
      },
      "source": [
        "Probability of atleast  1 error in m tests = $ 1-(1-\\alpha)^m$ \n",
        "where   m=2 and $\\alpha = 0.01$  \n",
        "Therefore,\n",
        "$ 1-(1- 0.01)^2 = 0.0199$ \n",
        "\n",
        "For 100 methods we have\n",
        "where   m=100 and $\\alpha = 0.01$  \n",
        "Therefore,\n",
        "$ 1-(1- 0.01)^100 = 0.6339$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f218b264eb57131ffc79e53a4c367fb8",
          "grade": false,
          "grade_id": "cell-f3a2631f20680559",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "w3yjnWOoj11i"
      },
      "source": [
        "<h2>Question 1.e (5 marks)</h2>\n",
        "The engineers have decided, for other reasons than warping, to use one of the two new 3D printing methods instead of the old one. They would now like us to determine which of these two new printing methods is superior.\n",
        "\n",
        "So, we need to test two hypotheses:\n",
        "\n",
        "- That method 1 is superior to method 2 (i.e. method1 has a <b>lower</b> mean warping than method2)\n",
        "- That method 2 is superior to method 1 (i.e. method2 has a <b>lower</b> mean warping than method1)\n",
        "\n",
        "For each of methods 1 and 2, please do the following (there are three cells below, corresponding to the the following three points):\n",
        "\n",
        "- Create a hypothesis test (defining the null and alternative hypotheses) which tests whether the chosen method is superior to the alternative method (i.e. either method 1 is superior to method 2, or method 2 is superior to method 1). Note that this should be a <b>one-sided</b> test.\n",
        "- Calculate a p-value\n",
        "- Interpret the p-value, using a confidence level $\\alpha = 0.01$\n",
        "\n",
        "<b>Do not use the inbuilt t.test function in R to answer this question. Use the formulas in the lecture notes to calculate the values from scratch. You may write your own version of a t-test function if you like, but you don't have to do this.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "97536f27b6bea689317e54910a7f6291",
          "grade": true,
          "grade_id": "cell-a3ad4f30d8ff49dc",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "dDTBCCQvj11j"
      },
      "source": [
        "$Hypothesis 1$\n",
        "\n",
        "H0 : $\\mu$(method1.50.units) > $\\mu$(method2.50.units)\n",
        "\n",
        "H1 : $\\mu$(method1.50.units) <= $\\mu$(method2.50.units)\n",
        "\n",
        "\n",
        "$Hypothesis 2$\n",
        "\n",
        "H0 : $\\mu$(method1.50.units) < $\\mu$(method2.50.units)\n",
        "\n",
        "H1 : $\\mu$(method1.50.units) >= $\\mu$(method2.50.units)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "5193c2ac0ebbb99113838f678666faf2",
          "grade": true,
          "grade_id": "cell-87c00949fd857a56",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "rfZeEjSqj11j",
        "outputId": "fcaa7bd1-4415-46e3-cb70-8d952a5f6a4d"
      },
      "source": [
        "# for method_!\n",
        "mu_method_1<-mean(method1.50.units)\n",
        "mu_method_2<-mean(method2.50.units)                # find all parametres using function\n",
        "sd_method_1<-sd(method1.50.units)\n",
        "sd_method_2<-sd(method2.50.units)\n",
        "l1<-length(method1.50.units)\n",
        "l2<-length(method1.50.units)\n",
        "t_test_1<-(mu_method_1 - mu_method_2)/sqrt((sd_method_1^2/l1) + (sd_method_2^2/l2))      # consider difference of means \n",
        "prob_1<- pt(t_test_1 , l1-1)            \n",
        "\n",
        "\n",
        "#for method_2\n",
        "\n",
        "t_test_2<-(mu_method_2 - mu_method_1)/sqrt((sd_method_1^2/l1) + (sd_method_2^2/l2))\n",
        "prob_2<- pt(t_test_2 , l2-1)\n",
        "\n",
        "prob_1\n",
        "prob_2                                           # your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.0270334042248327"
            ],
            "text/latex": "0.0270334042248327",
            "text/markdown": "0.0270334042248327",
            "text/plain": [
              "[1] 0.0270334"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.972966595775167"
            ],
            "text/latex": "0.972966595775167",
            "text/markdown": "0.972966595775167",
            "text/plain": [
              "[1] 0.9729666"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "d25ace974f05f84ac2a67c492c8be93d",
          "grade": true,
          "grade_id": "cell-f219acad3b917ddd",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "w_2ZP3RGj11k"
      },
      "source": [
        "for method 1 = 0.0270334042248327 for $\\alpha = 0.01$ since 0.027 > 0.01 , we do not reject null hypothesis\n",
        "\n",
        "for method 2 = 0.972966595775167  for $\\alpha = 0.01$ since 0.97 > 0.01 , we do not reject null hypothesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4a71d31afe4ce3a807e19fa1ea7d557f",
          "grade": false,
          "grade_id": "cell-c1e32afbaf648e01",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "dMCP89lNj11k"
      },
      "source": [
        "<h2>Question 1.f (2 marks)</h2>\n",
        "If you completed Question 1.e correctly, you should have found that the two p-values calculated (one for each of the two methods) sum to 1. Or, put another way, each p-value is $1 - $ the other p-value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "dd79484bbf4ed79e6cc5e07efc1554e4",
          "grade": false,
          "grade_id": "cell-01827fa9a27bbb03",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "QqbBHQMEj11l"
      },
      "source": [
        "<h2>Question 2 - Logistic Regression (19 marks)</h2>\n",
        "\n",
        "The engineers are now doing the first production run, which is going to take a while. One of your colleagues has challenged you to a game of chess while you wait for the test run to complete.\n",
        "\n",
        "The game is being played by correspondance (meaning that you aren't playing over a board, and can do other things while you play). Your opponent neglected to tell you that they are a former Australian chess champion, and you have unfortunately found yourself in a tricky position.\n",
        "\n",
        "Your opponent is now beginning to gloat, and has asked you to resign so as to not waste the time of such a great chess player as them (they're only kidding but it still hurts). You think that maybe you can use data science to determine whether the position is lost (meaning you should resign) or drawn (meaning you should keep playing and teach your colleague a lesson).\n",
        "\n",
        "One of your other colleagues suggested that, since your opponent is so good, they surely should be able to win in ten turns or less. Your opponent has agreed to these rules - if they cannot beat you in ten turns, the game will be declared a draw.\n",
        "\n",
        "You've found a dataset which contains thousands of chess positions where white has a rook and a king, and black has a king. Each entry in the original dataset also includes whether the position is winnable by white some number of turns (labelled by the number of turns, e.g. \"one\", \"two\", ..., \"fifteen\", \"sixteen\") or not (labelled \"draw\", since black can only draw with a king). In each position, it is black's turn to play.\n",
        "\n",
        "The code cell below imports the dataset and does a small transformation on it, converting it from a 17-class problem (\"draw\", \"one\", \"two\", ..., \"sixteen\") to a binary problem (\"loss\" or \"draw\"), where a position is considered a draw if either black can force a draw, or if white cannot win within ten moves (with optimal play on both sides). Each entry in this transformed dataset contains the following information:\n",
        "\n",
        "- The white king's file (a, b, c, d)\n",
        "- The white king's rank (1, 2, 3, 4)\n",
        "- The white rook's file (a, b, ..., h)\n",
        "- The white rook's rank (1, 2, ..., 8)\n",
        "- The black king's file (a, b, ..., h)\n",
        "- The black king's rank (1, 2, ..., 8)\n",
        "- The optimal end result of this position for black (\"loss\" if the position can be won by white in less than 10 moves, and \"draw\" otherwise). \n",
        "\n",
        "Notice that the white king's rank and file are always within 1-4 and a-d respectively. It appears that whoever prepared this dataset rotated the board so the white king was always in this quadrant, probably to reduce the number of variables in our model. This would be something we would have to worry about if we wanted to make this model work for any possible chess position; in any case, don't worry about this for the purposes of this assignment as it won't be an issue (your opponent's king happens to be in this quadrant so it's not a problem).\n",
        "\n",
        "<b>Important Note: In this question, you might see the error message \"prediction from a rank-deficient fit may be misleading...\" show up. You can ignore this for the purposes of this assignment (try to think why we might be seeing a \"rank-deficient\" fit - there's a discussion at https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it if you're curious).</b>\n",
        "\n",
        "<b>Background information</b> This dataset is derived from a real dataset, which was \"generated by Michael Bain and Arthur van Hoff at the Turing Institute, Glasgow, UK\". You can find the original dataset in the UCI repository at https://archive.ics.uci.edu/ml/datasets/Chess+%28King-Rook+vs.+King%29 (but don't download the dataset from there; instead, use the code that we have provided below as well as the file \"krkopt.data\" on Moodle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c25d5426e07cf0bab63fcc0966552b89",
          "grade": false,
          "grade_id": "cell-b87295b283cb8e03",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "M0wp36nxj11m",
        "outputId": "d51aacdc-82fe-4596-cb65-c5d0039cac6c"
      },
      "source": [
        "N = 250\n",
        "\n",
        "set.seed(42)\n",
        "\n",
        "raw.df <- read.csv(\"krkopt.data\")\n",
        "names(raw.df) <- c(\"wk_file\", \"wk_rank\", \"wr_file\", \"wr_rank\", \"bk_file\", \"bk_rank\", \"result\")\n",
        "\n",
        "draws <- c(\"draw\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\")\n",
        "\n",
        "draw.idx <- raw.df$result %in% draws\n",
        "loss.idx <- !draw.idx\n",
        "\n",
        "draw.df <- raw.df[draw.idx,]\n",
        "loss.df <- raw.df[loss.idx,]\n",
        "\n",
        "draw.df$result = 1\n",
        "loss.df$result = 0\n",
        "\n",
        "draw.df <- draw.df[sample(nrow(draw.df), N),]\n",
        "loss.df <- loss.df[sample(nrow(loss.df), N),]\n",
        "\n",
        "raw.df <- rbind(draw.df, loss.df)\n",
        "raw.df <- raw.df[sample(nrow(raw.df)),]\n",
        "head(raw.df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead><tr><th></th><th scope=col>wk_file</th><th scope=col>wk_rank</th><th scope=col>wr_file</th><th scope=col>wr_rank</th><th scope=col>bk_file</th><th scope=col>bk_rank</th><th scope=col>result</th></tr></thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>3342</th><td>d</td><td>2</td><td>g</td><td>6</td><td>a</td><td>1</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>10816</th><td>c</td><td>1</td><td>a</td><td>4</td><td>h</td><td>8</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>8164</th><td>d</td><td>4</td><td>e</td><td>1</td><td>h</td><td>8</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>8534</th><td>b</td><td>1</td><td>g</td><td>5</td><td>d</td><td>1</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>9077</th><td>c</td><td>3</td><td>a</td><td>3</td><td>f</td><td>2</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>25779</th><td>a</td><td>1</td><td>e</td><td>2</td><td>g</td><td>3</td><td>1</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": "\\begin{tabular}{r|lllllll}\n  & wk\\_file & wk\\_rank & wr\\_file & wr\\_rank & bk\\_file & bk\\_rank & result\\\\\n\\hline\n\t3342 & d & 2 & g & 6 & a & 1 & 0\\\\\n\t10816 & c & 1 & a & 4 & h & 8 & 1\\\\\n\t8164 & d & 4 & e & 1 & h & 8 & 0\\\\\n\t8534 & b & 1 & g & 5 & d & 1 & 0\\\\\n\t9077 & c & 3 & a & 3 & f & 2 & 0\\\\\n\t25779 & a & 1 & e & 2 & g & 3 & 1\\\\\n\\end{tabular}\n",
            "text/markdown": "\n| <!--/--> | wk_file | wk_rank | wr_file | wr_rank | bk_file | bk_rank | result |\n|---|---|---|---|---|---|---|---|\n| 3342 | d | 2 | g | 6 | a | 1 | 0 |\n| 10816 | c | 1 | a | 4 | h | 8 | 1 |\n| 8164 | d | 4 | e | 1 | h | 8 | 0 |\n| 8534 | b | 1 | g | 5 | d | 1 | 0 |\n| 9077 | c | 3 | a | 3 | f | 2 | 0 |\n| 25779 | a | 1 | e | 2 | g | 3 | 1 |\n\n",
            "text/plain": [
              "      wk_file wk_rank wr_file wr_rank bk_file bk_rank result\n",
              "3342  d       2       g       6       a       1       0     \n",
              "10816 c       1       a       4       h       8       1     \n",
              "8164  d       4       e       1       h       8       0     \n",
              "8534  b       1       g       5       d       1       0     \n",
              "9077  c       3       a       3       f       2       0     \n",
              "25779 a       1       e       2       g       3       1     "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0e552cea173b5c974564465dbfcd789e",
          "grade": false,
          "grade_id": "cell-07f9eb1f1f449db1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Yt8SVfhEj11m"
      },
      "source": [
        "<h2>Question 2.a (5 marks)</h2>\n",
        "Before we can use our dataset, we need to preprocess it to make it suitable for use in a logistic regression. At the moment, our dataset consists of six feature variables, all of which are categorical, as well as one label variable (which is binary). We need to perform \"one-hot encoding\" on all of these categorical variables (i.e. all of the categorical variables, but not the binary label variable - think about why it would be useless to do one-hot encoding on a binary variable, if we're avoiding the dummy variable trap as discussed below).\n",
        "\n",
        "One-hot encoding (referred to as \"indicator variables\" in the lectures - see slide 41 in week 8's lecture notes) is the process of taking a categorical feature (with $k$ possible values it can take) and converting it to a set of $k$ binary values (in practice we use $k-1$, to avoid the \"dummy variable trap\"). It is important to do this because, when categorical variables are encoded as a single integer (e.g. \"red\" = 1, \"green\" = 2, \"blue\" = 3, ...), a logistic regression (and many other modelling techniques) will fit the feature as though \"red\" is closer to \"green\" than it is to \"blue\" (since 1 is closer to 2 than to 3). So, we split the values up into individual variables to avoid this.\n",
        "\n",
        "In fact, R's built-in logistic regression automatically performs one-hot encoding on factor variables prior to running the algorithm (but we're going to ignore that and implement the encoding ourselves instead). To learn more about one-hot encoding, Wikipedia has a good page on it at https://en.wikipedia.org/wiki/Dummy_variable_(statistics) (note that this technique goes by several names, including one-hot encoding, dummy variables, indicator variables, and a few others listed on the Wikipedia page).\n",
        "\n",
        "When we perform one-hot encoding, we take the dataset and, for each column we wish to encode:\n",
        "\n",
        " - Find how many different values the factor variable can take, using the levels() function; call this number $k$\n",
        " - Create $k-1$ new columns in the data-frame, corresponding to each level of the factor. For each row, set the value to 1 if the original variable's value corresponded to the column, and 0 otherwise. If the variable took the last value of the factor (the $k$-th value), just set them all to 0. We use $k-1$ columns to avoid the \"dummy variable trap\" (https://en.wikipedia.org/wiki/Dummy_variable_(statistics))\n",
        " - Don't forget to delete the original factor column from the data frame when you're done\n",
        "\n",
        "So, prepare our dataset as follows:\n",
        "\n",
        " 1. Write a function to.factors, which takes the variables listed below and returns a data-frame identical to df, except that the columns listed in col.names have been converted to factor columns (you are allowed to use R's built-in as.factor() function here)\n",
        "     - df is a data frame for which we wish to convert some variables to factors\n",
        "     - col.names is a vector of strings, where each string corresponds to the name of a column in the data-frame which we want to convert to a factor column using the to.factor() function\n",
        "\n",
        "\n",
        " 2. Write a function one.hot.encode, which takes two variables (df and to.encode) and returns a data-frame identical to df, except that the factor columns listed in to.encode have been replaced with one-hot encoding columns as described above\n",
        "     - df is a data frame which we wish to apply one-hot encoding to\n",
        "     - to.encode is a vector of strings, where each string corresponds to the name of a column in the data-frame which we want to convert from a factor column to a one-hot encoding\n",
        "     \n",
        "The column names for the one-hot encoding should take the following form:\n",
        "\n",
        "    original_col_name.factor_name\n",
        "    \n",
        "For example, the column encoding when \"wk_file\" = \"b\" should have the name \"wk_file_b\", and the column encoding when \"wr_rank\" = 6 should have the name \"wr_rank_6\".\n",
        "\n",
        "The to.factors function should look like this:\n",
        "\n",
        "    to.factors <- function(df, col.names) {\n",
        "        # Your code here\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "The one.hot.encode function should look like this:\n",
        "    \n",
        "    one.hot <- function(df, to.encode) {\n",
        "        # Your code here\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "10f606f86b793f48f772c3dcc2a78abb",
          "grade": true,
          "grade_id": "cell-0cdd1016d1579628",
          "locked": false,
          "points": 5,
          "schema_version": 1,
          "solution": true
        },
        "id": "koTIngUej11n"
      },
      "source": [
        "factor.names <- c(\"wk_file\", \"wk_rank\", \"wr_file\", \"wr_rank\", \"bk_file\", \"bk_rank\")\n",
        "to.factors <- function(df, col.names) {\n",
        "    for (i in col.names){                    \n",
        "        df[[i]]<-as.factor(df[[i]])            # convert all columns to factors\n",
        "    }\n",
        "\n",
        "    return(df)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "one.hot <- function(df, to.encode) \n",
        "{\n",
        "for (m in to.encode){\n",
        "lt_of_levels = levels(df[,m])                             # FOR EACH COLUMN FIND LEVELS AND STORE IT IN A VARIABLE\n",
        "n<-length(lt_of_levels)                         \n",
        "    for ( k in lt_of_levels[-n]){                   # ITERATE THROUGH ALL DROPPING THE LAST \"K\" FACTOR\n",
        "             df[paste0(m,sep=\"_\",k)] <- ifelse(df[,m] == k,1,0)  \n",
        "                                                                     # FORM COLUMNS AND ASSIGN VALUES ACCORDINGLY\n",
        "             }\n",
        "         } \n",
        "       \n",
        "    df<-df[,-c(1:6)]                         # DROP THE ORIGINAL 6 COLUMNS \n",
        "    \n",
        "    return(df)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "bf337bc71daaab84db679081c3e5ad33",
          "grade": false,
          "grade_id": "cell-605dfeb48350ef16",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "HwTI3mhCj11n",
        "outputId": "92227a6f-9fed-4b0a-828f-c3e8a13fd43a"
      },
      "source": [
        "preprocess <- function(df, factor.names) {\n",
        "    fact.df <- to.factors(df, factor.names)\n",
        "    one.hot.df <- one.hot(fact.df, factor.names)\n",
        "    \n",
        "    return (one.hot.df)\n",
        "}\n",
        "\n",
        "factor.names <- c(\"wk_file\", \"wk_rank\", \"wr_file\", \"wr_rank\", \"bk_file\", \"bk_rank\")\n",
        "df.student.preprocessed <- preprocess(raw.df, factor.names)\n",
        "\n",
        "head(df.student.preprocessed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead><tr><th></th><th scope=col>result</th><th scope=col>wk_file_a</th><th scope=col>wk_file_b</th><th scope=col>wk_file_c</th><th scope=col>wk_rank_1</th><th scope=col>wk_rank_2</th><th scope=col>wk_rank_3</th><th scope=col>wr_file_a</th><th scope=col>wr_file_b</th><th scope=col>wr_file_c</th><th scope=col>...</th><th scope=col>bk_file_e</th><th scope=col>bk_file_f</th><th scope=col>bk_file_g</th><th scope=col>bk_rank_1</th><th scope=col>bk_rank_2</th><th scope=col>bk_rank_3</th><th scope=col>bk_rank_4</th><th scope=col>bk_rank_5</th><th scope=col>bk_rank_6</th><th scope=col>bk_rank_7</th></tr></thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>3342</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><th scope=row>10816</th><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><th scope=row>8164</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><th scope=row>8534</th><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><th scope=row>9077</th><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><th scope=row>25779</th><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllll}\n  & result & wk\\_file\\_a & wk\\_file\\_b & wk\\_file\\_c & wk\\_rank\\_1 & wk\\_rank\\_2 & wk\\_rank\\_3 & wr\\_file\\_a & wr\\_file\\_b & wr\\_file\\_c & ... & bk\\_file\\_e & bk\\_file\\_f & bk\\_file\\_g & bk\\_rank\\_1 & bk\\_rank\\_2 & bk\\_rank\\_3 & bk\\_rank\\_4 & bk\\_rank\\_5 & bk\\_rank\\_6 & bk\\_rank\\_7\\\\\n\\hline\n\t3342 & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t10816 & 1   & 0   & 0   & 1   & 1   & 0   & 0   & 1   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t8164 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t8534 & 0   & 0   & 1   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t9077 & 0   & 0   & 0   & 1   & 0   & 0   & 1   & 1   & 0   & 0   & ... & 0   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0  \\\\\n\t25779 & 1   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0  \\\\\n\\end{tabular}\n",
            "text/markdown": "\n| <!--/--> | result | wk_file_a | wk_file_b | wk_file_c | wk_rank_1 | wk_rank_2 | wk_rank_3 | wr_file_a | wr_file_b | wr_file_c | ... | bk_file_e | bk_file_f | bk_file_g | bk_rank_1 | bk_rank_2 | bk_rank_3 | bk_rank_4 | bk_rank_5 | bk_rank_6 | bk_rank_7 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 3342 | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 10816 | 1   | 0   | 0   | 1   | 1   | 0   | 0   | 1   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 8164 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 8534 | 0   | 0   | 1   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 9077 | 0   | 0   | 0   | 1   | 0   | 0   | 1   | 1   | 0   | 0   | ... | 0   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   |\n| 25779 | 1   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   |\n\n",
            "text/plain": [
              "      result wk_file_a wk_file_b wk_file_c wk_rank_1 wk_rank_2 wk_rank_3\n",
              "3342  0      0         0         0         0         1         0        \n",
              "10816 1      0         0         1         1         0         0        \n",
              "8164  0      0         0         0         0         0         0        \n",
              "8534  0      0         1         0         1         0         0        \n",
              "9077  0      0         0         1         0         0         1        \n",
              "25779 1      1         0         0         1         0         0        \n",
              "      wr_file_a wr_file_b wr_file_c ... bk_file_e bk_file_f bk_file_g bk_rank_1\n",
              "3342  0         0         0         ... 0         0         0         1        \n",
              "10816 1         0         0         ... 0         0         0         0        \n",
              "8164  0         0         0         ... 0         0         0         0        \n",
              "8534  0         0         0         ... 0         0         0         1        \n",
              "9077  1         0         0         ... 0         1         0         0        \n",
              "25779 0         0         0         ... 0         0         1         0        \n",
              "      bk_rank_2 bk_rank_3 bk_rank_4 bk_rank_5 bk_rank_6 bk_rank_7\n",
              "3342  0         0         0         0         0         0        \n",
              "10816 0         0         0         0         0         0        \n",
              "8164  0         0         0         0         0         0        \n",
              "8534  0         0         0         0         0         0        \n",
              "9077  1         0         0         0         0         0        \n",
              "25779 0         1         0         0         0         0        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "186dbaaac4f4e6dc03f3ffc95e7574be",
          "grade": false,
          "grade_id": "cell-c158c0c1f48996ff",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "87Z3-mj_j11n"
      },
      "source": [
        "<h2>Question 2.b (3 marks)</h2>\n",
        "<b>Important note: the rest of the questions in Question 2 rely on the dataset created in Question 2.a. To ensure you are not disadvantaged when answering the rest of these questions, we have provided another dataset (\"krkopt_preprocessed.csv\") which is the expected result from the code in Question 2.a. You <i>must</i> use this new dataset for the remainder of Question 2 (the code cell immediately below this question will import the data as \"df.processed\") You can compare your output from Question 2.a to this dataset if you want to check your code for Question 2.a.</b>\n",
        "\n",
        "We now have a dataset which has been preprocessed and is ready for use with R's built-in glm() function.\n",
        "\n",
        "Perform the following steps in the code block two blocks down (under the block that reads \"krkopt_preprocessed.csv\"):\n",
        "\n",
        "1. Use glm() to create a linear regression\n",
        "2. Use the predict() function to find the estimated probabilities for the training set (using type=\"response\") <b>Store these probabilities in a variable called \"probs\"</b>\n",
        "3. Convert these probabilities into categorical predictions, and find the accuracy of the model. <b>Store these predictions in a variable called \"preds\".\n",
        "\n",
        "The code block underneath this one plots the predicted probability of success (on the x-axis) vs. whether the print was a success or not (on the y-axis).\n",
        "\n",
        "Do not import any libraries to do any of the above tasks, as they are doable in R without importing other libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "db57702984259d7ed4fa9f44648de89e",
          "grade": false,
          "grade_id": "cell-16353df0eb95627f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "0GB6ATOZj11o",
        "outputId": "ebf1bf05-7a05-4a83-eadd-f0811b683f54"
      },
      "source": [
        "df.processed <- read.csv(\"krkopt_preprocessed.csv\")\n",
        "\n",
        "head(df.processed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead><tr><th scope=col>result</th><th scope=col>wk_file_a</th><th scope=col>wk_file_b</th><th scope=col>wk_file_c</th><th scope=col>wk_rank_1</th><th scope=col>wk_rank_2</th><th scope=col>wk_rank_3</th><th scope=col>wr_file_a</th><th scope=col>wr_file_b</th><th scope=col>wr_file_c</th><th scope=col>...</th><th scope=col>bk_file_e</th><th scope=col>bk_file_f</th><th scope=col>bk_file_g</th><th scope=col>bk_rank_1</th><th scope=col>bk_rank_2</th><th scope=col>bk_rank_3</th><th scope=col>bk_rank_4</th><th scope=col>bk_rank_5</th><th scope=col>bk_rank_6</th><th scope=col>bk_rank_7</th></tr></thead>\n",
              "<tbody>\n",
              "\t<tr><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "\t<tr><td>1  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllll}\n result & wk\\_file\\_a & wk\\_file\\_b & wk\\_file\\_c & wk\\_rank\\_1 & wk\\_rank\\_2 & wk\\_rank\\_3 & wr\\_file\\_a & wr\\_file\\_b & wr\\_file\\_c & ... & bk\\_file\\_e & bk\\_file\\_f & bk\\_file\\_g & bk\\_rank\\_1 & bk\\_rank\\_2 & bk\\_rank\\_3 & bk\\_rank\\_4 & bk\\_rank\\_5 & bk\\_rank\\_6 & bk\\_rank\\_7\\\\\n\\hline\n\t 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t 1   & 0   & 0   & 1   & 1   & 0   & 0   & 1   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t 0   & 0   & 1   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n\t 0   & 0   & 0   & 1   & 0   & 0   & 1   & 1   & 0   & 0   & ... & 0   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0  \\\\\n\t 1   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 1   & 0   & 0   & 1   & 0   & 0   & 0   & 0  \\\\\n\\end{tabular}\n",
            "text/markdown": "\n| result | wk_file_a | wk_file_b | wk_file_c | wk_rank_1 | wk_rank_2 | wk_rank_3 | wr_file_a | wr_file_b | wr_file_c | ... | bk_file_e | bk_file_f | bk_file_g | bk_rank_1 | bk_rank_2 | bk_rank_3 | bk_rank_4 | bk_rank_5 | bk_rank_6 | bk_rank_7 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 1   | 0   | 0   | 1   | 1   | 0   | 0   | 1   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 0   | 0   | 1   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   |\n| 0   | 0   | 0   | 1   | 0   | 0   | 1   | 1   | 0   | 0   | ... | 0   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   |\n| 1   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 1   | 0   | 0   | 1   | 0   | 0   | 0   | 0   |\n\n",
            "text/plain": [
              "  result wk_file_a wk_file_b wk_file_c wk_rank_1 wk_rank_2 wk_rank_3 wr_file_a\n",
              "1 0      0         0         0         0         1         0         0        \n",
              "2 1      0         0         1         1         0         0         1        \n",
              "3 0      0         0         0         0         0         0         0        \n",
              "4 0      0         1         0         1         0         0         0        \n",
              "5 0      0         0         1         0         0         1         1        \n",
              "6 1      1         0         0         1         0         0         0        \n",
              "  wr_file_b wr_file_c ... bk_file_e bk_file_f bk_file_g bk_rank_1 bk_rank_2\n",
              "1 0         0         ... 0         0         0         1         0        \n",
              "2 0         0         ... 0         0         0         0         0        \n",
              "3 0         0         ... 0         0         0         0         0        \n",
              "4 0         0         ... 0         0         0         1         0        \n",
              "5 0         0         ... 0         1         0         0         1        \n",
              "6 0         0         ... 0         0         1         0         0        \n",
              "  bk_rank_3 bk_rank_4 bk_rank_5 bk_rank_6 bk_rank_7\n",
              "1 0         0         0         0         0        \n",
              "2 0         0         0         0         0        \n",
              "3 0         0         0         0         0        \n",
              "4 0         0         0         0         0        \n",
              "5 0         0         0         0         0        \n",
              "6 1         0         0         0         0        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBS_RaF3j11o",
        "outputId": "21ceb132-1195-41f3-ab9f-62b71a46fd84"
      },
      "source": [
        "names(df.processed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<ol class=list-inline>\n",
              "\t<li>'result'</li>\n",
              "\t<li>'wk_file_a'</li>\n",
              "\t<li>'wk_file_b'</li>\n",
              "\t<li>'wk_file_c'</li>\n",
              "\t<li>'wk_rank_1'</li>\n",
              "\t<li>'wk_rank_2'</li>\n",
              "\t<li>'wk_rank_3'</li>\n",
              "\t<li>'wr_file_a'</li>\n",
              "\t<li>'wr_file_b'</li>\n",
              "\t<li>'wr_file_c'</li>\n",
              "\t<li>'wr_file_d'</li>\n",
              "\t<li>'wr_file_e'</li>\n",
              "\t<li>'wr_file_f'</li>\n",
              "\t<li>'wr_file_g'</li>\n",
              "\t<li>'wr_rank_1'</li>\n",
              "\t<li>'wr_rank_2'</li>\n",
              "\t<li>'wr_rank_3'</li>\n",
              "\t<li>'wr_rank_4'</li>\n",
              "\t<li>'wr_rank_5'</li>\n",
              "\t<li>'wr_rank_6'</li>\n",
              "\t<li>'wr_rank_7'</li>\n",
              "\t<li>'bk_file_a'</li>\n",
              "\t<li>'bk_file_b'</li>\n",
              "\t<li>'bk_file_c'</li>\n",
              "\t<li>'bk_file_d'</li>\n",
              "\t<li>'bk_file_e'</li>\n",
              "\t<li>'bk_file_f'</li>\n",
              "\t<li>'bk_file_g'</li>\n",
              "\t<li>'bk_rank_1'</li>\n",
              "\t<li>'bk_rank_2'</li>\n",
              "\t<li>'bk_rank_3'</li>\n",
              "\t<li>'bk_rank_4'</li>\n",
              "\t<li>'bk_rank_5'</li>\n",
              "\t<li>'bk_rank_6'</li>\n",
              "\t<li>'bk_rank_7'</li>\n",
              "</ol>\n"
            ],
            "text/latex": "\\begin{enumerate*}\n\\item 'result'\n\\item 'wk\\_file\\_a'\n\\item 'wk\\_file\\_b'\n\\item 'wk\\_file\\_c'\n\\item 'wk\\_rank\\_1'\n\\item 'wk\\_rank\\_2'\n\\item 'wk\\_rank\\_3'\n\\item 'wr\\_file\\_a'\n\\item 'wr\\_file\\_b'\n\\item 'wr\\_file\\_c'\n\\item 'wr\\_file\\_d'\n\\item 'wr\\_file\\_e'\n\\item 'wr\\_file\\_f'\n\\item 'wr\\_file\\_g'\n\\item 'wr\\_rank\\_1'\n\\item 'wr\\_rank\\_2'\n\\item 'wr\\_rank\\_3'\n\\item 'wr\\_rank\\_4'\n\\item 'wr\\_rank\\_5'\n\\item 'wr\\_rank\\_6'\n\\item 'wr\\_rank\\_7'\n\\item 'bk\\_file\\_a'\n\\item 'bk\\_file\\_b'\n\\item 'bk\\_file\\_c'\n\\item 'bk\\_file\\_d'\n\\item 'bk\\_file\\_e'\n\\item 'bk\\_file\\_f'\n\\item 'bk\\_file\\_g'\n\\item 'bk\\_rank\\_1'\n\\item 'bk\\_rank\\_2'\n\\item 'bk\\_rank\\_3'\n\\item 'bk\\_rank\\_4'\n\\item 'bk\\_rank\\_5'\n\\item 'bk\\_rank\\_6'\n\\item 'bk\\_rank\\_7'\n\\end{enumerate*}\n",
            "text/markdown": "1. 'result'\n2. 'wk_file_a'\n3. 'wk_file_b'\n4. 'wk_file_c'\n5. 'wk_rank_1'\n6. 'wk_rank_2'\n7. 'wk_rank_3'\n8. 'wr_file_a'\n9. 'wr_file_b'\n10. 'wr_file_c'\n11. 'wr_file_d'\n12. 'wr_file_e'\n13. 'wr_file_f'\n14. 'wr_file_g'\n15. 'wr_rank_1'\n16. 'wr_rank_2'\n17. 'wr_rank_3'\n18. 'wr_rank_4'\n19. 'wr_rank_5'\n20. 'wr_rank_6'\n21. 'wr_rank_7'\n22. 'bk_file_a'\n23. 'bk_file_b'\n24. 'bk_file_c'\n25. 'bk_file_d'\n26. 'bk_file_e'\n27. 'bk_file_f'\n28. 'bk_file_g'\n29. 'bk_rank_1'\n30. 'bk_rank_2'\n31. 'bk_rank_3'\n32. 'bk_rank_4'\n33. 'bk_rank_5'\n34. 'bk_rank_6'\n35. 'bk_rank_7'\n\n\n",
            "text/plain": [
              " [1] \"result\"    \"wk_file_a\" \"wk_file_b\" \"wk_file_c\" \"wk_rank_1\" \"wk_rank_2\"\n",
              " [7] \"wk_rank_3\" \"wr_file_a\" \"wr_file_b\" \"wr_file_c\" \"wr_file_d\" \"wr_file_e\"\n",
              "[13] \"wr_file_f\" \"wr_file_g\" \"wr_rank_1\" \"wr_rank_2\" \"wr_rank_3\" \"wr_rank_4\"\n",
              "[19] \"wr_rank_5\" \"wr_rank_6\" \"wr_rank_7\" \"bk_file_a\" \"bk_file_b\" \"bk_file_c\"\n",
              "[25] \"bk_file_d\" \"bk_file_e\" \"bk_file_f\" \"bk_file_g\" \"bk_rank_1\" \"bk_rank_2\"\n",
              "[31] \"bk_rank_3\" \"bk_rank_4\" \"bk_rank_5\" \"bk_rank_6\" \"bk_rank_7\""
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "501c733daf7d496d23f1b66efe73214a",
          "grade": true,
          "grade_id": "cell-a32209d09a8d311f",
          "locked": false,
          "points": 3,
          "schema_version": 1,
          "solution": true
        },
        "id": "hoKJCZZCj11p",
        "outputId": "1abcdcf6-bb1d-433a-803b-cd5884a5d8df"
      },
      "source": [
        " model<-glm(result ~ ., family=\"binomial\", data=df.processed)     # train model \n",
        "probs<-predict(model,type=\"response\")                              # predict based on model\n",
        "preds<-ifelse(probs > 0.5,1,0)                      # making categorical predictions based on thresholds               \n",
        "preds       \n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<dl class=dl-horizontal>\n",
              "\t<dt>1</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>2</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>3</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>4</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>5</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>6</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>7</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>8</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>9</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>10</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>11</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>12</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>13</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>14</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>15</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>16</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>17</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>18</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>19</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>20</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>21</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>22</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>23</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>24</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>25</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>26</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>27</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>28</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>29</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>30</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>31</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>32</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>33</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>34</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>35</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>36</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>37</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>38</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>39</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>40</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>41</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>42</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>43</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>44</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>45</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>46</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>47</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>48</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>49</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>50</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>51</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>52</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>53</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>54</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>55</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>56</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>57</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>58</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>59</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>60</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>61</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>62</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>63</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>64</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>65</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>66</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>67</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>68</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>69</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>70</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>71</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>72</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>73</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>74</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>75</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>76</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>77</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>78</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>79</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>80</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>81</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>82</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>83</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>84</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>85</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>86</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>87</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>88</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>89</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>90</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>91</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>92</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>93</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>94</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>95</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>96</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>97</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>98</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>99</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>100</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>101</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>102</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>103</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>104</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>105</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>106</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>107</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>108</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>109</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>110</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>111</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>112</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>113</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>114</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>115</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>116</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>117</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>118</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>119</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>120</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>121</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>122</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>123</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>124</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>125</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>126</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>127</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>128</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>129</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>130</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>131</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>132</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>133</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>134</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>135</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>136</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>137</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>138</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>139</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>140</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>141</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>142</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>143</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>144</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>145</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>146</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>147</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>148</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>149</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>150</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>151</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>152</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>153</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>154</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>155</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>156</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>157</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>158</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>159</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>160</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>161</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>162</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>163</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>164</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>165</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>166</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>167</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>168</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>169</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>170</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>171</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>172</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>173</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>174</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>175</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>176</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>177</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>178</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>179</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>180</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>181</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>182</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>183</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>184</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>185</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>186</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>187</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>188</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>189</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>190</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>191</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>192</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>193</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>194</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>195</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>196</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>197</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>198</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>199</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>200</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>201</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>202</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>203</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>204</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>205</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>206</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>207</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>208</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>209</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>210</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>211</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>212</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>213</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>214</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>215</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>216</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>217</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>218</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>219</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>220</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>221</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>222</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>223</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>224</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>225</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>226</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>227</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>228</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>229</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>230</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>231</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>232</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>233</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>234</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>235</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>236</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>237</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>238</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>239</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>240</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>241</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>242</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>243</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>244</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>245</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>246</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>247</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>248</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>249</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>250</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>251</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>252</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>253</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>254</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>255</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>256</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>257</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>258</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>259</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>260</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>261</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>262</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>263</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>264</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>265</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>266</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>267</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>268</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>269</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>270</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>271</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>272</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>273</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>274</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>275</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>276</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>277</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>278</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>279</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>280</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>281</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>282</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>283</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>284</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>285</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>286</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>287</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>288</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>289</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>290</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>291</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>292</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>293</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>294</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>295</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>296</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>297</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>298</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>299</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>300</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>301</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>302</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>303</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>304</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>305</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>306</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>307</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>308</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>309</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>310</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>311</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>312</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>313</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>314</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>315</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>316</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>317</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>318</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>319</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>320</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>321</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>322</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>323</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>324</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>325</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>326</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>327</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>328</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>329</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>330</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>331</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>332</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>333</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>334</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>335</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>336</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>337</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>338</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>339</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>340</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>341</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>342</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>343</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>344</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>345</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>346</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>347</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>348</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>349</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>350</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>351</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>352</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>353</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>354</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>355</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>356</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>357</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>358</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>359</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>360</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>361</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>362</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>363</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>364</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>365</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>366</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>367</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>368</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>369</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>370</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>371</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>372</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>373</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>374</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>375</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>376</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>377</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>378</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>379</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>380</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>381</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>382</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>383</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>384</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>385</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>386</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>387</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>388</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>389</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>390</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>391</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>392</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>393</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>394</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>395</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>396</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>397</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>398</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>399</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>400</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>401</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>402</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>403</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>404</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>405</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>406</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>407</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>408</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>409</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>410</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>411</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>412</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>413</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>414</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>415</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>416</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>417</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>418</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>419</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>420</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>421</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>422</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>423</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>424</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>425</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>426</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>427</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>428</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>429</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>430</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>431</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>432</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>433</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>434</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>435</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>436</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>437</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>438</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>439</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>440</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>441</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>442</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>443</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>444</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>445</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>446</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>447</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>448</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>449</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>450</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>451</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>452</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>453</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>454</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>455</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>456</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>457</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>458</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>459</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>460</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>461</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>462</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>463</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>464</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>465</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>466</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>467</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>468</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>469</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>470</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>471</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>472</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>473</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>474</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>475</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>476</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>477</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>478</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>479</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>480</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>481</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>482</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>483</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>484</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>485</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>486</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>487</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>488</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>489</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>490</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>491</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>492</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>493</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>494</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>495</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>496</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>497</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>498</dt>\n",
              "\t\t<dd>0</dd>\n",
              "\t<dt>499</dt>\n",
              "\t\t<dd>1</dd>\n",
              "\t<dt>500</dt>\n",
              "\t\t<dd>0</dd>\n",
              "</dl>\n"
            ],
            "text/latex": "\\begin{description*}\n\\item[1] 0\n\\item[2] 0\n\\item[3] 0\n\\item[4] 0\n\\item[5] 0\n\\item[6] 1\n\\item[7] 0\n\\item[8] 0\n\\item[9] 0\n\\item[10] 1\n\\item[11] 1\n\\item[12] 1\n\\item[13] 1\n\\item[14] 0\n\\item[15] 0\n\\item[16] 0\n\\item[17] 1\n\\item[18] 0\n\\item[19] 0\n\\item[20] 1\n\\item[21] 0\n\\item[22] 0\n\\item[23] 1\n\\item[24] 1\n\\item[25] 1\n\\item[26] 0\n\\item[27] 1\n\\item[28] 0\n\\item[29] 1\n\\item[30] 1\n\\item[31] 1\n\\item[32] 1\n\\item[33] 0\n\\item[34] 0\n\\item[35] 1\n\\item[36] 0\n\\item[37] 0\n\\item[38] 1\n\\item[39] 1\n\\item[40] 1\n\\item[41] 0\n\\item[42] 0\n\\item[43] 1\n\\item[44] 1\n\\item[45] 0\n\\item[46] 1\n\\item[47] 0\n\\item[48] 0\n\\item[49] 1\n\\item[50] 1\n\\item[51] 1\n\\item[52] 0\n\\item[53] 0\n\\item[54] 1\n\\item[55] 0\n\\item[56] 0\n\\item[57] 0\n\\item[58] 0\n\\item[59] 1\n\\item[60] 1\n\\item[61] 1\n\\item[62] 1\n\\item[63] 1\n\\item[64] 1\n\\item[65] 0\n\\item[66] 1\n\\item[67] 1\n\\item[68] 1\n\\item[69] 1\n\\item[70] 1\n\\item[71] 1\n\\item[72] 0\n\\item[73] 0\n\\item[74] 1\n\\item[75] 0\n\\item[76] 0\n\\item[77] 0\n\\item[78] 1\n\\item[79] 0\n\\item[80] 0\n\\item[81] 0\n\\item[82] 1\n\\item[83] 1\n\\item[84] 0\n\\item[85] 1\n\\item[86] 1\n\\item[87] 0\n\\item[88] 1\n\\item[89] 0\n\\item[90] 0\n\\item[91] 1\n\\item[92] 0\n\\item[93] 1\n\\item[94] 1\n\\item[95] 1\n\\item[96] 1\n\\item[97] 1\n\\item[98] 0\n\\item[99] 1\n\\item[100] 1\n\\item[101] 1\n\\item[102] 0\n\\item[103] 0\n\\item[104] 0\n\\item[105] 1\n\\item[106] 1\n\\item[107] 1\n\\item[108] 0\n\\item[109] 1\n\\item[110] 1\n\\item[111] 0\n\\item[112] 0\n\\item[113] 1\n\\item[114] 0\n\\item[115] 0\n\\item[116] 1\n\\item[117] 1\n\\item[118] 0\n\\item[119] 1\n\\item[120] 1\n\\item[121] 1\n\\item[122] 1\n\\item[123] 0\n\\item[124] 0\n\\item[125] 1\n\\item[126] 0\n\\item[127] 0\n\\item[128] 1\n\\item[129] 0\n\\item[130] 1\n\\item[131] 0\n\\item[132] 1\n\\item[133] 1\n\\item[134] 1\n\\item[135] 0\n\\item[136] 0\n\\item[137] 1\n\\item[138] 0\n\\item[139] 1\n\\item[140] 0\n\\item[141] 1\n\\item[142] 0\n\\item[143] 1\n\\item[144] 1\n\\item[145] 1\n\\item[146] 1\n\\item[147] 1\n\\item[148] 1\n\\item[149] 0\n\\item[150] 1\n\\item[151] 0\n\\item[152] 0\n\\item[153] 0\n\\item[154] 0\n\\item[155] 1\n\\item[156] 1\n\\item[157] 0\n\\item[158] 1\n\\item[159] 1\n\\item[160] 1\n\\item[161] 1\n\\item[162] 1\n\\item[163] 1\n\\item[164] 0\n\\item[165] 0\n\\item[166] 1\n\\item[167] 0\n\\item[168] 1\n\\item[169] 1\n\\item[170] 1\n\\item[171] 0\n\\item[172] 0\n\\item[173] 0\n\\item[174] 0\n\\item[175] 0\n\\item[176] 0\n\\item[177] 1\n\\item[178] 1\n\\item[179] 0\n\\item[180] 1\n\\item[181] 0\n\\item[182] 1\n\\item[183] 0\n\\item[184] 0\n\\item[185] 1\n\\item[186] 1\n\\item[187] 1\n\\item[188] 0\n\\item[189] 1\n\\item[190] 1\n\\item[191] 0\n\\item[192] 0\n\\item[193] 1\n\\item[194] 1\n\\item[195] 1\n\\item[196] 0\n\\item[197] 0\n\\item[198] 1\n\\item[199] 1\n\\item[200] 1\n\\item[201] 0\n\\item[202] 1\n\\item[203] 1\n\\item[204] 0\n\\item[205] 1\n\\item[206] 1\n\\item[207] 1\n\\item[208] 1\n\\item[209] 0\n\\item[210] 0\n\\item[211] 0\n\\item[212] 0\n\\item[213] 1\n\\item[214] 0\n\\item[215] 1\n\\item[216] 0\n\\item[217] 1\n\\item[218] 1\n\\item[219] 0\n\\item[220] 0\n\\item[221] 0\n\\item[222] 1\n\\item[223] 0\n\\item[224] 1\n\\item[225] 0\n\\item[226] 0\n\\item[227] 0\n\\item[228] 0\n\\item[229] 1\n\\item[230] 1\n\\item[231] 1\n\\item[232] 1\n\\item[233] 0\n\\item[234] 0\n\\item[235] 0\n\\item[236] 1\n\\item[237] 0\n\\item[238] 0\n\\item[239] 0\n\\item[240] 1\n\\item[241] 0\n\\item[242] 0\n\\item[243] 0\n\\item[244] 0\n\\item[245] 0\n\\item[246] 1\n\\item[247] 0\n\\item[248] 0\n\\item[249] 0\n\\item[250] 1\n\\item[251] 0\n\\item[252] 1\n\\item[253] 1\n\\item[254] 0\n\\item[255] 0\n\\item[256] 0\n\\item[257] 1\n\\item[258] 0\n\\item[259] 0\n\\item[260] 1\n\\item[261] 1\n\\item[262] 0\n\\item[263] 1\n\\item[264] 1\n\\item[265] 1\n\\item[266] 1\n\\item[267] 0\n\\item[268] 0\n\\item[269] 0\n\\item[270] 0\n\\item[271] 1\n\\item[272] 0\n\\item[273] 0\n\\item[274] 0\n\\item[275] 1\n\\item[276] 1\n\\item[277] 0\n\\item[278] 0\n\\item[279] 1\n\\item[280] 1\n\\item[281] 0\n\\item[282] 0\n\\item[283] 1\n\\item[284] 1\n\\item[285] 1\n\\item[286] 0\n\\item[287] 0\n\\item[288] 0\n\\item[289] 0\n\\item[290] 1\n\\item[291] 0\n\\item[292] 0\n\\item[293] 1\n\\item[294] 0\n\\item[295] 0\n\\item[296] 1\n\\item[297] 0\n\\item[298] 1\n\\item[299] 1\n\\item[300] 1\n\\item[301] 1\n\\item[302] 1\n\\item[303] 1\n\\item[304] 1\n\\item[305] 0\n\\item[306] 1\n\\item[307] 0\n\\item[308] 1\n\\item[309] 1\n\\item[310] 0\n\\item[311] 0\n\\item[312] 0\n\\item[313] 1\n\\item[314] 1\n\\item[315] 0\n\\item[316] 1\n\\item[317] 0\n\\item[318] 1\n\\item[319] 0\n\\item[320] 0\n\\item[321] 1\n\\item[322] 0\n\\item[323] 0\n\\item[324] 1\n\\item[325] 0\n\\item[326] 0\n\\item[327] 1\n\\item[328] 1\n\\item[329] 0\n\\item[330] 1\n\\item[331] 1\n\\item[332] 0\n\\item[333] 1\n\\item[334] 1\n\\item[335] 0\n\\item[336] 1\n\\item[337] 0\n\\item[338] 1\n\\item[339] 0\n\\item[340] 1\n\\item[341] 0\n\\item[342] 0\n\\item[343] 0\n\\item[344] 0\n\\item[345] 0\n\\item[346] 0\n\\item[347] 1\n\\item[348] 1\n\\item[349] 1\n\\item[350] 1\n\\item[351] 0\n\\item[352] 1\n\\item[353] 0\n\\item[354] 1\n\\item[355] 0\n\\item[356] 1\n\\item[357] 1\n\\item[358] 1\n\\item[359] 0\n\\item[360] 1\n\\item[361] 0\n\\item[362] 0\n\\item[363] 1\n\\item[364] 0\n\\item[365] 1\n\\item[366] 0\n\\item[367] 0\n\\item[368] 1\n\\item[369] 0\n\\item[370] 1\n\\item[371] 1\n\\item[372] 0\n\\item[373] 0\n\\item[374] 0\n\\item[375] 0\n\\item[376] 0\n\\item[377] 0\n\\item[378] 0\n\\item[379] 0\n\\item[380] 0\n\\item[381] 1\n\\item[382] 0\n\\item[383] 1\n\\item[384] 1\n\\item[385] 1\n\\item[386] 1\n\\item[387] 0\n\\item[388] 0\n\\item[389] 0\n\\item[390] 1\n\\item[391] 0\n\\item[392] 1\n\\item[393] 1\n\\item[394] 0\n\\item[395] 0\n\\item[396] 0\n\\item[397] 1\n\\item[398] 0\n\\item[399] 0\n\\item[400] 1\n\\item[401] 1\n\\item[402] 1\n\\item[403] 0\n\\item[404] 0\n\\item[405] 1\n\\item[406] 0\n\\item[407] 1\n\\item[408] 1\n\\item[409] 1\n\\item[410] 1\n\\item[411] 1\n\\item[412] 0\n\\item[413] 1\n\\item[414] 0\n\\item[415] 0\n\\item[416] 1\n\\item[417] 1\n\\item[418] 0\n\\item[419] 0\n\\item[420] 1\n\\item[421] 0\n\\item[422] 1\n\\item[423] 1\n\\item[424] 0\n\\item[425] 0\n\\item[426] 1\n\\item[427] 1\n\\item[428] 0\n\\item[429] 1\n\\item[430] 1\n\\item[431] 1\n\\item[432] 1\n\\item[433] 0\n\\item[434] 0\n\\item[435] 0\n\\item[436] 1\n\\item[437] 0\n\\item[438] 1\n\\item[439] 1\n\\item[440] 1\n\\item[441] 1\n\\item[442] 1\n\\item[443] 0\n\\item[444] 1\n\\item[445] 1\n\\item[446] 0\n\\item[447] 0\n\\item[448] 1\n\\item[449] 1\n\\item[450] 0\n\\item[451] 0\n\\item[452] 0\n\\item[453] 1\n\\item[454] 0\n\\item[455] 1\n\\item[456] 0\n\\item[457] 1\n\\item[458] 0\n\\item[459] 1\n\\item[460] 1\n\\item[461] 0\n\\item[462] 0\n\\item[463] 1\n\\item[464] 0\n\\item[465] 1\n\\item[466] 0\n\\item[467] 0\n\\item[468] 0\n\\item[469] 1\n\\item[470] 0\n\\item[471] 1\n\\item[472] 1\n\\item[473] 0\n\\item[474] 1\n\\item[475] 0\n\\item[476] 0\n\\item[477] 1\n\\item[478] 0\n\\item[479] 1\n\\item[480] 1\n\\item[481] 0\n\\item[482] 0\n\\item[483] 1\n\\item[484] 0\n\\item[485] 0\n\\item[486] 1\n\\item[487] 0\n\\item[488] 0\n\\item[489] 1\n\\item[490] 1\n\\item[491] 0\n\\item[492] 1\n\\item[493] 1\n\\item[494] 0\n\\item[495] 1\n\\item[496] 1\n\\item[497] 0\n\\item[498] 0\n\\item[499] 1\n\\item[500] 0\n\\end{description*}\n",
            "text/markdown": "1\n:   02\n:   03\n:   04\n:   05\n:   06\n:   17\n:   08\n:   09\n:   010\n:   111\n:   112\n:   113\n:   114\n:   015\n:   016\n:   017\n:   118\n:   019\n:   020\n:   121\n:   022\n:   023\n:   124\n:   125\n:   126\n:   027\n:   128\n:   029\n:   130\n:   131\n:   132\n:   133\n:   034\n:   035\n:   136\n:   037\n:   038\n:   139\n:   140\n:   141\n:   042\n:   043\n:   144\n:   145\n:   046\n:   147\n:   048\n:   049\n:   150\n:   151\n:   152\n:   053\n:   054\n:   155\n:   056\n:   057\n:   058\n:   059\n:   160\n:   161\n:   162\n:   163\n:   164\n:   165\n:   066\n:   167\n:   168\n:   169\n:   170\n:   171\n:   172\n:   073\n:   074\n:   175\n:   076\n:   077\n:   078\n:   179\n:   080\n:   081\n:   082\n:   183\n:   184\n:   085\n:   186\n:   187\n:   088\n:   189\n:   090\n:   091\n:   192\n:   093\n:   194\n:   195\n:   196\n:   197\n:   198\n:   099\n:   1100\n:   1101\n:   1102\n:   0103\n:   0104\n:   0105\n:   1106\n:   1107\n:   1108\n:   0109\n:   1110\n:   1111\n:   0112\n:   0113\n:   1114\n:   0115\n:   0116\n:   1117\n:   1118\n:   0119\n:   1120\n:   1121\n:   1122\n:   1123\n:   0124\n:   0125\n:   1126\n:   0127\n:   0128\n:   1129\n:   0130\n:   1131\n:   0132\n:   1133\n:   1134\n:   1135\n:   0136\n:   0137\n:   1138\n:   0139\n:   1140\n:   0141\n:   1142\n:   0143\n:   1144\n:   1145\n:   1146\n:   1147\n:   1148\n:   1149\n:   0150\n:   1151\n:   0152\n:   0153\n:   0154\n:   0155\n:   1156\n:   1157\n:   0158\n:   1159\n:   1160\n:   1161\n:   1162\n:   1163\n:   1164\n:   0165\n:   0166\n:   1167\n:   0168\n:   1169\n:   1170\n:   1171\n:   0172\n:   0173\n:   0174\n:   0175\n:   0176\n:   0177\n:   1178\n:   1179\n:   0180\n:   1181\n:   0182\n:   1183\n:   0184\n:   0185\n:   1186\n:   1187\n:   1188\n:   0189\n:   1190\n:   1191\n:   0192\n:   0193\n:   1194\n:   1195\n:   1196\n:   0197\n:   0198\n:   1199\n:   1200\n:   1201\n:   0202\n:   1203\n:   1204\n:   0205\n:   1206\n:   1207\n:   1208\n:   1209\n:   0210\n:   0211\n:   0212\n:   0213\n:   1214\n:   0215\n:   1216\n:   0217\n:   1218\n:   1219\n:   0220\n:   0221\n:   0222\n:   1223\n:   0224\n:   1225\n:   0226\n:   0227\n:   0228\n:   0229\n:   1230\n:   1231\n:   1232\n:   1233\n:   0234\n:   0235\n:   0236\n:   1237\n:   0238\n:   0239\n:   0240\n:   1241\n:   0242\n:   0243\n:   0244\n:   0245\n:   0246\n:   1247\n:   0248\n:   0249\n:   0250\n:   1251\n:   0252\n:   1253\n:   1254\n:   0255\n:   0256\n:   0257\n:   1258\n:   0259\n:   0260\n:   1261\n:   1262\n:   0263\n:   1264\n:   1265\n:   1266\n:   1267\n:   0268\n:   0269\n:   0270\n:   0271\n:   1272\n:   0273\n:   0274\n:   0275\n:   1276\n:   1277\n:   0278\n:   0279\n:   1280\n:   1281\n:   0282\n:   0283\n:   1284\n:   1285\n:   1286\n:   0287\n:   0288\n:   0289\n:   0290\n:   1291\n:   0292\n:   0293\n:   1294\n:   0295\n:   0296\n:   1297\n:   0298\n:   1299\n:   1300\n:   1301\n:   1302\n:   1303\n:   1304\n:   1305\n:   0306\n:   1307\n:   0308\n:   1309\n:   1310\n:   0311\n:   0312\n:   0313\n:   1314\n:   1315\n:   0316\n:   1317\n:   0318\n:   1319\n:   0320\n:   0321\n:   1322\n:   0323\n:   0324\n:   1325\n:   0326\n:   0327\n:   1328\n:   1329\n:   0330\n:   1331\n:   1332\n:   0333\n:   1334\n:   1335\n:   0336\n:   1337\n:   0338\n:   1339\n:   0340\n:   1341\n:   0342\n:   0343\n:   0344\n:   0345\n:   0346\n:   0347\n:   1348\n:   1349\n:   1350\n:   1351\n:   0352\n:   1353\n:   0354\n:   1355\n:   0356\n:   1357\n:   1358\n:   1359\n:   0360\n:   1361\n:   0362\n:   0363\n:   1364\n:   0365\n:   1366\n:   0367\n:   0368\n:   1369\n:   0370\n:   1371\n:   1372\n:   0373\n:   0374\n:   0375\n:   0376\n:   0377\n:   0378\n:   0379\n:   0380\n:   0381\n:   1382\n:   0383\n:   1384\n:   1385\n:   1386\n:   1387\n:   0388\n:   0389\n:   0390\n:   1391\n:   0392\n:   1393\n:   1394\n:   0395\n:   0396\n:   0397\n:   1398\n:   0399\n:   0400\n:   1401\n:   1402\n:   1403\n:   0404\n:   0405\n:   1406\n:   0407\n:   1408\n:   1409\n:   1410\n:   1411\n:   1412\n:   0413\n:   1414\n:   0415\n:   0416\n:   1417\n:   1418\n:   0419\n:   0420\n:   1421\n:   0422\n:   1423\n:   1424\n:   0425\n:   0426\n:   1427\n:   1428\n:   0429\n:   1430\n:   1431\n:   1432\n:   1433\n:   0434\n:   0435\n:   0436\n:   1437\n:   0438\n:   1439\n:   1440\n:   1441\n:   1442\n:   1443\n:   0444\n:   1445\n:   1446\n:   0447\n:   0448\n:   1449\n:   1450\n:   0451\n:   0452\n:   0453\n:   1454\n:   0455\n:   1456\n:   0457\n:   1458\n:   0459\n:   1460\n:   1461\n:   0462\n:   0463\n:   1464\n:   0465\n:   1466\n:   0467\n:   0468\n:   0469\n:   1470\n:   0471\n:   1472\n:   1473\n:   0474\n:   1475\n:   0476\n:   0477\n:   1478\n:   0479\n:   1480\n:   1481\n:   0482\n:   0483\n:   1484\n:   0485\n:   0486\n:   1487\n:   0488\n:   0489\n:   1490\n:   1491\n:   0492\n:   1493\n:   1494\n:   0495\n:   1496\n:   1497\n:   0498\n:   0499\n:   1500\n:   0\n\n",
            "text/plain": [
              "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
              "  0   0   0   0   0   1   0   0   0   1   1   1   1   0   0   0   1   0   0   1 \n",
              " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
              "  0   0   1   1   1   0   1   0   1   1   1   1   0   0   1   0   0   1   1   1 \n",
              " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
              "  0   0   1   1   0   1   0   0   1   1   1   0   0   1   0   0   0   0   1   1 \n",
              " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
              "  1   1   1   1   0   1   1   1   1   1   1   0   0   1   0   0   0   1   0   0 \n",
              " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
              "  0   1   1   0   1   1   0   1   0   0   1   0   1   1   1   1   1   0   1   1 \n",
              "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
              "  1   0   0   0   1   1   1   0   1   1   0   0   1   0   0   1   1   0   1   1 \n",
              "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
              "  1   1   0   0   1   0   0   1   0   1   0   1   1   1   0   0   1   0   1   0 \n",
              "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
              "  1   0   1   1   1   1   1   1   0   1   0   0   0   0   1   1   0   1   1   1 \n",
              "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
              "  1   1   1   0   0   1   0   1   1   1   0   0   0   0   0   0   1   1   0   1 \n",
              "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
              "  0   1   0   0   1   1   1   0   1   1   0   0   1   1   1   0   0   1   1   1 \n",
              "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
              "  0   1   1   0   1   1   1   1   0   0   0   0   1   0   1   0   1   1   0   0 \n",
              "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
              "  0   1   0   1   0   0   0   0   1   1   1   1   0   0   0   1   0   0   0   1 \n",
              "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
              "  0   0   0   0   0   1   0   0   0   1   0   1   1   0   0   0   1   0   0   1 \n",
              "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
              "  1   0   1   1   1   1   0   0   0   0   1   0   0   0   1   1   0   0   1   1 \n",
              "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
              "  0   0   1   1   1   0   0   0   0   1   0   0   1   0   0   1   0   1   1   1 \n",
              "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
              "  1   1   1   1   0   1   0   1   1   0   0   0   1   1   0   1   0   1   0   0 \n",
              "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
              "  1   0   0   1   0   0   1   1   0   1   1   0   1   1   0   1   0   1   0   1 \n",
              "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
              "  0   0   0   0   0   0   1   1   1   1   0   1   0   1   0   1   1   1   0   1 \n",
              "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
              "  0   0   1   0   1   0   0   1   0   1   1   0   0   0   0   0   0   0   0   0 \n",
              "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
              "  1   0   1   1   1   1   0   0   0   1   0   1   1   0   0   0   1   0   0   1 \n",
              "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
              "  1   1   0   0   1   0   1   1   1   1   1   0   1   0   0   1   1   0   0   1 \n",
              "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
              "  0   1   1   0   0   1   1   0   1   1   1   1   0   0   0   1   0   1   1   1 \n",
              "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
              "  1   1   0   1   1   0   0   1   1   0   0   0   1   0   1   0   1   0   1   1 \n",
              "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
              "  0   0   1   0   1   0   0   0   1   0   1   1   0   1   0   0   1   0   1   1 \n",
              "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
              "  0   0   1   0   0   1   0   0   1   1   0   1   1   0   1   1   0   0   1   0 "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1455d91018e61bf609b87e4d660f68e2",
          "grade": false,
          "grade_id": "cell-2b434249f017e60a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "hZBXQzH5j11p",
        "outputId": "514ce0cf-31a6-438d-a755-7d73617c6db7"
      },
      "source": [
        "plot(df.processed$result ~ probs)\n",
        "table(preds, df.processed$result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     \n",
              "preds   0   1\n",
              "    0 213  35\n",
              "    1  37 215"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAALQCAMAAACOibeuAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAYAUlEQVR4nO3di2KiugJG4QQQL1V4/7cdQYEEvCAk0flZ39mntRYJ6tKhgGpq\nQIj59gIAIRE0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0\npBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0\npBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0\npBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0\npBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0\npBA0pBA0pBA0pBA0pBA0pBA0pBA0pBA0pCQI2gALLagtfMBfGAKaCBpSCBpSCBpSkgb9ty/a\n9fai/Is1BDYuYdBV5vwtmkcZApuXMOjS2OO5PXU5WVPGGAKblzBoa8796bOxMYbA5iUM2tvm\n/XoDOEFjIZ6hISXtOvTp0p5iHRqxpNxslztbObIqyhDYurTboct2O7Qt9myHRhz/3Z5C73iq\nN8dXmdHfoebR+cMs+rMnU3bjPB3MvagzTXehYTbDt+breEEenOtfSfNgaf3pnCU3D3/TX5PR\nEWrTaSZTTabuz/WnnVxyjieDvb7Mg3vi8R30yjeDvt9Y/Q/3q/5+Wuen0Ty8udzu69GUkyme\njuTewcPZ47u9duJ1r0w/D3cUd3Dv7h4tizOON//+MWLcuXu91MNXbxpvqklg7tV6kNqLDGfW\nOsv8dJ77atDur43z37tpnZ9G8zD3r/3Zkym7Qe59vBhpmFt/tjHdQjqV1850pvYm9s/1r+R9\nXsYfYZjO9OMYU3tfTX8t+oW6/3e/zGiYenSem7hzof5R0//itnD9Uo7rexS0f8n+//9Z0E+e\nVd8M4dVpRt9fTev8NJqHGZ89mdKYyXhPRvLnNrqQF7Rxphlm2uc4zNS9kv2CjK7E6KLOv1nd\nk73pT/cL003oPlT6YZyrY/rHiXHa7X/hxu08SpwfzfR7P8Wo9O4h8MkT+LM74wORgn79wPOH\nJujR0AQ9vjM+ECToZUMQtDcOQf/vQft1Gue/d9M6P43mYeqhL2duzpTdIF0Nz0ca5taf3WXh\nB+1MZ2pvYv9c/0p28fgjDNOZfhxjau+r6a9Fv1BdX33E7jD16DynOfdCff9Oyf3U9STMrmT/\nF/4l+/8v6/m/C3q4t7rniuH56NW0zk+jeXhz6Z7SvC+TKZ6ONEzrnu2eO3zrv0zn4Y7iDu5H\n4C+LM443/7o2o1kM53h5Tafxppo05l6tB6G9iPCzp+DX5qfz3DeD7u+l/odHV+rhtM5P03n4\nZ0+m7MZ5Oph7UWcaM9zb/tzqvqp6MvF4FPdKmgdL60/nLLl5+Jv+moyymE4zmWoydX+uP+3k\nknM8Gez1ZR7cE4/voFe+GzTwCkFDSsKg3/9zsXoIbF7CoA8EjehSrnKc7euXxgYYAluXdB36\n/Pqw/hBDYOPS/lF4cF6FFWkIbBtbOSCFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGF\noCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGF\noCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGF\noCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGFoCGF\noCGFoCGFoCElZdCXnbH7uj5kxpaRhsDWJQy6subqsG++mjzKENi8hEGX5vq8XFqzq+qqPR1+\nCGxewqBte0FjqvabjTEENi9h0MYMX7tvgYfA5n3hGbr5WvEMjSi+sA5dVvfT4YfA5rGVA1LY\nDg0p7CmEFIKGFIKGlG8FzXZoRPE7QRtXiCGwRaxyQApBQwpBQ0rSoP/2RbuGXJR/sYbAxqXc\n9Z05f/Wx6xtRJD04yR7P7anLyXJwEqJIevjouT995vBRRJH8AP9HPwQbApvHMzSkpF2HPl3a\nU6xDI5aUm+1yZytHVkUZAluXdjt02W6HtsWe7dCIgz2FkELQkELQkELQkELQkELQkELQkELQ\nkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQ\nkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkLIy6P4ju+3LjzpeMwTw\ngUBBX15/GP2aIYAPrAj6ZFzZl5cKaKx5hs7cnt982HH0pQIaodahwyJoLMRWDkhZEbTxfXmp\ngAZBQwqrHJBC0JBC0JCyerMd69D4JQQNKWFWOf7yYv2ivB4CmCPQOnRldqsX5c0QwAyh/ihk\nlQM/IVDQB8Px0PgFwf4o3AdbpJqgsVigoLNDsCUaDwF8gB0rkELQkLI26ENW15cs8AtWCBpL\nrQz61Gyus81aNC/Bwi9YGXRujvXZZPXR5MEWqSZoLBbgNYVnU7JjBT8iQNCFORE0fsTqVY7z\nqdlJyCoHfsP6PwrbnYSmeZZ+pyqb/eP7zJj8GHypgMbqzXa2WYOuszeFNi72ul5S2duuxddP\n6ASNhRLuWNmZorp+2V2ube/ah0HwIbB5CYM2prp/aY6ffnl0HkFjodVBn4p2S8dlxuWaC1rj\n/BByqYDG2qDz28sJjX1f9M6cr38SNl+aZ+iXK9EEjYVWBn0wedUEfZjxEqyzseW5Luy16FP2\neqsIQWOhlUHb6ypxu/YwZ8fKyc59PQBBY6EAewpnB13Xx137ntLF/s0KCkFjoZVBZ/dn6DPv\n4I+fEGYd+royEfQ1WASNhdZu5SjMnD1/q4YA5guyHdoUM/Z8j2bCdmjE8K3XFE6Djvbu6diS\nlUEXLw/JWIygsVCAzXYREDQWCrDZLgKCxkIrg66K/IOXe//tbxtFivLNhQgaCyV8w/PK/eRZ\nDvBHFAmDLo09tofa1ZeT5QB/RJFws529HTnaOnOAP6JI+oqVZz8EGwKbtyro9lXc9SEzds7m\naJ6hEd+KoJsXcNfd0Rz2/ea76zr06XbYKOvQiGVF0KXJrxX/mayqq/x1oDe58ydk9vIBQNBY\naEXQtt2psmtfTPXmVdx3f2X7dG6LPduhEcfyoM3EV5cKaKx9hj7d1jXmPUN/PATwoRVBN+9+\nVGXtpouqmLMOHXWpgMaKoC/tekb79gVmzvtyLBgC+NCa7dDnvNsAbXdhD7ojaCz0rVesfH0I\naCJoSFkRdLSNdgSNxQgaUla/L4dtdhT+2ffv1bh4CGC+lUGX9yPozmyHxk8I9apvVjnwE1a/\nnW73DM2ub/yC1asctjlw7mTfvOHziiGADwT5SIrmnQlCLdB0CGC+1TtWju2bNc742M3lQwCz\nsacQUggaUoK8P/S8zylcPAQwW5A/CutZn1O4dAhgvjCfsTLrcwoXDgF8YPWOlQ8+p3DZEMAH\nAuz6Jmj8jgBveM7nFOJ3hFmH5nMK8SNWHw/N5xTilwTZDr3gcwo/GQKYjT2FkELQkLI26ENW\n15fMZB98FtanQwDzrQz61Gyzs81fhUGLJmgstDLo3BzbbdDHsJs5CBoLBdhT2L7imz2F+AkB\ngi6a9/AnaPyE1asc51Pzgm9WOfAb1v9RaJoXfBsT9FWFBI2FVm+2u31AWxZ2VyFBYyF2rEAK\nQUMKewohhT2FkMKeQkhhTyGksKcQUthTCCnsKYQU9hRCCjtWIIWgIYW304UU3k4XUng7XUjh\n7XQhhbfThRTeThdSeDtdSOHtdCGFt9OFFPYUQgpBQ8raoKvSXr/asgq0PA+GAOZbGfTF3rdC\ns+sbP2H1K1Z2zXNzVZoi1BKNhwA+EGBPoX8iCILGQgGO5WhUBI2fsDLo0uTNO8z85e0LsYIh\naCwU5Hho9hTiV6zeDn1s9hTmnx3J8Xb9hKCx0Fd2rBA0YkkYtPHFGAKbt36VI597cNKfJWjE\nlvKPwqowebtDkVUOxLL6AH/bvAfY3AP8j8Y0z+UEjVhWvwTr3H6f+xKsS26KiqARTfJd3/vr\nczpBI5Zgz9B27sXP2Zu/CJctFdBIuw59syNoxMKub0gJs+ubF8niR3zrNYXsWEEUK4Mulh41\nOg169m5E4LlQm+3CImgsFOC97SIgaCy0MuiqyMN+yvd0COADq1c5Plnt/dvf3gqvKN88Cgga\nCyUMusqcqV9vtyZoLJRws11p7PG2o/xysq9fVEvQWChh0PZ+3EfjzbEfBI2Fwuwp3M35YGQz\nf3sfQWOhUMdyzHgnMJ6hEd/qN5qZf7RdM+3tLR1Zh0Ysq98K7INXrOTOVo7s5Q4ZgsZCSV+x\n8le226FtsWc7NOJYvcrRPUPzdrr4BWv/KNy369B/lgP88RPC7SkMedQnQWMhgoaUb71i5etD\nQBNBQwpBQ0qooPmMFfwEgoYUVjkgZUXQ5T7okjwaAvjQiqCbtQzexgC/ZVXQF4LGj1kR9C7O\nXsKFSwU0VgRdFQSNX8NbgUEKQUMK26EhZdVWDtah8WsIGlLWrnIU95dg7QItz4MhgPmCvUh2\n6Vv5vx0C+EDStzFYNATwgWBvNDP7gzc/HQL4wPq3AmveM+ZkTdBD7wgaCyV8s8alQwDzhXk7\n3WLO2+kuHgKYjT2FkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQ\nkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQkELQ\nkELQkELQkELQkELQkELQkELQkJIy6GpnTH7/RMPXnw1O0FgoYdCVdT5zlqARRcKgS3O4Vn2w\neTsTgkYMCYO2twtebHYhaESSMOiu4SrPCRqRJAw6M1V3KidoxJEw6IPZ3U9dTE7QiCLlZruy\nr/hkCBpRJN2xci66U5cdQSMG9hRCCkFDCkFDyreC5o9CRPE7QRtXiCGwRaxyQApBQwpBQ0rS\noP/2xe2Q6PIv1hDYuJQH+GfOX315lCGweUkP8LfHc3vqcrKmjDEENi/pAf7n/vTZ2BhDYPO+\ncID/9IdgQ2DzeIaGlLTr0KdLe4p1aMSScrNd7mzlyKpXUxI0Fkq7Hbpst0PbYs92aMTBnkJI\nIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhI\nIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhI\nIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhI\nIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhISRr0374wjaL8izUENi5h0FVm\nBnmUIbB5CYMujT2e21OXkzVljCGweQmDtubcnz4bG2MIbF7CoI159kOwIbB5PENDStp16NOl\nPcU6NGJJudkud7ZyZFWUIbB1abdDl+12aFvs2Q6NOP6fPYVmNmfS9mLzLnqbtB6fM51sMvt+\n6fpT3Ylh4uFKOL+o/cUbzai7rH8b+LfHdLbD6dHE00u5U/uTj2/2yU/9InuXejIb56Z8cl2m\nl3qyNO/9L0E/D/GV2r27Xk11n/R9/uO5tZepvd90cxktdV3X7pf+q7cEww/ufPvboPtSjycc\nZtudvs/bv6B7qdpbFnfy0c3uPUT76z48UofHysPZvL8u/g8Pz5hPKOjavaHr/r/JE+/oAvcJ\n/Xu9n1l9/9qfuk3ifK3vv75/vz9L3q+Ccf53v1rdZbqvQ9XejNpf9v/V/mXq8YT1MHDdL+Lk\nIt6lau+S90fC9I5wL+Bczbq7gYdLjRbIuy/vN93j6+L/8PCM+b4V9OuH38Nnivcxu/+0Df93\nnlG9p5e6/+39juifI4cZuUV3P/XfvKLv393nnFHqdVeyV5bxHnbe48C9gHOjjBNzp+qy7PN0\n+/IeZv0czRDo5Iafltz/NCzysJyTEZ3fPb8uo8gfnfGB3wnaD/TVLwmaoJ/6VtAfDkHQBD3P\nfxI069DeZerxhPUwcN0v4uQiozDdSxrWoWNaFPTjZGv/Ofpp2MP9+m6O3iR1Xbtn9nn2sxtN\n6ffjLZ4/I3e+/W3QfanHEw6zdR6WzsTDmf2lam9Z3MlHN/twAfe6u/+UDTN7MJv318X/4eEZ\n8yUNetUB/i87m0Y33NxvEvUnrcfnTCebzL5fuv5Ud2KYeLgSXs3e4o1m1F3Wvw3822M6W/ch\nZaYXfDhHM558fLNPfuoX2bvUk9k4N+WT6zK91JOleS9h0Bzgj/gSBs0B/ogvYdAcPor4EgY9\nWoOKMQQ2j2doSEm7Ds0B/ogs5WY7DvBHdGm3Q3OAPyL7X/YUArMQNKQQNKQQNKQQNKT8aNDA\nQgtqCx/wj43LSP/DQMFGImhG+oWBCJqRpAYiaEaSGoigGUlqIIJmJKmBCJqRpAYiaEaSGoig\nGUlqIIJmJKmBCJqRpAYiaEaSGui/DxqIgqAhhaAhhaAhhaAhhaAhhaAhhaAhhaAhhaAhhaAh\nhaAhhaAhhaAhhaAhhaAhJWnQpTW2rF6dEW2kQ5ZqpKu/KLfqZKTzzpjdJf5AVbS76Xq/+DfV\n+pFSBn37nKHsxRnRRirbM2z4O+XRVahsjFt1MtIp0nUaD3Sxt4EiPHTqs/8GowGCSBj0n7Hn\n+mzN39Mzoo10NruqeTbYRR+pUSz9rPbPRrLXM6ri9UfqhRho1w5Rhr/x6mYY96YKEUTCoEtz\nun49mv3TM6KNVNyuZvjQHl2F46L3Nf54pGPbWfX6Q09DDGRi3XjXZ5jcm2uIIBIGXZjmH62z\nKZ6eEW2ku/D3yYORLqN7KdZIO+ezfKMOdF+BCv7Iuc6y9O+SEEEkDHrySI/20H8y48rkCUbK\nzSVG0JORMlPvbbsqFXmg/X2VI/g/pPW5fviZ8atuvi0FfWj/RYs80t4cY/zr/OjWu33wafSB\n6kPzV6E9hB5oPAxBzx+pdbHB122mI7X/XiYKuvmjcBf8ifPRY7QR/gl6NAxBzx+pUdngKxyP\nVgSazWiJgm7WoS/Bt3pOBjo0qxzXR06Up+j/OWg7XtzJGdFGauQRtndPRtq1azUxgp5cp1hP\nB5OBMtOsp1cx9heMFj9EEMm3clzGWzkusbZyeDO+ZHmMHQPjkdZ8INlnI0XbFDkZKN5mu/Fc\nQwSRMOh9++x1GvYETM6INtL1dIT1jQcjxQv6ya13CX7FJgPdnjfDb/BueTdUiCC2sacw/N3+\nbKRWkj2F17Xnqlm1PcYeqDTN0RVl+Oedxv+8p/C6MtZo07pdD+eMuCPtYj1vTq+TfyrmSPtI\nt95koDzW3VQPN1WwIFIGfTtq6zasGZ0Rd6RoKwLT6+SfijrSKY9y600HinU31eOgAwSRMmgg\nOoKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKG\nFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIKGFIL+AXE+YGqbuCl/\nAEGHw035Awg6HG7KH0DQ4XBTpnFttrx/wJMxVdZ+WOohM9lh9Mvmo61Mfvregv7vCDoNY/bd\nR/AZU5jmUyyHj/9rz7mdPNw+fi7KB8VvAkGnYe4fknpsTubNp8Ef3XP6k9acm19F+aD4TSDo\nNMz9Y6yL5mT70b/F/Zx89EtWN1Yh6DTuf/c135yT03PK6+rH+fydZZRA0GnMDbre2+sqtL18\nZSEVEHQas4O+rnqUGevQixF0GrcV55PZ9d0W47Xq9mQ39XeWUgC3XBrdhoxTX+uDrRzXX2bN\nz2zlWI6g0zCm3exc1MPTr7sdetf98njbDv33xUX9vxF0GteKi2G/4M3BensK9+2Z7Z5Cel6M\noNNgrTgRbuc0CDoRbuc0CDoRbuc0CDoRbmdIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhI\nIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhIIWhI+QfD\n5nrobrfE/QAAAABJRU5ErkJggg==",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "16649891e7156830c9a2c21368ba7cc6",
          "grade": false,
          "grade_id": "cell-c51bf3b6d527ceac",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "uhRaEFrSj11q"
      },
      "source": [
        "<h2>Question 2.c (4 marks)</h2>\n",
        "The above model seems to do reasonably well, but let's see if we can do better using some variable selection.\n",
        "\n",
        "You've been discussing your model with some of your colleagues, and they have made several suggestions:\n",
        "\n",
        " - One colleague has suggested that it does not matter where the rook or the white king is (<b>only</b> where the black king is); that is, we only need to consider the following variables:\n",
        " \n",
        "     - bk_rank_1, bk_rank_2, ..., bk_rank_7\n",
        "     - bk_file_a, bk_file_b, ..., bk_file_g\n",
        "\n",
        "\n",
        "- Another colleague has suggested that it only matters where the white rook and king are (and <b>not</b> where the black king is); that is, we only need to consider the following variables:\n",
        "\n",
        "     - wr_rank_1, wr_rank_2, ..., wr_rank_7\n",
        "     - wr_file_a, wr_file_b, ..., wr_rank_g,\n",
        "     - wk_rank_1, wk_rank_2, wk_rank_3\n",
        "     - wk_file_a, wk_file_b, wk_file_c\n",
        "\n",
        "\n",
        " - Your opponent has heard what you've been up to, and they have suggested that it only matters where the white rook is (not the black or white king king). You don't trust them, but you'll give it a try... you only need to consider the following variables:\n",
        " \n",
        "     - wr_rank_1, wr_rank_2, wr_rank_3\n",
        "     - wr_file_a, wr_file_b, wk_file_c\n",
        "     \n",
        "For each of these three proposals, create a linear model which only contains those variables. Then calculate the accuracy of each of these models using the evaluate function provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "30f7642c9f3329c3befc7b6dc1219f7c",
          "grade": false,
          "grade_id": "cell-5aa4104f68b5e638",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "wWwy_vnPj11q"
      },
      "source": [
        "evaluate <- function(fit, truth) {\n",
        "    probs <- predict(fit, type=\"response\")\n",
        "    preds <- probs > 0.5\n",
        "    accuracy <- mean(preds == truth)\n",
        "    \n",
        "    return (accuracy)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "a8df06e655f4a90f6d5c2aa6ebc1f013",
          "grade": true,
          "grade_id": "cell-38efde9ef475383c",
          "locked": false,
          "points": 4,
          "schema_version": 1,
          "solution": true
        },
        "id": "L5uQKQBxj11q",
        "outputId": "b180121d-8333-4566-c187-e46c114f92f7"
      },
      "source": [
        "model_1<-glm(result ~bk_rank_1 +bk_rank_2+bk_rank_3+bk_rank_4+bk_rank_5+bk_rank_6+bk_rank_7+bk_file_a+bk_file_a+bk_file_b+bk_file_c+bk_file_d+bk_file_e+bk_file_f+bk_file_g, family=\"binomial\", data=df.processed) \n",
        "model_2<-glm(result ~wr_rank_1 +wr_rank_2+wr_rank_3+wr_rank_4+wr_rank_5+wr_rank_6+wr_rank_7+wr_file_a+wr_file_b+wr_file_c+wr_file_d+wr_file_e+wr_file_f+wr_file_g+wk_rank_1+wk_rank_2+wk_rank_3+wk_file_a+wk_file_b+wk_file_c, family=\"binomial\", data=df.processed)\n",
        "model_3<-glm(result ~wr_rank_1+wr_rank_2+wr_rank_3+wr_file_a+wr_file_b+wr_file_c,family=\"binomial\", data=df.processed) \n",
        "accuracy_1<-evaluate(model_1,df.processed$result)\n",
        "accuracy_2<-evaluate(model_2,df.processed$result)\n",
        "accuracy_3<-evaluate(model_3,df.processed$result)                     # select and train data only based on given clumn names \n",
        "print(accuracy_1)\n",
        "print(accuracy_2)       # print accuracy \n",
        "print(accuracy_3)\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] 0.798\n",
            "[1] 0.686\n",
            "[1] 0.546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b9a04a3038a59c96505629398bbf9cba",
          "grade": false,
          "grade_id": "cell-cc60c865de44c6af",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "WZgiLetMj11r"
      },
      "source": [
        "<h2>Question 2.d (1 mark)</h2>\n",
        "We can also use an automated, step-wise process to determine remove variables, using R's built-in step() function.\n",
        "\n",
        "The code to do this is already given to you in the first code cell below, as a function called \"to.stepwise\". You need to call this function in the second code cell below, two times. Each time will use a different value of k:\n",
        "\n",
        " - fit.aic: uses k = 2\n",
        " - fit.bic: uses k = log(N) where your dataset has N rows\n",
        "\n",
        "You should then run the third code cell below, which will call your evaluate() function on fit.aic and fit.bic. Take a look at how the AIC and BIC models do in comparison to the three models we came up with above (we'll discuss this in the next question)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4e7328324203b47390b0361015d9efb1",
          "grade": false,
          "grade_id": "cell-286efc1decde74e8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "UrZ1je6aj11r"
      },
      "source": [
        "to.stepwise <- function(df, k) {\n",
        "    fit <- glm(result ~ ., family=\"binomial\", data=df)\n",
        "    return (step(fit, k=k))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "af595aa9cf4330d2ea134cf475c80675",
          "grade": true,
          "grade_id": "cell-86272f145b7e150a",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "6aYXBZd6j11r",
        "outputId": "62dda3e3-a8c6-44f6-ce71-7a8230241101"
      },
      "source": [
        "n<-nrow(df.processed)\n",
        "fit.aic<-to.stepwise(df.processed,2)\n",
        "fit.bic<-to.stepwise(df.processed,log(n))# your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start:  AIC=377.33\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_f  1   307.33 375.33\n",
            "- bk_rank_3  1   307.34 375.34\n",
            "- wr_rank_6  1   307.34 375.34\n",
            "- wr_file_d  1   307.40 375.40\n",
            "- wr_rank_2  1   307.55 375.55\n",
            "- wr_file_b  1   307.57 375.57\n",
            "- wr_file_g  1   307.90 375.90\n",
            "- wk_rank_3  1   307.97 375.97\n",
            "- wr_rank_1  1   308.16 376.16\n",
            "- wr_file_e  1   308.41 376.41\n",
            "- wr_file_a  1   308.54 376.54\n",
            "- bk_file_b  1   308.61 376.61\n",
            "<none>           307.33 377.33\n",
            "- wr_rank_7  1   309.50 377.50\n",
            "- wr_file_c  1   309.67 377.67\n",
            "- wk_rank_2  1   310.34 378.34\n",
            "- bk_rank_4  1   310.58 378.58\n",
            "- bk_rank_6  1   310.88 378.88\n",
            "- bk_file_a  1   311.22 379.22\n",
            "- wr_rank_5  1   311.57 379.57\n",
            "- wr_rank_3  1   313.41 381.41\n",
            "- wk_file_c  1   313.96 381.96\n",
            "- bk_rank_7  1   316.55 384.55\n",
            "- wk_file_a  1   317.14 385.14\n",
            "- bk_rank_5  1   317.43 385.43\n",
            "- wk_file_b  1   319.56 387.56\n",
            "- bk_rank_2  1   321.53 389.53\n",
            "- wr_rank_4  1   321.59 389.59\n",
            "- bk_file_g  1   325.45 393.45\n",
            "- wk_rank_1  1   326.10 394.10\n",
            "- bk_file_f  1   327.30 395.30\n",
            "- bk_file_c  1   329.91 397.91\n",
            "- bk_file_d  1   332.31 400.31\n",
            "- bk_file_e  1   335.65 403.65\n",
            "- bk_rank_1  1   388.57 456.57\n",
            "\n",
            "Step:  AIC=375.33\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + bk_rank_4 + \n",
            "    bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- bk_rank_3  1   307.34 373.34\n",
            "- wr_rank_6  1   307.34 373.34\n",
            "- wr_file_d  1   307.42 373.42\n",
            "- wr_rank_2  1   307.55 373.55\n",
            "- wr_file_b  1   307.66 373.66\n",
            "- wk_rank_3  1   307.97 373.97\n",
            "- wr_file_g  1   308.06 374.06\n",
            "- wr_rank_1  1   308.16 374.16\n",
            "- bk_file_b  1   308.61 374.61\n",
            "- wr_file_e  1   308.70 374.70\n",
            "- wr_file_a  1   309.00 375.00\n",
            "<none>           307.33 375.33\n",
            "- wr_rank_7  1   309.51 375.51\n",
            "- wk_rank_2  1   310.34 376.34\n",
            "- wr_file_c  1   310.48 376.48\n",
            "- bk_rank_4  1   310.60 376.60\n",
            "- bk_rank_6  1   310.88 376.88\n",
            "- bk_file_a  1   311.25 377.25\n",
            "- wr_rank_5  1   311.57 377.57\n",
            "- wr_rank_3  1   313.41 379.41\n",
            "- wk_file_c  1   313.96 379.96\n",
            "- bk_rank_7  1   316.65 382.65\n",
            "- wk_file_a  1   317.17 383.17\n",
            "- bk_rank_5  1   317.43 383.43\n",
            "- wk_file_b  1   319.56 385.56\n",
            "- wr_rank_4  1   321.60 387.60\n",
            "- bk_rank_2  1   321.67 387.67\n",
            "- bk_file_g  1   325.47 391.47\n",
            "- wk_rank_1  1   326.14 392.14\n",
            "- bk_file_f  1   327.32 393.32\n",
            "- bk_file_c  1   329.91 395.91\n",
            "- bk_file_d  1   332.31 398.31\n",
            "- bk_file_e  1   335.66 401.66\n",
            "- bk_rank_1  1   388.59 454.59\n",
            "\n",
            "Step:  AIC=373.34\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + \n",
            "    bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_6  1   307.36 371.36\n",
            "- wr_file_d  1   307.43 371.43\n",
            "- wr_rank_2  1   307.56 371.56\n",
            "- wr_file_b  1   307.68 371.68\n",
            "- wk_rank_3  1   308.01 372.01\n",
            "- wr_file_g  1   308.07 372.07\n",
            "- wr_rank_1  1   308.16 372.16\n",
            "- bk_file_b  1   308.61 372.61\n",
            "- wr_file_e  1   308.71 372.71\n",
            "- wr_file_a  1   309.00 373.00\n",
            "<none>           307.34 373.34\n",
            "- wr_rank_7  1   309.51 373.51\n",
            "- wk_rank_2  1   310.43 374.43\n",
            "- wr_file_c  1   310.48 374.48\n",
            "- bk_file_a  1   311.38 375.38\n",
            "- wr_rank_5  1   311.57 375.57\n",
            "- bk_rank_4  1   311.88 375.88\n",
            "- bk_rank_6  1   311.99 375.99\n",
            "- wr_rank_3  1   313.41 377.41\n",
            "- wk_file_c  1   313.99 377.99\n",
            "- wk_file_a  1   317.25 381.25\n",
            "- bk_rank_7  1   319.24 383.24\n",
            "- wk_file_b  1   319.56 383.56\n",
            "- bk_rank_5  1   320.85 384.85\n",
            "- wr_rank_4  1   321.62 385.62\n",
            "- bk_file_g  1   325.48 389.48\n",
            "- bk_rank_2  1   326.20 390.20\n",
            "- wk_rank_1  1   326.58 390.58\n",
            "- bk_file_f  1   327.33 391.33\n",
            "- bk_file_c  1   330.38 394.38\n",
            "- bk_file_d  1   332.57 396.57\n",
            "- bk_file_e  1   335.66 399.66\n",
            "- bk_rank_1  1   414.00 478.00\n",
            "\n",
            "Step:  AIC=371.36\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + \n",
            "    bk_file_c + bk_file_d + bk_file_e + bk_file_f + bk_file_g + \n",
            "    bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + \n",
            "    bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_d  1   307.45 369.45\n",
            "- wr_file_b  1   307.70 369.70\n",
            "- wr_rank_2  1   307.80 369.80\n",
            "- wk_rank_3  1   308.03 370.03\n",
            "- wr_file_g  1   308.10 370.10\n",
            "- bk_file_b  1   308.64 370.64\n",
            "- wr_file_e  1   308.72 370.72\n",
            "- wr_rank_1  1   308.79 370.79\n",
            "- wr_file_a  1   309.02 371.02\n",
            "<none>           307.36 371.36\n",
            "- wk_rank_2  1   310.46 372.46\n",
            "- wr_file_c  1   310.48 372.48\n",
            "- wr_rank_7  1   311.03 373.03\n",
            "- bk_file_a  1   311.41 373.41\n",
            "- bk_rank_4  1   311.88 373.88\n",
            "- bk_rank_6  1   312.00 374.00\n",
            "- wk_file_c  1   314.01 376.01\n",
            "- wr_rank_5  1   314.60 376.60\n",
            "- wr_rank_3  1   317.14 379.14\n",
            "- wk_file_a  1   317.28 379.28\n",
            "- bk_rank_7  1   319.26 381.26\n",
            "- wk_file_b  1   319.57 381.57\n",
            "- bk_rank_5  1   320.87 382.87\n",
            "- bk_file_g  1   325.48 387.48\n",
            "- bk_rank_2  1   326.21 388.21\n",
            "- wk_rank_1  1   326.61 388.61\n",
            "- bk_file_f  1   327.33 389.33\n",
            "- wr_rank_4  1   327.59 389.59\n",
            "- bk_file_c  1   330.52 392.52\n",
            "- bk_file_d  1   332.58 394.58\n",
            "- bk_file_e  1   335.89 397.89\n",
            "- bk_rank_1  1   414.01 476.01\n",
            "\n",
            "Step:  AIC=369.45\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_e + \n",
            "    wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_2  1   307.87 367.87\n",
            "- wr_file_b  1   307.97 367.97\n",
            "- wk_rank_3  1   308.18 368.18\n",
            "- wr_file_g  1   308.41 368.41\n",
            "- wr_file_e  1   308.73 368.73\n",
            "- bk_file_b  1   308.74 368.74\n",
            "- wr_rank_1  1   308.84 368.84\n",
            "- wr_file_a  1   309.05 369.05\n",
            "<none>           307.45 369.45\n",
            "- wk_rank_2  1   310.53 370.53\n",
            "- wr_rank_7  1   311.05 371.05\n",
            "- wr_file_c  1   311.39 371.39\n",
            "- bk_file_a  1   311.44 371.44\n",
            "- bk_rank_4  1   312.04 372.04\n",
            "- bk_rank_6  1   312.15 372.15\n",
            "- wk_file_c  1   314.45 374.45\n",
            "- wr_rank_5  1   314.65 374.65\n",
            "- wr_rank_3  1   317.20 377.20\n",
            "- wk_file_a  1   317.41 377.41\n",
            "- bk_rank_7  1   319.28 379.28\n",
            "- wk_file_b  1   319.64 379.64\n",
            "- bk_rank_5  1   321.04 381.04\n",
            "- bk_file_g  1   325.48 385.48\n",
            "- bk_rank_2  1   326.22 386.22\n",
            "- wk_rank_1  1   326.78 386.78\n",
            "- bk_file_f  1   327.37 387.37\n",
            "- wr_rank_4  1   327.59 387.59\n",
            "- bk_file_c  1   330.53 390.53\n",
            "- bk_file_d  1   332.65 392.65\n",
            "- bk_file_e  1   336.05 396.05\n",
            "- bk_rank_1  1   414.05 474.05\n",
            "\n",
            "Step:  AIC=367.87\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_e + \n",
            "    wr_file_g + wr_rank_1 + wr_rank_3 + wr_rank_4 + wr_rank_5 + \n",
            "    wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + bk_file_d + \n",
            "    bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_b  1   308.45 366.45\n",
            "- wk_rank_3  1   308.66 366.66\n",
            "- wr_file_g  1   308.84 366.84\n",
            "- wr_rank_1  1   308.88 366.88\n",
            "- wr_file_e  1   309.07 367.07\n",
            "- bk_file_b  1   309.37 367.37\n",
            "- wr_file_a  1   309.44 367.44\n",
            "<none>           307.87 367.87\n",
            "- wr_rank_7  1   311.06 369.06\n",
            "- wk_rank_2  1   311.16 369.16\n",
            "- bk_file_a  1   311.68 369.68\n",
            "- wr_file_c  1   311.74 369.74\n",
            "- bk_rank_4  1   312.09 370.09\n",
            "- bk_rank_6  1   312.43 370.43\n",
            "- wr_rank_5  1   314.86 372.86\n",
            "- wk_file_c  1   315.00 373.00\n",
            "- wr_rank_3  1   317.58 375.58\n",
            "- wk_file_a  1   317.88 375.88\n",
            "- bk_rank_7  1   319.77 377.77\n",
            "- wk_file_b  1   320.23 378.23\n",
            "- bk_rank_5  1   321.35 379.35\n",
            "- bk_rank_2  1   326.45 384.45\n",
            "- bk_file_g  1   327.41 385.41\n",
            "- wk_rank_1  1   327.47 385.47\n",
            "- bk_file_f  1   328.08 386.08\n",
            "- wr_rank_4  1   328.72 386.72\n",
            "- bk_file_c  1   331.59 389.59\n",
            "- bk_file_d  1   333.18 391.18\n",
            "- bk_file_e  1   336.69 394.69\n",
            "- bk_rank_1  1   415.47 473.47\n",
            "\n",
            "Step:  AIC=366.45\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_c + wr_file_e + wr_file_g + \n",
            "    wr_rank_1 + wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + \n",
            "    bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_g  1   309.12 365.12\n",
            "- wk_rank_3  1   309.27 365.27\n",
            "- wr_rank_1  1   309.40 365.40\n",
            "- bk_file_b  1   310.10 366.10\n",
            "- wr_file_e  1   310.28 366.28\n",
            "<none>           308.45 366.45\n",
            "- wr_file_a  1   310.86 366.86\n",
            "- wr_rank_7  1   311.56 367.56\n",
            "- wk_rank_2  1   311.72 367.72\n",
            "- wr_file_c  1   311.77 367.77\n",
            "- bk_file_a  1   312.15 368.15\n",
            "- bk_rank_4  1   313.01 369.01\n",
            "- bk_rank_6  1   313.13 369.13\n",
            "- wk_file_c  1   315.42 371.42\n",
            "- wr_rank_5  1   315.49 371.49\n",
            "- wk_file_a  1   318.01 374.01\n",
            "- wr_rank_3  1   318.49 374.49\n",
            "- wk_file_b  1   320.48 376.48\n",
            "- bk_rank_7  1   321.65 377.65\n",
            "- bk_rank_5  1   321.84 377.84\n",
            "- bk_rank_2  1   327.15 383.15\n",
            "- wk_rank_1  1   328.24 384.24\n",
            "- bk_file_f  1   329.30 385.30\n",
            "- wr_rank_4  1   329.46 385.46\n",
            "- bk_file_g  1   329.58 385.58\n",
            "- bk_file_c  1   333.22 389.22\n",
            "- bk_file_d  1   334.41 390.41\n",
            "- bk_file_e  1   337.73 393.73\n",
            "- bk_rank_1  1   415.75 471.75\n",
            "\n",
            "Step:  AIC=365.12\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_c + wr_file_e + wr_rank_1 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + \n",
            "    bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wk_rank_3  1   309.87 363.87\n",
            "- wr_rank_1  1   310.03 364.03\n",
            "- bk_file_b  1   310.94 364.94\n",
            "<none>           309.12 365.12\n",
            "- wr_file_e  1   311.45 365.45\n",
            "- wr_file_a  1   312.00 366.00\n",
            "- wr_file_c  1   312.01 366.01\n",
            "- wk_rank_2  1   312.12 366.12\n",
            "- wr_rank_7  1   312.13 366.13\n",
            "- bk_file_a  1   312.67 366.67\n",
            "- bk_rank_6  1   313.26 367.26\n",
            "- bk_rank_4  1   313.60 367.60\n",
            "- wk_file_c  1   316.22 370.22\n",
            "- wr_rank_5  1   316.63 370.63\n",
            "- wk_file_a  1   319.19 373.19\n",
            "- wr_rank_3  1   319.36 373.36\n",
            "- wk_file_b  1   321.09 375.09\n",
            "- bk_rank_5  1   322.29 376.29\n",
            "- bk_rank_7  1   322.71 376.71\n",
            "- bk_rank_2  1   328.10 382.10\n",
            "- wk_rank_1  1   328.41 382.41\n",
            "- wr_rank_4  1   330.10 384.10\n",
            "- bk_file_f  1   330.80 384.80\n",
            "- bk_file_g  1   331.15 385.15\n",
            "- bk_file_c  1   333.65 387.65\n",
            "- bk_file_d  1   335.81 389.81\n",
            "- bk_file_e  1   339.56 393.56\n",
            "- bk_rank_1  1   416.52 470.52\n",
            "\n",
            "Step:  AIC=363.87\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_1 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + \n",
            "    bk_file_c + bk_file_d + bk_file_e + bk_file_f + bk_file_g + \n",
            "    bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + \n",
            "    bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_1  1   310.71 362.71\n",
            "<none>           309.87 363.87\n",
            "- bk_file_b  1   312.31 364.31\n",
            "- wk_rank_2  1   312.50 364.50\n",
            "- wr_file_e  1   312.60 364.60\n",
            "- wr_file_c  1   312.79 364.79\n",
            "- wr_rank_7  1   312.81 364.81\n",
            "- wr_file_a  1   312.90 364.90\n",
            "- bk_file_a  1   313.14 365.14\n",
            "- bk_rank_6  1   314.09 366.09\n",
            "- bk_rank_4  1   314.26 366.26\n",
            "- wr_rank_5  1   317.36 369.36\n",
            "- wk_file_c  1   318.41 370.41\n",
            "- wr_rank_3  1   319.93 371.93\n",
            "- wk_file_a  1   320.41 372.41\n",
            "- wk_file_b  1   322.55 374.55\n",
            "- bk_rank_5  1   322.98 374.98\n",
            "- bk_rank_7  1   324.27 376.27\n",
            "- bk_rank_2  1   329.36 381.36\n",
            "- wr_rank_4  1   330.86 382.86\n",
            "- bk_file_g  1   331.90 383.90\n",
            "- bk_file_f  1   333.18 385.18\n",
            "- bk_file_c  1   335.62 387.62\n",
            "- bk_file_d  1   337.98 389.98\n",
            "- wk_rank_1  1   339.64 391.64\n",
            "- bk_file_e  1   342.42 394.42\n",
            "- bk_rank_1  1   418.58 470.58\n",
            "\n",
            "Step:  AIC=362.71\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "<none>           310.71 362.71\n",
            "- wr_rank_7  1   313.10 363.10\n",
            "- wk_rank_2  1   313.49 363.49\n",
            "- wr_file_c  1   313.52 363.52\n",
            "- wr_file_e  1   313.57 363.57\n",
            "- wr_file_a  1   313.69 363.69\n",
            "- bk_file_b  1   313.70 363.70\n",
            "- bk_file_a  1   313.91 363.91\n",
            "- bk_rank_4  1   315.06 365.06\n",
            "- bk_rank_6  1   315.32 365.32\n",
            "- wr_rank_5  1   317.42 367.42\n",
            "- wk_file_c  1   319.33 369.33\n",
            "- wr_rank_3  1   319.94 369.94\n",
            "- wk_file_a  1   320.61 370.61\n",
            "- bk_rank_5  1   323.23 373.23\n",
            "- wk_file_b  1   323.36 373.36\n",
            "- bk_rank_7  1   325.77 375.77\n",
            "- bk_rank_2  1   329.77 379.77\n",
            "- wr_rank_4  1   330.91 380.91\n",
            "- bk_file_g  1   333.71 383.71\n",
            "- bk_file_f  1   334.74 384.74\n",
            "- bk_file_c  1   337.63 387.63\n",
            "- bk_file_d  1   339.60 389.60\n",
            "- wk_rank_1  1   340.85 390.85\n",
            "- bk_file_e  1   344.00 394.00\n",
            "- bk_rank_1  1   418.79 468.79\n",
            "Start:  AIC=524.84\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_f  1   307.33 518.62\n",
            "- bk_rank_3  1   307.34 518.64\n",
            "- wr_rank_6  1   307.34 518.64\n",
            "- wr_file_d  1   307.40 518.70\n",
            "- wr_rank_2  1   307.55 518.85\n",
            "- wr_file_b  1   307.57 518.86\n",
            "- wr_file_g  1   307.90 519.20\n",
            "- wk_rank_3  1   307.97 519.27\n",
            "- wr_rank_1  1   308.16 519.45\n",
            "- wr_file_e  1   308.41 519.70\n",
            "- wr_file_a  1   308.54 519.84\n",
            "- bk_file_b  1   308.61 519.90\n",
            "- wr_rank_7  1   309.50 520.80\n",
            "- wr_file_c  1   309.67 520.97\n",
            "- wk_rank_2  1   310.34 521.64\n",
            "- bk_rank_4  1   310.58 521.88\n",
            "- bk_rank_6  1   310.88 522.18\n",
            "- bk_file_a  1   311.22 522.51\n",
            "- wr_rank_5  1   311.57 522.86\n",
            "- wr_rank_3  1   313.41 524.70\n",
            "<none>           307.33 524.84\n",
            "- wk_file_c  1   313.96 525.26\n",
            "- bk_rank_7  1   316.55 527.84\n",
            "- wk_file_a  1   317.14 528.44\n",
            "- bk_rank_5  1   317.43 528.72\n",
            "- wk_file_b  1   319.56 530.85\n",
            "- bk_rank_2  1   321.53 532.83\n",
            "- wr_rank_4  1   321.59 532.89\n",
            "- bk_file_g  1   325.45 536.75\n",
            "- wk_rank_1  1   326.10 537.39\n",
            "- bk_file_f  1   327.30 538.60\n",
            "- bk_file_c  1   329.91 541.20\n",
            "- bk_file_d  1   332.31 543.61\n",
            "- bk_file_e  1   335.65 546.94\n",
            "- bk_rank_1  1   388.57 599.87\n",
            "\n",
            "Step:  AIC=518.62\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + bk_rank_4 + \n",
            "    bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- bk_rank_3  1   307.34 512.42\n",
            "- wr_rank_6  1   307.34 512.43\n",
            "- wr_file_d  1   307.42 512.51\n",
            "- wr_rank_2  1   307.55 512.63\n",
            "- wr_file_b  1   307.66 512.74\n",
            "- wk_rank_3  1   307.97 513.06\n",
            "- wr_file_g  1   308.06 513.14\n",
            "- wr_rank_1  1   308.16 513.24\n",
            "- bk_file_b  1   308.61 513.69\n",
            "- wr_file_e  1   308.70 513.79\n",
            "- wr_file_a  1   309.00 514.08\n",
            "- wr_rank_7  1   309.51 514.59\n",
            "- wk_rank_2  1   310.34 515.42\n",
            "- wr_file_c  1   310.48 515.56\n",
            "- bk_rank_4  1   310.60 515.68\n",
            "- bk_rank_6  1   310.88 515.96\n",
            "- bk_file_a  1   311.25 516.33\n",
            "- wr_rank_5  1   311.57 516.65\n",
            "- wr_rank_3  1   313.41 518.49\n",
            "<none>           307.33 518.62\n",
            "- wk_file_c  1   313.96 519.05\n",
            "- bk_rank_7  1   316.65 521.73\n",
            "- wk_file_a  1   317.17 522.26\n",
            "- bk_rank_5  1   317.43 522.51\n",
            "- wk_file_b  1   319.56 524.64\n",
            "- wr_rank_4  1   321.60 526.68\n",
            "- bk_rank_2  1   321.67 526.75\n",
            "- bk_file_g  1   325.47 530.55\n",
            "- wk_rank_1  1   326.14 531.22\n",
            "- bk_file_f  1   327.32 532.41\n",
            "- bk_file_c  1   329.91 535.00\n",
            "- bk_file_d  1   332.31 537.39\n",
            "- bk_file_e  1   335.66 540.74\n",
            "- bk_rank_1  1   388.59 593.67\n",
            "\n",
            "Step:  AIC=512.42\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + \n",
            "    bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_6  1   307.36 506.23\n",
            "- wr_file_d  1   307.43 506.30\n",
            "- wr_rank_2  1   307.56 506.43\n",
            "- wr_file_b  1   307.68 506.55\n",
            "- wk_rank_3  1   308.01 506.87\n",
            "- wr_file_g  1   308.07 506.94\n",
            "- wr_rank_1  1   308.16 507.03\n",
            "- bk_file_b  1   308.61 507.48\n",
            "- wr_file_e  1   308.71 507.58\n",
            "- wr_file_a  1   309.00 507.87\n",
            "- wr_rank_7  1   309.51 508.38\n",
            "- wk_rank_2  1   310.43 509.29\n",
            "- wr_file_c  1   310.48 509.35\n",
            "- bk_file_a  1   311.38 510.24\n",
            "- wr_rank_5  1   311.57 510.44\n",
            "- bk_rank_4  1   311.88 510.74\n",
            "- bk_rank_6  1   311.99 510.85\n",
            "- wr_rank_3  1   313.41 512.28\n",
            "<none>           307.34 512.42\n",
            "- wk_file_c  1   313.99 512.86\n",
            "- wk_file_a  1   317.25 516.11\n",
            "- bk_rank_7  1   319.24 518.11\n",
            "- wk_file_b  1   319.56 518.43\n",
            "- bk_rank_5  1   320.85 519.72\n",
            "- wr_rank_4  1   321.62 520.48\n",
            "- bk_file_g  1   325.48 524.34\n",
            "- bk_rank_2  1   326.20 525.07\n",
            "- wk_rank_1  1   326.58 525.45\n",
            "- bk_file_f  1   327.33 526.20\n",
            "- bk_file_c  1   330.38 529.25\n",
            "- bk_file_d  1   332.57 531.44\n",
            "- bk_file_e  1   335.66 534.53\n",
            "- bk_rank_1  1   414.00 612.87\n",
            "\n",
            "Step:  AIC=506.23\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + \n",
            "    bk_file_c + bk_file_d + bk_file_e + bk_file_f + bk_file_g + \n",
            "    bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + \n",
            "    bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_d  1   307.45 500.10\n",
            "- wr_file_b  1   307.70 500.35\n",
            "- wr_rank_2  1   307.80 500.46\n",
            "- wk_rank_3  1   308.03 500.69\n",
            "- wr_file_g  1   308.10 500.75\n",
            "- bk_file_b  1   308.64 501.29\n",
            "- wr_file_e  1   308.72 501.37\n",
            "- wr_rank_1  1   308.79 501.44\n",
            "- wr_file_a  1   309.02 501.68\n",
            "- wk_rank_2  1   310.46 503.12\n",
            "- wr_file_c  1   310.48 503.13\n",
            "- wr_rank_7  1   311.03 503.69\n",
            "- bk_file_a  1   311.41 504.06\n",
            "- bk_rank_4  1   311.88 504.53\n",
            "- bk_rank_6  1   312.00 504.65\n",
            "<none>           307.36 506.23\n",
            "- wk_file_c  1   314.01 506.66\n",
            "- wr_rank_5  1   314.60 507.26\n",
            "- wr_rank_3  1   317.14 509.79\n",
            "- wk_file_a  1   317.28 509.93\n",
            "- bk_rank_7  1   319.26 511.91\n",
            "- wk_file_b  1   319.57 512.22\n",
            "- bk_rank_5  1   320.87 513.53\n",
            "- bk_file_g  1   325.48 518.13\n",
            "- bk_rank_2  1   326.21 518.86\n",
            "- wk_rank_1  1   326.61 519.27\n",
            "- bk_file_f  1   327.33 519.99\n",
            "- wr_rank_4  1   327.59 520.24\n",
            "- bk_file_c  1   330.52 523.17\n",
            "- bk_file_d  1   332.58 525.23\n",
            "- bk_file_e  1   335.89 528.55\n",
            "- bk_rank_1  1   414.01 606.66\n",
            "\n",
            "Step:  AIC=500.1\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_e + \n",
            "    wr_file_g + wr_rank_1 + wr_rank_2 + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_2  1   307.87 494.31\n",
            "- wr_file_b  1   307.97 494.41\n",
            "- wk_rank_3  1   308.18 494.62\n",
            "- wr_file_g  1   308.41 494.85\n",
            "- wr_file_e  1   308.73 495.17\n",
            "- bk_file_b  1   308.74 495.18\n",
            "- wr_rank_1  1   308.84 495.28\n",
            "- wr_file_a  1   309.05 495.49\n",
            "- wk_rank_2  1   310.53 496.97\n",
            "- wr_rank_7  1   311.05 497.49\n",
            "- wr_file_c  1   311.39 497.83\n",
            "- bk_file_a  1   311.44 497.88\n",
            "- bk_rank_4  1   312.04 498.48\n",
            "- bk_rank_6  1   312.15 498.59\n",
            "<none>           307.45 500.10\n",
            "- wk_file_c  1   314.45 500.89\n",
            "- wr_rank_5  1   314.65 501.09\n",
            "- wr_rank_3  1   317.20 503.64\n",
            "- wk_file_a  1   317.41 503.85\n",
            "- bk_rank_7  1   319.28 505.72\n",
            "- wk_file_b  1   319.64 506.07\n",
            "- bk_rank_5  1   321.04 507.48\n",
            "- bk_file_g  1   325.48 511.92\n",
            "- bk_rank_2  1   326.22 512.66\n",
            "- wk_rank_1  1   326.78 513.22\n",
            "- bk_file_f  1   327.37 513.80\n",
            "- wr_rank_4  1   327.59 514.03\n",
            "- bk_file_c  1   330.53 516.97\n",
            "- bk_file_d  1   332.65 519.09\n",
            "- bk_file_e  1   336.05 522.49\n",
            "- bk_rank_1  1   414.05 600.49\n",
            "\n",
            "Step:  AIC=494.31\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_e + \n",
            "    wr_file_g + wr_rank_1 + wr_rank_3 + wr_rank_4 + wr_rank_5 + \n",
            "    wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + bk_file_d + \n",
            "    bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_b  1   308.45 488.68\n",
            "- wk_rank_3  1   308.66 488.88\n",
            "- wr_file_g  1   308.84 489.06\n",
            "- wr_rank_1  1   308.88 489.11\n",
            "- wr_file_e  1   309.07 489.30\n",
            "- bk_file_b  1   309.37 489.60\n",
            "- wr_file_a  1   309.44 489.67\n",
            "- wr_rank_7  1   311.06 491.28\n",
            "- wk_rank_2  1   311.16 491.38\n",
            "- bk_file_a  1   311.68 491.91\n",
            "- wr_file_c  1   311.74 491.96\n",
            "- bk_rank_4  1   312.09 492.32\n",
            "- bk_rank_6  1   312.43 492.65\n",
            "<none>           307.87 494.31\n",
            "- wr_rank_5  1   314.86 495.08\n",
            "- wk_file_c  1   315.00 495.22\n",
            "- wr_rank_3  1   317.58 497.81\n",
            "- wk_file_a  1   317.88 498.10\n",
            "- bk_rank_7  1   319.77 499.99\n",
            "- wk_file_b  1   320.23 500.45\n",
            "- bk_rank_5  1   321.35 501.57\n",
            "- bk_rank_2  1   326.45 506.68\n",
            "- bk_file_g  1   327.41 507.64\n",
            "- wk_rank_1  1   327.47 507.69\n",
            "- bk_file_f  1   328.08 508.30\n",
            "- wr_rank_4  1   328.72 508.94\n",
            "- bk_file_c  1   331.59 511.81\n",
            "- bk_file_d  1   333.18 513.41\n",
            "- bk_file_e  1   336.69 516.92\n",
            "- bk_rank_1  1   415.47 595.69\n",
            "\n",
            "Step:  AIC=488.68\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_c + wr_file_e + wr_file_g + \n",
            "    wr_rank_1 + wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + \n",
            "    bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_g  1   309.12 483.13\n",
            "- wk_rank_3  1   309.27 483.28\n",
            "- wr_rank_1  1   309.40 483.41\n",
            "- bk_file_b  1   310.10 484.11\n",
            "- wr_file_e  1   310.28 484.29\n",
            "- wr_file_a  1   310.86 484.87\n",
            "- wr_rank_7  1   311.56 485.57\n",
            "- wk_rank_2  1   311.72 485.72\n",
            "- wr_file_c  1   311.77 485.78\n",
            "- bk_file_a  1   312.15 486.16\n",
            "- bk_rank_4  1   313.01 487.02\n",
            "- bk_rank_6  1   313.13 487.14\n",
            "<none>           308.45 488.68\n",
            "- wk_file_c  1   315.42 489.43\n",
            "- wr_rank_5  1   315.49 489.50\n",
            "- wk_file_a  1   318.01 492.02\n",
            "- wr_rank_3  1   318.49 492.49\n",
            "- wk_file_b  1   320.48 494.49\n",
            "- bk_rank_7  1   321.65 495.66\n",
            "- bk_rank_5  1   321.84 495.85\n",
            "- bk_rank_2  1   327.15 501.16\n",
            "- wk_rank_1  1   328.24 502.25\n",
            "- bk_file_f  1   329.30 503.31\n",
            "- wr_rank_4  1   329.46 503.46\n",
            "- bk_file_g  1   329.58 503.59\n",
            "- bk_file_c  1   333.22 507.23\n",
            "- bk_file_d  1   334.41 508.42\n",
            "- bk_file_e  1   337.73 511.74\n",
            "- bk_rank_1  1   415.75 589.76\n",
            "\n",
            "Step:  AIC=483.13\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_c + wr_file_e + wr_rank_1 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + \n",
            "    bk_file_b + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + \n",
            "    bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wk_rank_3  1   309.87 477.67\n",
            "- wr_rank_1  1   310.03 477.82\n",
            "- bk_file_b  1   310.94 478.73\n",
            "- wr_file_e  1   311.45 479.25\n",
            "- wr_file_a  1   312.00 479.80\n",
            "- wr_file_c  1   312.01 479.80\n",
            "- wk_rank_2  1   312.12 479.91\n",
            "- wr_rank_7  1   312.13 479.92\n",
            "- bk_file_a  1   312.67 480.46\n",
            "- bk_rank_6  1   313.26 481.05\n",
            "- bk_rank_4  1   313.60 481.39\n",
            "<none>           309.12 483.13\n",
            "- wk_file_c  1   316.22 484.01\n",
            "- wr_rank_5  1   316.63 484.42\n",
            "- wk_file_a  1   319.19 486.99\n",
            "- wr_rank_3  1   319.36 487.15\n",
            "- wk_file_b  1   321.09 488.89\n",
            "- bk_rank_5  1   322.29 490.09\n",
            "- bk_rank_7  1   322.71 490.51\n",
            "- bk_rank_2  1   328.10 495.89\n",
            "- wk_rank_1  1   328.41 496.21\n",
            "- wr_rank_4  1   330.10 497.89\n",
            "- bk_file_f  1   330.80 498.59\n",
            "- bk_file_g  1   331.15 498.94\n",
            "- bk_file_c  1   333.65 501.44\n",
            "- bk_file_d  1   335.81 503.60\n",
            "- bk_file_e  1   339.56 507.35\n",
            "- bk_rank_1  1   416.52 584.31\n",
            "\n",
            "Step:  AIC=477.67\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_1 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + \n",
            "    bk_file_c + bk_file_d + bk_file_e + bk_file_f + bk_file_g + \n",
            "    bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + \n",
            "    bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_1  1   310.71 472.29\n",
            "- bk_file_b  1   312.31 473.89\n",
            "- wk_rank_2  1   312.50 474.08\n",
            "- wr_file_e  1   312.60 474.18\n",
            "- wr_file_c  1   312.79 474.37\n",
            "- wr_rank_7  1   312.81 474.39\n",
            "- wr_file_a  1   312.90 474.48\n",
            "- bk_file_a  1   313.14 474.72\n",
            "- bk_rank_6  1   314.09 475.67\n",
            "- bk_rank_4  1   314.26 475.84\n",
            "<none>           309.87 477.67\n",
            "- wr_rank_5  1   317.36 478.94\n",
            "- wk_file_c  1   318.41 479.99\n",
            "- wr_rank_3  1   319.93 481.51\n",
            "- wk_file_a  1   320.41 481.99\n",
            "- wk_file_b  1   322.55 484.13\n",
            "- bk_rank_5  1   322.98 484.56\n",
            "- bk_rank_7  1   324.27 485.85\n",
            "- bk_rank_2  1   329.36 490.94\n",
            "- wr_rank_4  1   330.86 492.44\n",
            "- bk_file_g  1   331.90 493.48\n",
            "- bk_file_f  1   333.18 494.76\n",
            "- bk_file_c  1   335.62 497.20\n",
            "- bk_file_d  1   337.98 499.56\n",
            "- wk_rank_1  1   339.64 501.22\n",
            "- bk_file_e  1   342.42 504.00\n",
            "- bk_rank_1  1   418.58 580.16\n",
            "\n",
            "Step:  AIC=472.29\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_rank_7  1   313.10 468.46\n",
            "- wk_rank_2  1   313.49 468.85\n",
            "- wr_file_c  1   313.52 468.89\n",
            "- wr_file_e  1   313.57 468.93\n",
            "- wr_file_a  1   313.69 469.05\n",
            "- bk_file_b  1   313.70 469.07\n",
            "- bk_file_a  1   313.91 469.27\n",
            "- bk_rank_4  1   315.06 470.42\n",
            "- bk_rank_6  1   315.32 470.69\n",
            "<none>           310.71 472.29\n",
            "- wr_rank_5  1   317.42 472.79\n",
            "- wk_file_c  1   319.33 474.70\n",
            "- wr_rank_3  1   319.94 475.30\n",
            "- wk_file_a  1   320.61 475.97\n",
            "- bk_rank_5  1   323.23 478.60\n",
            "- wk_file_b  1   323.36 478.73\n",
            "- bk_rank_7  1   325.77 481.14\n",
            "- bk_rank_2  1   329.77 485.13\n",
            "- wr_rank_4  1   330.91 486.27\n",
            "- bk_file_g  1   333.71 489.07\n",
            "- bk_file_f  1   334.74 490.11\n",
            "- bk_file_c  1   337.63 492.99\n",
            "- bk_file_d  1   339.60 494.97\n",
            "- wk_rank_1  1   340.85 496.22\n",
            "- bk_file_e  1   344.00 499.37\n",
            "- bk_rank_1  1   418.79 574.15\n",
            "\n",
            "Step:  AIC=468.46\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + bk_file_a + bk_file_b + bk_file_c + bk_file_d + \n",
            "    bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- bk_file_b  1   315.45 464.60\n",
            "- wr_file_e  1   315.62 464.77\n",
            "- wr_file_c  1   315.76 464.91\n",
            "- wk_rank_2  1   315.86 465.01\n",
            "- wr_file_a  1   316.15 465.30\n",
            "- bk_file_a  1   316.23 465.38\n",
            "- bk_rank_4  1   317.60 466.75\n",
            "- bk_rank_6  1   318.11 467.26\n",
            "- wr_rank_5  1   318.32 467.47\n",
            "<none>           313.10 468.46\n",
            "- wr_rank_3  1   320.75 469.90\n",
            "- wk_file_c  1   322.01 471.16\n",
            "- wk_file_a  1   323.96 473.11\n",
            "- wk_file_b  1   326.21 475.36\n",
            "- bk_rank_5  1   326.53 475.68\n",
            "- bk_rank_7  1   328.13 477.28\n",
            "- wr_rank_4  1   331.21 480.36\n",
            "- bk_rank_2  1   331.60 480.75\n",
            "- bk_file_g  1   335.38 484.53\n",
            "- bk_file_f  1   336.34 485.49\n",
            "- bk_file_c  1   339.07 488.22\n",
            "- bk_file_d  1   341.47 490.62\n",
            "- wk_rank_1  1   342.43 491.58\n",
            "- bk_file_e  1   344.87 494.02\n",
            "- bk_rank_1  1   421.05 570.20\n",
            "\n",
            "Step:  AIC=464.6\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_file_e + wr_rank_3 + wr_rank_4 + \n",
            "    wr_rank_5 + bk_file_a + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + \n",
            "    bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_e  1   318.08 461.01\n",
            "- wr_file_c  1   318.14 461.08\n",
            "- wk_rank_2  1   318.79 461.72\n",
            "- wr_file_a  1   318.89 461.82\n",
            "- bk_rank_4  1   319.77 462.71\n",
            "- bk_rank_6  1   320.33 463.26\n",
            "- wr_rank_5  1   320.56 463.49\n",
            "<none>           315.45 464.60\n",
            "- bk_file_a  1   322.22 465.16\n",
            "- wr_rank_3  1   322.90 465.84\n",
            "- wk_file_c  1   323.12 466.05\n",
            "- wk_file_a  1   325.19 468.12\n",
            "- wk_file_b  1   327.70 470.64\n",
            "- bk_rank_5  1   329.03 471.96\n",
            "- bk_rank_7  1   329.18 472.11\n",
            "- wr_rank_4  1   333.88 476.81\n",
            "- bk_rank_2  1   335.06 477.99\n",
            "- bk_file_g  1   335.76 478.69\n",
            "- bk_file_f  1   336.49 479.42\n",
            "- bk_file_c  1   339.07 482.01\n",
            "- bk_file_d  1   341.64 484.57\n",
            "- wk_rank_1  1   345.17 488.10\n",
            "- bk_file_e  1   345.19 488.12\n",
            "- bk_rank_1  1   425.77 568.71\n",
            "\n",
            "Step:  AIC=461.01\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_a + wr_file_c + wr_rank_3 + wr_rank_4 + wr_rank_5 + \n",
            "    bk_file_a + bk_file_c + bk_file_d + bk_file_e + bk_file_f + \n",
            "    bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + \n",
            "    bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_a  1   320.59 457.31\n",
            "- wk_rank_2  1   321.27 457.99\n",
            "- wr_file_c  1   321.97 458.69\n",
            "- bk_rank_4  1   322.68 459.40\n",
            "- wr_rank_5  1   323.44 460.16\n",
            "- bk_rank_6  1   323.45 460.17\n",
            "<none>           318.08 461.01\n",
            "- bk_file_a  1   324.43 461.16\n",
            "- wk_file_c  1   325.50 462.22\n",
            "- wr_rank_3  1   325.61 462.33\n",
            "- wk_file_a  1   328.06 464.78\n",
            "- wk_file_b  1   329.37 466.09\n",
            "- bk_rank_7  1   330.97 467.69\n",
            "- bk_rank_5  1   332.37 469.09\n",
            "- wr_rank_4  1   334.63 471.35\n",
            "- bk_file_g  1   337.72 474.44\n",
            "- bk_rank_2  1   338.40 475.13\n",
            "- bk_file_f  1   338.53 475.25\n",
            "- bk_file_c  1   340.79 477.51\n",
            "- bk_file_d  1   344.08 480.80\n",
            "- wk_rank_1  1   347.19 483.91\n",
            "- bk_file_e  1   348.09 484.81\n",
            "- bk_rank_1  1   426.96 563.69\n",
            "\n",
            "Step:  AIC=457.31\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wr_file_c + wr_rank_3 + wr_rank_4 + wr_rank_5 + bk_file_a + \n",
            "    bk_file_c + bk_file_d + bk_file_e + bk_file_f + bk_file_g + \n",
            "    bk_rank_1 + bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + \n",
            "    bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wk_rank_2  1   323.32 453.83\n",
            "- bk_rank_4  1   324.47 454.97\n",
            "- bk_rank_6  1   325.33 455.84\n",
            "- wr_file_c  1   325.47 455.97\n",
            "- wr_rank_5  1   326.69 457.19\n",
            "<none>           320.59 457.31\n",
            "- bk_file_a  1   327.01 457.52\n",
            "- wk_file_c  1   328.43 458.93\n",
            "- wr_rank_3  1   328.82 459.33\n",
            "- wk_file_a  1   330.93 461.43\n",
            "- wk_file_b  1   332.45 462.96\n",
            "- bk_rank_7  1   333.25 463.75\n",
            "- bk_rank_5  1   334.40 464.90\n",
            "- wr_rank_4  1   338.48 468.99\n",
            "- bk_file_g  1   341.06 471.57\n",
            "- bk_rank_2  1   341.21 471.72\n",
            "- bk_file_f  1   341.34 471.85\n",
            "- bk_file_c  1   342.05 472.56\n",
            "- bk_file_d  1   345.30 475.81\n",
            "- wk_rank_1  1   348.10 478.61\n",
            "- bk_file_e  1   349.68 480.19\n",
            "- bk_rank_1  1   429.99 560.50\n",
            "\n",
            "Step:  AIC=453.83\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wr_file_c + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + bk_file_a + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- bk_rank_4  1   326.57 450.86\n",
            "- bk_rank_6  1   327.94 452.24\n",
            "- wr_file_c  1   328.17 452.46\n",
            "- bk_file_a  1   328.80 453.09\n",
            "<none>           323.32 453.83\n",
            "- wr_rank_5  1   329.86 454.16\n",
            "- wr_rank_3  1   331.57 455.86\n",
            "- wk_file_c  1   334.28 458.58\n",
            "- wk_file_a  1   335.09 459.38\n",
            "- bk_rank_7  1   335.22 459.51\n",
            "- bk_rank_5  1   336.78 461.07\n",
            "- wk_file_b  1   339.41 463.70\n",
            "- wr_rank_4  1   341.23 465.52\n",
            "- bk_file_f  1   344.08 468.38\n",
            "- bk_file_g  1   344.15 468.44\n",
            "- bk_rank_2  1   344.58 468.87\n",
            "- bk_file_c  1   346.26 470.56\n",
            "- bk_file_d  1   349.49 473.79\n",
            "- wk_rank_1  1   349.70 474.00\n",
            "- bk_file_e  1   354.25 478.54\n",
            "- bk_rank_1  1   437.81 562.10\n",
            "\n",
            "Step:  AIC=450.86\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wr_file_c + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + bk_file_a + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- bk_rank_6  1   329.55 447.62\n",
            "- wr_file_c  1   331.19 449.26\n",
            "<none>           326.57 450.86\n",
            "- bk_file_a  1   333.11 451.19\n",
            "- wr_rank_5  1   333.56 451.64\n",
            "- wr_rank_3  1   334.78 452.86\n",
            "- bk_rank_7  1   336.05 454.13\n",
            "- bk_rank_5  1   337.46 455.53\n",
            "- wk_file_c  1   338.07 456.15\n",
            "- wk_file_a  1   339.17 457.24\n",
            "- wk_file_b  1   342.89 460.96\n",
            "- wr_rank_4  1   345.14 463.22\n",
            "- bk_file_f  1   347.22 465.30\n",
            "- bk_file_g  1   347.92 466.00\n",
            "- bk_file_c  1   349.17 467.25\n",
            "- bk_file_d  1   351.62 469.70\n",
            "- wk_rank_1  1   351.67 469.75\n",
            "- bk_file_e  1   356.94 475.02\n",
            "- bk_rank_2  1   357.50 475.58\n",
            "- bk_rank_1  1   473.07 591.15\n",
            "\n",
            "Step:  AIC=447.62\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wr_file_c + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + bk_file_a + bk_file_c + \n",
            "    bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
            "    bk_rank_2 + bk_rank_5 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "- wr_file_c  1   334.44 446.30\n",
            "<none>           329.55 447.62\n",
            "- wr_rank_5  1   336.50 448.37\n",
            "- bk_file_a  1   337.16 449.02\n",
            "- bk_rank_7  1   337.45 449.31\n",
            "- wr_rank_3  1   337.90 449.77\n",
            "- bk_rank_5  1   338.81 450.67\n",
            "- wk_file_c  1   340.89 452.76\n",
            "- wk_file_a  1   342.06 453.92\n",
            "- wk_file_b  1   346.37 458.24\n",
            "- wr_rank_4  1   348.54 460.40\n",
            "- bk_file_f  1   350.12 461.98\n",
            "- bk_file_g  1   350.46 462.32\n",
            "- bk_file_c  1   351.25 463.11\n",
            "- bk_file_d  1   354.23 466.09\n",
            "- wk_rank_1  1   354.77 466.63\n",
            "- bk_file_e  1   362.48 474.35\n",
            "- bk_rank_2  1   367.29 479.15\n",
            "- bk_rank_1  1   497.63 609.49\n",
            "\n",
            "Step:  AIC=446.3\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wr_rank_3 + \n",
            "    wr_rank_4 + wr_rank_5 + bk_file_a + bk_file_c + bk_file_d + \n",
            "    bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + \n",
            "    bk_rank_5 + bk_rank_7\n",
            "\n",
            "            Df Deviance    AIC\n",
            "<none>           334.44 446.30\n",
            "- wr_rank_5  1   341.90 447.55\n",
            "- wr_rank_3  1   342.75 448.40\n",
            "- bk_file_a  1   343.20 448.84\n",
            "- bk_rank_7  1   343.50 449.15\n",
            "- bk_rank_5  1   343.76 449.41\n",
            "- wk_file_a  1   345.96 451.61\n",
            "- wk_file_c  1   346.56 452.20\n",
            "- wr_rank_4  1   351.97 457.61\n",
            "- wk_file_b  1   352.87 458.52\n",
            "- bk_file_f  1   355.47 461.12\n",
            "- bk_file_g  1   355.86 461.51\n",
            "- bk_file_c  1   357.02 462.67\n",
            "- bk_file_d  1   357.46 463.11\n",
            "- wk_rank_1  1   357.57 463.22\n",
            "- bk_file_e  1   365.31 470.96\n",
            "- bk_rank_2  1   369.05 474.70\n",
            "- bk_rank_1  1   498.26 603.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "8649648dadffe5d963d8152dd2aec2a4",
          "grade": false,
          "grade_id": "cell-875723adb3d08f35",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "scrolled": true,
        "id": "uRVpi8nuj11s",
        "outputId": "8310ec9e-1ba2-4f97-f2db-10a5453d2549"
      },
      "source": [
        "evaluate(fit.aic, df.processed$result)\n",
        "evaluate(fit.bic, df.processed$result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.86"
            ],
            "text/latex": "0.86",
            "text/markdown": "0.86",
            "text/plain": [
              "[1] 0.86"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.856"
            ],
            "text/latex": "0.856",
            "text/markdown": "0.856",
            "text/plain": [
              "[1] 0.856"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "8c2bec9aeb80b8bc954c440f64e87273",
          "grade": false,
          "grade_id": "cell-612455e9b9a7631c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "dbTUwZudj11s"
      },
      "source": [
        "<h2>Question 2.e (2 marks)</h2>\n",
        "The step-wise AIC method, if implemented correctly, should find the formula\n",
        "\n",
        "    result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
        "        wr_file_a + wr_file_c + wr_file_e + wr_rank_3 + wr_rank_4 + \n",
        "        wr_rank_5 + wr_rank_7 + bk_file_a + bk_file_b + bk_file_c + \n",
        "        bk_file_d + bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + \n",
        "        bk_rank_2 + bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
        "    \n",
        "with accuracy at around 0.86.\n",
        "\n",
        "The step-wise BIC method, if implemented correctly, should find the formula\n",
        "\n",
        "    result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wr_rank_3 + \n",
        "        wr_rank_4 + wr_rank_5 + bk_file_a + bk_file_c + bk_file_d + \n",
        "        bk_file_e + bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + \n",
        "        bk_rank_5 + bk_rank_7\n",
        "\n",
        "with accuracy at around 0.856.\n",
        "\n",
        "Given all this information, were any of your colleagues right? Or were they all wrong? Also, can you draw a conclusion from the formula found by BIC as to which pieces are important to have in the right position (out of the white king, the black king or the white rook)? Be sure to justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9b4bedbd016ce2c415230e061995c6c6",
          "grade": false,
          "grade_id": "cell-3a845524cfba567a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Z-6NtXkDj11t"
      },
      "source": [
        "<h2>Question 2.f (2 marks)</h2>\n",
        "You have finished your model, and have decided to use the AIC model (which should be saved as a variable named \"fit.aic\"). You now want to apply your model to the chess position you find yourself in against your colleague.\n",
        "\n",
        "You are in the following position (it's your move, i.e. black's move):\n",
        "\n",
        "<img src=\"chess_pos.png\" alt=\"drawing\" width=\"200\"/>\n",
        "<!--![Chess Position](chess_pos.png)-->\n",
        "\n",
        "<i>Note: the above image will only show if you include the file \"chess_pos.png\" (contained in the \"data.zip\" file you've been given on Moodle) in the same folder as this Jupyter notebook, and then re-run this Markdown cell by putting it into edit mode and then running it. Alternatively, you can just look at the file itself.</i>\n",
        "\n",
        "This position has the following variable values:\n",
        " \n",
        " - wk_file = c\n",
        " - wk_rank = 3\n",
        " - wr_file = f\n",
        " - wr_rank = 5\n",
        " - bk_file = e\n",
        " - bk_rank = 5\n",
        "\n",
        "You will need to convert this position into a dataframe of one row with the one-hot encoding we used above, and then pass this into the predict function using the model \"fit.aic\" and using type=\"response\" for the predict function to get a probability.\n",
        "\n",
        "Do this in the cell below and see what result you get. Then, in the Markdown cell below that, state whether your model indicates that you truly should resign in this position (i.e. the position is lost) or whether your opponent was bluffing (i.e. the position does not seem to be winnable in less than 10 moves).\n",
        "\n",
        "<b>Note: you might be tempted to create a simple dataframe with one row and convert this to one-hot encoding using the functions you have defined above - this probably won't work, and we recommend that you create the dataframe using some other method. If you're curious, try to think about why this is the case.</b>\n",
        "\n",
        "<b>Hint: This Stack-Overflow post might help: https://stackoverflow.com/questions/10689055/create-an-empty-data-frame. In particular, consider how to use the line\n",
        "    \n",
        "    empty_df <- df[FALSE,]\n",
        "    \n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "45b443e3e77329072e40fbe1925885c5",
          "grade": true,
          "grade_id": "cell-6392aa38b567d392",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "H5Cti6Chj11t",
        "outputId": "a04f624f-c454-4ef8-8a6e-952448f75306"
      },
      "source": [
        "empty_df<-df.processed[FALSE,]\n",
        "empty_df[1,]<-c(0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0)\n",
        "fit.aic_1<-to.stepwise(empty_df[1,],2)\n",
        "pr<-predict(fit.aic_1,type=\"response\")\n",
        "pr<-ifelse(pr > 0.5,1,0)\n",
        "pr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6 + bk_rank_7\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4 + bk_rank_5 + bk_rank_6\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4 + bk_rank_5\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3 + \n",
            "    bk_rank_4\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2 + bk_rank_3\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1 + bk_rank_2\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g + bk_rank_1\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f + bk_file_g\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e + \n",
            "    bk_file_f\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d + bk_file_e\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c + bk_file_d\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b + bk_file_c\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a + bk_file_b\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7 + \n",
            "    bk_file_a\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6 + wr_rank_7\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5 + wr_rank_6\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4 + wr_rank_5\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3 + wr_rank_4\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2 + \n",
            "    wr_rank_3\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1 + wr_rank_2\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g + wr_rank_1\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f + wr_file_g\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e + wr_file_f\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d + \n",
            "    wr_file_e\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c + wr_file_d\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b + wr_file_c\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a + wr_file_b\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3 + wr_file_a\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2 + \n",
            "    wk_rank_3\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1 + wk_rank_2\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c + wk_rank_1\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b + wk_file_c\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a + wk_file_b\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ wk_file_a\n",
            "\n",
            "\n",
            "Step:  AIC=2\n",
            "result ~ 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<strong>1:</strong> 0"
            ],
            "text/latex": "\\textbf{1:} 0",
            "text/markdown": "**1:** 0",
            "text/plain": [
              "1 \n",
              "0 "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkiBun9ej11u"
      },
      "source": [
        "<h2>Question 2.g (2 marks)</h2>\n",
        "In our code above (and also in the code in Question 3), we train our models using all of our training data, and also evaluate the models on our training data. Describe a problem with this approach, and give an example of how we could fix this (other than leave-one-out cross-validation, which is used in Question 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "165b1f6617c65d889736865d5090d8a6",
          "grade": true,
          "grade_id": "cell-0986dd1be543302c",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "gp5DRsWej11u"
      },
      "source": [
        "The modelwhen trained on only training data will often predict only similar data set and will perform poorly as it is not a  generalised model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a864cdae890795ca750e8c12ef91b5cb",
          "grade": false,
          "grade_id": "cell-decf4f2de1336b89",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "VbSqg92dj11u"
      },
      "source": [
        "<h2>Question 3 - Linear Regression (20 marks)</h2>\n",
        "\n",
        "Just as you give your opponent the finishing move that draws your chess game (which is no small feat against an Australian champion - you have the admiration of your colleagues), the engineers come running into your office in a panic. They tell you that their deadline for finishing their 3D printing process is tomorrow, but that they are still having trouble solving a problem with their printing process.\n",
        "\n",
        "They have printed 50 test cases, with varying degrees of success. They have found that a large number of their cases turn out very rough to the touch; they are concerned that management will see this and scrap the 3D printing project altogether (and their jobs with it!).\n",
        "\n",
        "You calmly ask them to send over all the data they have, and get to work developing a linear regression model to try to identify which component(s) of the process are causing the roughness problem.\n",
        "\n",
        "<b>Important Note: In this question, you might see the error message \"prediction from a rank-deficient fit may be misleading...\" show up. You can ignore this for the purposes of this assignment (try to think why we might be seeing a \"rank-deficient\" fit - there's a discussion at https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it if you're curious).</b>\n",
        "\n",
        "<b>Background information</b> This dataset is derived from a real dataset, which was uploaded to Kaggle on 9/14/2018 by Ahmet Okudan. The Kaggle upload indicates that the dataset \"comes from research by TR/Selcuk University Mechanical Engineering department\". More information can be found at https://www.kaggle.com/afumetto/3dprinter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "442446b21c9c58b0577c7a67280104a3",
          "grade": false,
          "grade_id": "cell-01499d7d0f9eda2c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "qjTgwaQSj11v",
        "outputId": "014fef85-ff18-4285-ddc4-3bee1463175f"
      },
      "source": [
        "data.3d <- read.csv(\"data.csv\")\n",
        "data.3d <- data.3d[,-c(11, 12)]\n",
        "head(data.3d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead><tr><th scope=col>layer_height</th><th scope=col>wall_thickness</th><th scope=col>infill_density</th><th scope=col>infill_pattern</th><th scope=col>nozzle_temperature</th><th scope=col>bed_temperature</th><th scope=col>print_speed</th><th scope=col>material</th><th scope=col>fan_speed</th><th scope=col>roughness</th></tr></thead>\n",
              "<tbody>\n",
              "\t<tr><td>0.02     </td><td> 8       </td><td>90       </td><td>grid     </td><td>220      </td><td>60       </td><td>40       </td><td>abs      </td><td>  0      </td><td>25       </td></tr>\n",
              "\t<tr><td>0.02     </td><td> 7       </td><td>90       </td><td>honeycomb</td><td>225      </td><td>65       </td><td>40       </td><td>abs      </td><td> 25      </td><td>32       </td></tr>\n",
              "\t<tr><td>0.02     </td><td> 1       </td><td>80       </td><td>grid     </td><td>230      </td><td>70       </td><td>40       </td><td>abs      </td><td> 50      </td><td>40       </td></tr>\n",
              "\t<tr><td>0.02     </td><td> 4       </td><td>70       </td><td>honeycomb</td><td>240      </td><td>75       </td><td>40       </td><td>abs      </td><td> 75      </td><td>68       </td></tr>\n",
              "\t<tr><td>0.02     </td><td> 6       </td><td>90       </td><td>grid     </td><td>250      </td><td>80       </td><td>40       </td><td>abs      </td><td>100      </td><td>92       </td></tr>\n",
              "\t<tr><td>0.02     </td><td>10       </td><td>40       </td><td>honeycomb</td><td>200      </td><td>60       </td><td>40       </td><td>pla      </td><td>  0      </td><td>60       </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/latex": "\\begin{tabular}{r|llllllllll}\n layer\\_height & wall\\_thickness & infill\\_density & infill\\_pattern & nozzle\\_temperature & bed\\_temperature & print\\_speed & material & fan\\_speed & roughness\\\\\n\\hline\n\t 0.02      &  8        & 90        & grid      & 220       & 60        & 40        & abs       &   0       & 25       \\\\\n\t 0.02      &  7        & 90        & honeycomb & 225       & 65        & 40        & abs       &  25       & 32       \\\\\n\t 0.02      &  1        & 80        & grid      & 230       & 70        & 40        & abs       &  50       & 40       \\\\\n\t 0.02      &  4        & 70        & honeycomb & 240       & 75        & 40        & abs       &  75       & 68       \\\\\n\t 0.02      &  6        & 90        & grid      & 250       & 80        & 40        & abs       & 100       & 92       \\\\\n\t 0.02      & 10        & 40        & honeycomb & 200       & 60        & 40        & pla       &   0       & 60       \\\\\n\\end{tabular}\n",
            "text/markdown": "\n| layer_height | wall_thickness | infill_density | infill_pattern | nozzle_temperature | bed_temperature | print_speed | material | fan_speed | roughness |\n|---|---|---|---|---|---|---|---|---|---|\n| 0.02      |  8        | 90        | grid      | 220       | 60        | 40        | abs       |   0       | 25        |\n| 0.02      |  7        | 90        | honeycomb | 225       | 65        | 40        | abs       |  25       | 32        |\n| 0.02      |  1        | 80        | grid      | 230       | 70        | 40        | abs       |  50       | 40        |\n| 0.02      |  4        | 70        | honeycomb | 240       | 75        | 40        | abs       |  75       | 68        |\n| 0.02      |  6        | 90        | grid      | 250       | 80        | 40        | abs       | 100       | 92        |\n| 0.02      | 10        | 40        | honeycomb | 200       | 60        | 40        | pla       |   0       | 60        |\n\n",
            "text/plain": [
              "  layer_height wall_thickness infill_density infill_pattern nozzle_temperature\n",
              "1 0.02          8             90             grid           220               \n",
              "2 0.02          7             90             honeycomb      225               \n",
              "3 0.02          1             80             grid           230               \n",
              "4 0.02          4             70             honeycomb      240               \n",
              "5 0.02          6             90             grid           250               \n",
              "6 0.02         10             40             honeycomb      200               \n",
              "  bed_temperature print_speed material fan_speed roughness\n",
              "1 60              40          abs        0       25       \n",
              "2 65              40          abs       25       32       \n",
              "3 70              40          abs       50       40       \n",
              "4 75              40          abs       75       68       \n",
              "5 80              40          abs      100       92       \n",
              "6 60              40          pla        0       60       "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYDWhhm2j11v",
        "outputId": "f2ca2e70-fb53-443a-f1a9-d9b9297e35bc"
      },
      "source": [
        "N<-nrow(data.3d)\n",
        "N"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "50"
            ],
            "text/latex": "50",
            "text/markdown": "50",
            "text/plain": [
              "[1] 50"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1bf08ff0b84cd7a1424ac3d71edce273",
          "grade": false,
          "grade_id": "cell-ac703a3f1c1429e6",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "emPnVdbej11v"
      },
      "source": [
        "<h2>Question 3.a (1 mark)</h2>\n",
        "\n",
        "First things first - we need to take a look at our data. Write a function \"plot.variables\" which takes two arguments - var.x and var.y. It then plots var.x vs var.y. Make sure your plots have a reasonable label (e.g. var.x vs var.y, substituting in the actual values of var.x and var.y) as well as reasonable x-axis and y-axis labels (e.g. var.x on the x-axis and var.y on the y-axis, again substituting the actual values of var.x and var.y).\n",
        "\n",
        "Your function should look like this:\n",
        "\n",
        "    plot.variables <- function(var.x, var.y) {\n",
        "        # Your code here\n",
        "    }\n",
        "    \n",
        "Then run the second code cell below, which will plot out all your other variables vs roughness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "16fb0897c07fa06f275a399fb373b977",
          "grade": true,
          "grade_id": "cell-18c20aa93f7a3a78",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "2JrkiryQj11w"
      },
      "source": [
        "plot.variables <- function(var.x, var.y) {\n",
        "    plot(data.3d[,var.x],data.3d[,var.y],col=\"red\",xlab = var.x,ylab = var.y)\n",
        "   \n",
        "}                                                                                   # your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "bac929d64d6c54846dd60f888c93961a",
          "grade": false,
          "grade_id": "cell-e9cd72dc1f039fab",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "clVjcM54j11w",
        "outputId": "4626f6b2-7a68-4f03-c6a5-afddd5d2fb5c"
      },
      "source": [
        "par(mfrow=c(3, 2))\n",
        "\n",
        "options(repr.plot.width=6, repr.plot.height=6)\n",
        "\n",
        "for (name in names(data.3d)) {\n",
        "    if (name != \"roughness\") {\n",
        "        plot.variables(name, \"roughness\")\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAALQCAMAAACOibeuAAAACVBMVEUAAAD/AAD///9nGWQe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\niv8AhG1cZAu4bqgAAAAASUVORK5CYII=",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAALQCAMAAACOibeuAAAACVBMVEUAAAD/AAD///9nGWQe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",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "446078456f0568359f55b3a1c2c3179c",
          "grade": false,
          "grade_id": "cell-d713612002a3ad19",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "z2Utin3jj11w"
      },
      "source": [
        "<h2>Question 3.b (1 mark)</h2>\n",
        "\n",
        "Now we have an idea of what our dataset looks like, we can start working on creating a linear model. Using R's built-in lm() function, create a linear model of roughness vs all the other variables in our dataset. Save this model in a variable called \"fit.lm\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "7ab4afa0b2395d87c55cb0f20f212a9a",
          "grade": true,
          "grade_id": "cell-9b11504a0cd00878",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "nu3XhjSqj11x"
      },
      "source": [
        "fit.lm<-lm(roughness ~.,data= data.3d)\n",
        "    # your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "dba740ae48a3ad83e4ec77490d60e044",
          "grade": false,
          "grade_id": "cell-55423954a58459d6",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "S1BK0iWMj11x",
        "outputId": "4d687c03-a74c-4859-c68d-bce846458d5d"
      },
      "source": [
        "fit.lm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = roughness ~ ., data = data.3d)\n",
              "\n",
              "Coefficients:\n",
              "            (Intercept)             layer_height           wall_thickness  \n",
              "             -2.371e+03                1.269e+03                2.334e+00  \n",
              "         infill_density  infill_patternhoneycomb       nozzle_temperature  \n",
              "             -4.231e-02               -1.255e-01                1.506e+01  \n",
              "        bed_temperature              print_speed              materialpla  \n",
              "             -1.613e+01                6.496e-01                2.985e+02  \n",
              "              fan_speed  \n",
              "                     NA  \n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "39eeca015a8598d93e1a85a81f479d0a",
          "grade": false,
          "grade_id": "cell-21a37ed806d75a72",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vPWpk0Sgj11x"
      },
      "source": [
        "<h2>Question 3.c (1 mark)</h2>\n",
        "\n",
        "It would be nice to get an idea of how our estimates compare with the true roughness values using a graph. In the code cell below, use R to create a plot of the predicted roughness (using our linear model) on the x-axis vs the actual roughness on the y-axis.\n",
        "\n",
        "Like in Question 3.a, make sure to use a reasonable label for both your plot and your axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "00cc0a66e82c41d0430c873587d87881",
          "grade": true,
          "grade_id": "cell-dbccf7395c230ad3",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "CUCf_RRej11y",
        "outputId": "38455d71-5597-4875-f139-9701a3062383"
      },
      "source": [
        "predicted_values <- predict(fit.lm,data.3d)\n",
        "plot(predicted_values,data.3d$roughness, col=\"red\")# your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(fit.lm, data.3d):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAALQCAMAAACOibeuAAAAM1BMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD///89ODILAAAACXBIWXMAABJ0\nAAASdAHeZh94AAAa6ElEQVR4nO3di3abuBpAYQkwxhgo7/+0NWAcfAOsK/zsb53VSadJ0Gl3\nGVnCoFpAEBV7AIBLBA1RCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQNUQgaohA0RCFoiELQEIWg\nIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUMUgoYoBA1RCBqiEDREIWiIQtAQhaAhCkFD\nFIKGKAQNUQgaohA0RCFoiELQEIWgIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUMUgoYo\nBA1RCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQNUQgaohA0RCFoiELQEIWgIQpBQxSChigEDVEI\nGqIQNEQhaIhC0BCFoCEKQUMUgoYoBA1RCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQNUQgaohA0\nRCFoiELQEIWgIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUMUgoYoAYJWgCGD2twHHOEQ\nkImgIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUMUgoYoBA1RCBoO/fv3L/IICBrO9DVH\nTpqg4cy/yY+xEDRc+ffyzygIGq4QdMxDwDmCjnkIuMccOuIh4B6rHBEPAR9Yh452CMhE0BCF\noOFV6EkIQcOj8C8TgwZ9PWf93Zqy/OrrENiU8At5AYNukskdyFIvh8C2RNhqCRh0rvSl6j+q\nS61yH4fAtngN+vPsPGDQWlWPjyulfRwC2+Ix6G+z84BBP93pdP62pwQthL859LfvzBkaHnlb\n5fh67g87hy7r/iPm0MfhaR16C0G36WSVI2m8HAIHsYmg22ver0Pr7Mw6NOxsYA69rUNg3zaw\nyrGtQ2Dvoq9D3xWJUlnp9RA4ruDr0PdXhrOLHAQNU6GDzlXetG2dq8LHIXB4oYPWql+va1Ti\n4xA4vNBBj1ve71vflo8DBTqhgz6NQbP1DR+CBp2di1Jdbh82OVvf8CJo0I/phFKarW/4EHId\nuqqKIsv6l4b5bM8EDVPsFEIUgoYoEYIutEpmt1UIGsaCzqEzpYv2zLu+4U/AoKvhIg51ato6\nY+sbXgQM+tStPefDjgpb3/Aj+NV2Kpv8xPUhcHjBg74Mcw22vuFF0CnHadxOaU5sfcOLkPe2\n0495hpo/QRM0TAVdh87HjPX8G1YIGqbYKYQoBA1RCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQN\nUQgaohA0RCFoROX6oUIEjYjcP/aNoBGR+wdzEjTi8fDoZIJGPAQNUQgasjCHhh+eHsm9fNiW\nVQ44576rHw7NOjRcc/9f/mgIGj5em0VD0CBo7wg6KIL2jaDDYg7tGUGHFXGVwzWCRifSOrR7\nBA1RCBqiEDREIeiDEzN5viPoQxO0vHFH0IcmaAH6jqCPTNIW4R1BHxlBG37J3fWc9Y9HzvKr\nr0PgFwRt+CW9JlF/eHj9JjCHNvuSXq70peo/qkvNgzc3gVUOsy/paVU9Pq54NPJGsA5tXNvT\n8+p5eD284AwNUcLOocu6/4g5NHwJuWyXTlY5ksbLIXB0Ydeh834dWmdn1qHhBzuFEIWgIQpb\n3xCFrW+IwtY3RGFjBaK2v7ez9a2mDA8BA7IuUOIMfXiyLiFl6/vohF3kz9b30RE0W9+iEDQ7\nhbIwh3Y/igiHwIhVDstDFlolhd9D4CesQ5upMqWL9szWN/wJGHTVl5yrU9PWmZo9RxM0DAUM\n+tStPefDjkqjEh+HwOEF3/pW2eQnrg8BCzJm0sGDvgxzDba+N0bKWkfQKcdp3B5sTmx9b4yU\n1eiQF/jrxzxDzZ+gCTo4MfuFQdeh8zFjPXt+JujwCNorgg6NoL0i6OCYQ/tE0MGxyuETQUfA\nOrQ/BA1DBA1RCBqiEDREIWiIQtAHJWNN4x1BH5KUVed3BH1IUvYF3xH0EX29cmP/ExGCPqIv\nQUuYiBD0EX0L+tO/3BmCPqSP6Yq4hJSgD+nj5IKgvSFo7z68/CNobwg6CubQvhB0FKxy+ELQ\nkbAO7QdBwxBBQxSChigEDVEIGqIQNEQh6C2Itlq2/2W6VwQdX7T9DAkbKa8IOr5oO84Strpf\nEXR00a4JEnEx0iuCjo6gXSLo6AjaJYKOjzm0QwQdH6scDhH0FrAO7UzQoK/nrH88cpZffR0C\nBxfysW6J+sPD6+FFwKBzpS9V/1Fdah68CS8CBq1V9fi44tHI8MI26CK5nXATlSxMivuvU99+\n4mJUQMcy6LILU3eT4uWiOUPDP8ugU3W5xZm0l4VXeZ3bHLqs+4+YQ8MXy6C7E3TVxTk/hRik\nk1WOpHE8KqDjIOhMleuCbq95vw6tszPr0PDDespRld10eM2Uw/AQwA/sXxQqde5O0KWzIbUE\nDWPWy3bDy7vksvrLi0SpbCF/goahgBsrwzT7/spwdpGDoGEqdNC5ypu2rXNV+DgEDi/0TqFW\n/XpdoxLHowI6AXcK+6DH5b33ZT419fuogE7AncI+09MYNFvf8CHgTqFS2bkoVbce0uRsfcOL\ngDuFk+mEUpqtb/gQcqewqooiy/qXhvlszwTtn7x3Ew7YKTwkie/3HoTfKfz5EHBP4h05BiHf\n9d2clErvZ3LesRKTyHsmDUK+61sP9zAYvglBR0TQ37+kzPqVjnr56/rt7qbQ/ctHgo6JoL9+\nSTosxCm9XLQevrDWSU3QkTGH/vIlhUqbrs1CnZa/7v6FTZoSdGSscnz5ku5ao6drNGYkalx8\nTlKCjo116I9f0k83Vgb9dxavVUrQ8MIy6OR+hq7mLwcd5I+Ky4UL6ggahtzMoUs9f8H+XZWN\nH9UngoYPtqsc2aq7iVodAljPyTq0ytzufBM0TIXc+t7UISATQUMUgoYotkGfEx/vayVoGLIM\n+uznjdoEDUPWW99r1p+tDgH8wMHWtwcEDUOWQWdq/t2uhggahiyDrnW64iZgVocAfmA95eBF\nIbaEoCEKGys7JfUCfVsEvUty30Jli6B3Se6bXG1Z3zmJre8IBN+GwBZb33tE0F+x9b1HBP0V\nW9+7xBz6G8ugc7a+o5hb5Xhd0DvWAp/1m2TZ+o7jW6avqR9tgc8iaPUs8qgweJ2MHG1yQtCy\nvL5cPNzLRzZWZCHoIF+ywUMIRdB2XzKZcqTzz6M3PgR+whza6kueZtGzD4f1PSoMWOWw+5KT\n7p4CVGp1bbP5p8MaHwK/YR3a4ktyVfX/rFTaNmtuqfv7IYAfuNr6ftz53AmCnuf8pCvnLG59\ncdJ4htYEHYrzabGkebb1lGOcQ+frnvftbVQH4nzhQtJKiO2LwvTvhufK3aWkBD3D+dKyqLVq\nRzc8707T3UPsHSHoGQQ9J+hO4fU8PMEiyxcu0SPoGQQ9J2DQTTLdV/RyiGNgDj0jYNC3F5CX\nYU2k7l9EejjEMbDKMSPgnZPGJb5ONb9PTtDzWIf+KmDQSn37iYtRAR03U45rmr3/y1ecoeGf\nozl083iO93fdJkzdf8QcGr64elG4Zts7nUxQktl3ixM0DDkKulh1MfQ179ehdXZmHRp+OHtR\n6G6bsCVoGHMUdOL2jmAEDUNsfccjZ/F3Q9j6jkXS9tyGWAd96dYussuKr2Pr+4mkCyg2xOH1\n0EvYWJkSdYnbhlgGXTzesbL8qnBh69vbfcW2iaD9sAw6ebyncPkd35yhpwjaD5fv+l7A1vcT\n5tBeODtDr9gpZOt7ilUOLwLOodn6fsE6tAcBVzlMDwGsZ78Ona1dhzY+BLBayK3v5nQ7k5f3\nb8I7VqwwXfnCMujshxuONnq4kGP4JgRtgReUX7latlsh7144NoVOl7+QoOex5PeV9bLd+ucU\n6uELa53UBG2FTZnvLINufnhO4dhwk6YEbYWgvwt4G4O/s3mSErQNgv4uYNDF453htUoJ2gZz\n6K9CLtvlj4rLhb8ABD2PVY6vgr4Fq3rcjqY+EbQV1qG/sAo6769IKhKlHT6j8PkQwE8sgu42\nSm7/GN73qtcv3/kZFdCxCDpX6a3ia3claJO6e0ah4aiAjkXQul+GO6nu6ozG4WNkW4KGMfOg\n1ZuoowI6tmfocphrcIbGNlgEfbq13AzvwWocPufbcFRAxyLoup9n9Lt/Suna4aCOHDQLzHZs\n1qGrdFyA1ienq3bHDZotQFtBdwq3dIht4iINWwS9JVxGZ80iaG+LdgRN0MYIekvuMw4m0eZs\npxxZf6OZq15+CJbxIY7kX/+S8B+vC41ZBp0/bgXGOrQLj5oJ2lDAmzUaHuJgxvkGRZuxDFr/\ncrNGs0McDK8L7VhPOXT3ru9S81g3R2aD5uXiIlc3a1zxrG/TQxzM9xkH24grOLpZY+loOB8P\ncSzfs2VyvQI7hdvzZWLB7HoNgt4Ngl7DQdCFdv1kZIL+hKDXsLp8NFO6aM/cwT+Qj3Pof/9Y\n+piyCLrqS87VqWnrbNUzVnyO6gA+vFzsaybpCcu3YHUL0d3HzYrnFBocAs/eyv3Xn7D/MQ15\nsLrarv9xzR35TQ+Bef8e0xCKvrMO+jLMNdj6joGg31hNOR7vJGxOXG0XA0G/sb+3Xf+v3J6g\nCXrtVRvMoV85uPvoDXcfdWv1VRuscrxip3CLfrhqg3XoZwS9QewJmiPoDSJoc66CZh3aIYI2\nR9BbxJXPxoJOOa7n4fkVWb7wtM7DB817U0wFDLpJJrelmb867+hB8+5BYwGDzpW+DO8Rr0s9\nv7NI0DDkKujr8rtkx1sedBZue0DQMGQbdL7+3nZPn8KTZOGF9X05Rsvv++YM7Q0z7gfrOydd\n2lTVdaoW1i3afg5dDg+uYA7tFGsiEw7ubXe+nZ2rNW8qTCerHMnsMywI+hesWk84CLrs3k+4\namPlmvfr0Do7sw7tDvuKU5ZBZ7cpR62S9spOYTQEPWUZdNmFnD4e7+YKQf+AoKdsl+3O3c9O\n6od3YBXJ8r3wCPoXzKEnAu4UDrOS+yvD+b8ABP0LVjkmQgedq7xp2zqfvzENQf+GdegHV4+k\n0Mvvku0/t3/i/eKNaQgahhwFXa/d+h4/7/3zvT0jDkdiEXT5lODyrcDU8Prx/hO2vuGDzRl6\nen1zsrz1rVR2Lkp1uX3Y5Jvd+mY6um+u5tBrvu5vOqGU3ubWNwsGexfyLVhVVRRZ1r80zGd7\njhj05MdII+Cvk5WAF/jbHiKA6Jtu/BfCVsAL/E0PEVD8oOMeXoCAF/gbHiKk2EHHPr4AAS/w\nNzxEUJHPkARtLeQF/maHCCryHJagrQW8wF89czwqV+KuMjCHthXwAv9iF0HHxSqHrZAX+Fd6\n7bzkSEH/e3tQW6yRiBD0Av9q7fsAjhM0p2THgt6s8TbrqJY/ye4QO8Ok2bGwQW/oENvAsoZr\nFkH/sGoRYFT7RNCuEXRUBO2a7ZQj092e91U7vYvBcYJmDu2a9bUcw6u81esXvx9CNlY5HHN1\ngT9TDlMsPDtlfXHSeIbm4fXYAusph+4usyu1Orsa0eshgB/Yvigcb5Hr9A0rBA1T1hsrl+4O\nuUv3qrM7BLAaO4XbwEtDRwh6C1i8c4agt4DtFWcI+ge+5gVsgLtD0Kv5mxcQtDsEvZq/eQFB\nu0PQa/msjjm0MwS9ltegWeVwhaDX8jsvYB3aEYJezdm8gHg9IujVHM0LmF54RdA/cHJqXXGi\n5xRujqADW56Kcwq3QdDLnJ4wVwS98OuYQ9BLHJ8wF4Nml8UKQS9xfcJc+n4EbYWgFzjva+mM\nT9BWCHqBh74W5uTMoW0Q9ILwJ0xWOWwQ9JIIJ0zWoc0R9BJOmLtC0Ms4Ye4IQUMUgoYoBA1R\nCBqiBA36es6GG+HlCw9SJmgYChh0k0weYDH/xEKChqGAQedKX4a7Sdelnr/jP0HDUMCg9eQh\nhQs3SCdoGAoY9NNTK3jWN7zgDA1Rws6hy7r/iDk0fAm5bJdOVjmSxsshcHRh16Hzfh1aZ2fW\noeEHO4UQhaAhClvfEIWtb4jC1jdEYWMFomxn61tNGR4Ch8cZGqKw9Q1R2PqGKGx9QxR2CiEK\nQUMUgu483ewr6J2/uM2YYwT9cjvGoPdm5EaQzhH0yw1zg949l3ubOxd0p3D1ZmDQoJ9uaR70\n/uY8fcK9gEEXBD13ZDgRcspR6fmLRh0cwsS3oP2/XiNo94LOoav5DW8XhzDxcQ4d5PUac2jn\nwr4oLCbXJ3k6hIGPqxxBWmOVwzlWOTrv69ChZgOsQztG0J8xvd0pgv6MoHeKoL/g9do+EfQX\nvF7bJ4L+itdre0TQEIWgIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoJexZbgjBP3irV4u6tgV\ngn7yoV4uu9sVgn7yXi8XRu8LQU99qJeg94Wgpwh69wh66lO9zKF3haCffKiXVY5dIegnH+tl\nHXpHCPrFvV4i3imC/oRpxm4R9Ce8ENwtgv6Apbr9IugPCHq/CPoDgt4vgv6EOfRuEfQnrHLs\nFkF/xjr0ThE0RCFoiCIyaOYLxyUwaF7RHZnEoCc/4mjkBc2uyKERNEQhaIiy86A/rWcwhz6y\noEFfz5nqZPnVySE+r2ewynFkAYNuEvUndXGIb+di1qGPK2DQudKX4dn1dalVbn8IZst4EzBo\nrarHx5XS9ocgaLwJGLRS335ieAiCxps9n6FZz8CbsHPosu4/cjSHZj0Db0Iu26WTVY6kcXII\n1jPwLOw6dN6vQ+vs7GYdGni1851C4Nl2glZTfg4B+cIHXSRKZaXXQ+C4gq9D318Zzi5yEDRM\nhQ46V3nTtnWuCh+HwOGFDlqrfr2uUYmPQ+DwQgc9vt5zsfUNvAkd9GkM2sXWN/AqaNDZuSjV\n5fZhkzvZ+gZeBQ36scaslHaz9Q08C7kOXVVFkWX9S8N8tuePh+CyDaywnZ3C+UNwYR1W2U3Q\nkx+Br3YSNG9OwToEDVEIGqLsJGjm0FhnN0GzyoE19hI069BYZT9BAysQNEQRETSzEYwEBM3r\nRfyREPTkRxzd3oJ+n12w54KJfQX9aXZB0JjYWdCTH5//HUGjt6ugP7fLHBp/JATNKgceBATN\nOjT+7CroFbML2j64nQW9MLtg9nF4+wp6egbmIbL4YG9Bjz6ei1nBw26Dnvz48i8J+sh2GvTc\nijRBH5mUoIcZNXPow5MR9DijZpXj8HYa9Mu5+O9nrEMf3G6Dnp6LmTtjtNegn87FBI3RfoOe\nIGiMRATN6gZGQoJmdQMDGUGzuoE7KUEDPYKGKAQNUQgaohA0RCFoiELQEIWgIQpBQxSChigE\nDVEIGqIQNETZaNCAIYPa3Acc6SBrbWowjGaG0WgIOi5G8x1Br7KpwTCaGQS9yqYGw2hmEPQq\nmxoMo5lB0KtsajCMZgZBr7KpwTCaGQS9yqYGw2hmEPQqmxoMo5lB0KtsajCMZgZBr7KpwTCa\nGQS9yqYGw2hmbDdoIBSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUMUgoYoBA1RCBqiEDREIWiI\nQtAQhaAhiv+gc6103ng/zKxi/L85GUyscRXJpyFEGk1zUupUtRsZTeeq7EbjPei0v4tk4vsw\ns6rxNpaTwcQaV94fVzfbGI3uj1u9DCHin1mjhz8q49H4DvqqdNVWWl09H2fO7fDqdTCxxlWp\nU9P9F+O0idHk3ThylbWbGE0nG/6ozEfjO+hclbcfL+rs+TgzCpXeg54MJta4smEk3YA2MBqt\nmvtgtjCa/qDDH5X5aHwHnam67c5LmefjzFB5ew96MpjI4+oGtJnRKN1uZDT1eO4xH43voJWa\n/iOK6nUU3T/ijqtR6XZGk6ui3choUlUPRzUfzQGCfhtF9IRus6ByK6O5/Uc+nxw76mjO6tIS\n9O+jiB50rbPNjKbIdD893cBo+mkFQf8+ithBNzrd0Gja9tTNOTYwmqRbzNx60HpLQU8GE3Nc\nabKl0XQzer2F0Zz65YzhqOajCbPKUcdc5WgfvxuTwcQbV52k9XZG0/tbc4k5munD3MxH4zvo\nc//XrhxeeERzD3oymGjjKlV6/2gDoxnWoetuGy7+aKZBm4/mCDuFj6A3sBtWP3rewmj6ncIm\n6+bQGxhNb+s7hW3S/51Llz/Rp3ECNhlMpHGdJs9IjT+a+7Ucr0OI+Wd2/6MyHo33oJv+Winf\nR1kwBj0ZTKRxTR/6G380/ZVsSfE6hJh/Zvc/KuPRxF19ABwjaIhC0BCFoCEKQUMUgoYoBA1R\nCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQNUQgaohA0RCFoiELQEIWgIQpBQxSChigEDVEIGqIQ\nNEQhaIhC0BCFoCEKQUMUgoYoBA1RCBqiEDREIWhvJg/ZG5XfP3PFN8MK/D558x508uV3m6Dd\n4ffJm/cGv1VJ0O7w++QNQcfA75ORW1/5/dFMSjVJ/5jTIlF6eKBU92ypfDLluP20e3rs+PSr\nj5/50KjhycmJatoyU4+jPD8G++9blKlS6ZfJ+QERtBGlzuPD85S6VZd3j/B9PE4v7T7KHg32\nP9XNI+iPn/knvT8LOG3PwxPg8vYt6L9vUQyfUwT8P79pBG1E3R9veuk+TLvnC5fdP5q0e47v\n5f6L9wYv3a+cuiyHIj9/5p+LOrfDE4FV9/0v/a8+Bz35FlpV3eckwX8LNoqgjaj7A6iz7sP+\nob1Z/9jsZnjc+rX/RTU+Fv7a/YIei/z8mdNv3tX5tyLyIejJtxhGghFBG7knOFmZmzwi9uUX\n1XOaXz5z4nSbc9TDw9rr8px+CHryLfLblKWq/P6/3ROCNuI16OttzpH35+708Rjlr0G35+55\n3br2+/93PwjayKegv/3ie9CfPnNKJ93/ulN1UpT1x6Cnn17mCXPoEUEbUffJ7+kRV/Y3lx0+\nvI7lpW9z6E+fOZWron9h2P/7l6Cvwxz6ZeLMMvWI3wgj4ypH+WipX7Boi+51Wvm8ylF0SxL5\nsMpRf/3MqVvD/au+7q9N9TeHTlTRLW2op2+RDCshnKHvCNqIUsMKcvt3chzmu/1ktl8lPr2t\nQ9/i687Tnz/zSTKsUuf3mfJ1/JvxWLT++xaXx6egQ9BGblVltwnu/cNBcev1NLw4O7/sFN46\n7H7hmvRBf/zMJ5f7lOKWenq9Lw4On3z62ym8f4t+p5CeRwRthDnrVvEHY4Sgt4o/GCPOg1Z/\nHH/ng+G3zwhBbxW/fRCFoCEKQUMUgoYoBA1RCBqiEDREIWiIQtAQhaAhCkFDFIKGKAQNUQga\nohA0RCFoiELQEIWgIQpBQxSChigEDVEIGqIQNEQhaIhC0BCFoCEKQUOU/3ycHlqP5j6mAAAA\nAElFTkSuQmCC",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1304210f5ef7502a9adc04b8076f6bdc",
          "grade": false,
          "grade_id": "cell-1ec0df52c6d700ba",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "PVcGtr2gj11y"
      },
      "source": [
        "<h2>Question 3.d (2 marks)</h2>\n",
        "\n",
        "The graph seems to indicate that our model does reasonably well, but we would like to have more mathematically-sound way of determining how close our model gets, on average.\n",
        "\n",
        "Write a function, \"mean.sq.error\", which takes two arguments (model and df) and returns the mean <b>squared</b> error of the predictions determined by model and df.\n",
        "\n",
        "Then write another function, \"mean.abs.error\", which takes two arguments (model and df) and returns the mean <b>absolute</b> error of the predictions determined by model and df.\n",
        "\n",
        "Your MSE function should look like this:\n",
        "\n",
        "    mean.sq.error <- function(model, df) {\n",
        "        # Your code here\n",
        "    }\n",
        "    \n",
        "Your MAE function should look like this:\n",
        "\n",
        "    mean.abs.error <- function(model, df) {\n",
        "        # Your code here\n",
        "    }\n",
        "\n",
        "You may assume that \"roughness\" is the name of the target variable for the dataframe passed into your function (i.e., you can hard-code this into the function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "0ee8fd157b8f985be49cbab3caeb286a",
          "grade": true,
          "grade_id": "cell-ebdbb185fef8840f",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "BoIP0h8Oj11y"
      },
      "source": [
        "mean.sq.error <- function(model, df) {\n",
        "    \n",
        "    predicted_values <- predict(model,df)\n",
        "    mse<-mean((data.3d$roughness - predicted_values)^2)\n",
        "    return(mse)\n",
        "}\n",
        "mean.abs.error <- function(model, df) {\n",
        "    \n",
        "    predicted_values <- predict(model,df)\n",
        "    mae<-mean(abs(df$roughness - predicted_values))\n",
        "    return(mae)                                                                        # Your code here\n",
        "}                                                                # Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5d9cd5866369f29ed3829d3aa83c2d35",
          "grade": false,
          "grade_id": "cell-1b42cddee087d259",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "7Ig_TPfUj11z",
        "outputId": "6e6de032-0aa5-4953-fe15-316705836d37"
      },
      "source": [
        "mean.sq.error(fit.lm, data.3d)\n",
        "mean.abs.error(fit.lm, data.3d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(model, df):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1199.35220414104"
            ],
            "text/latex": "1199.35220414104",
            "text/markdown": "1199.35220414104",
            "text/plain": [
              "[1] 1199.352"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(model, df):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "27.6021083464572"
            ],
            "text/latex": "27.6021083464572",
            "text/markdown": "27.6021083464572",
            "text/plain": [
              "[1] 27.60211"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5b7e054e11c78ba66804d73d06de285b",
          "grade": false,
          "grade_id": "cell-9c7e95f56b41ac55",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "BhXLIr9Zj11z"
      },
      "source": [
        "<h2>Question 3.e (1 mark)</h2>\n",
        "\n",
        "We need to narrow down what could be the cause for print roughness; we will use some variable selection to do this. Like in Question 2.d, we've given you the code for running step-wise variable selection, although (again, like in Question 2.d) you need to provide the values of k for the AIC (k = 2) and BIC (k = log(N) for N data points in our dataset). You need to call this function in the second code cell below, two times. Each time will use a different value of k:\n",
        "\n",
        " - fit.lm.aic: uses k = 2\n",
        " - fit.lm.bic: uses k = log(N) where your dataset has N rows\n",
        "\n",
        "You should then run the third code cell below, which will call your mean.sq.error() and mean.abs.error() functions on fit.lm.aic and fit.lm.bic. Take a look at how the AIC and BIC models do in comparison to the three models we came up with above (although you don't have to discuss this in your answer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a42e44d0a13e1e1e300c1acf68b8ec83",
          "grade": false,
          "grade_id": "cell-0d513ddd47690e24",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "skmDnj3jj11z"
      },
      "source": [
        "to.stepwise <- function(df, k) {\n",
        "    fit <- lm(roughness ~ ., data=df)\n",
        "    return (step(fit, k=k))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8ce8687f2833e937b40ea6c2ee341a7a",
          "grade": true,
          "grade_id": "cell-71b7df979d7b7b8c",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "KjQRUKKCj110",
        "outputId": "29043702-c122-41d5-9ca5-9c23fbd08d1c"
      },
      "source": [
        "N<-nrow(data.3d)\n",
        "fit.lm.aic<- to.stepwise(data.3d,2)\n",
        "fit.lm.bic<- to.stepwise(data.3d,log(N)) \n",
        "N# your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start:  AIC=372.48\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    infill_pattern + nozzle_temperature + bed_temperature + print_speed + \n",
            "    material + fan_speed\n",
            "\n",
            "\n",
            "Step:  AIC=372.48\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    infill_pattern + nozzle_temperature + bed_temperature + print_speed + \n",
            "    material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- infill_pattern      1         0  59968 370.48\n",
            "- infill_density      1        48  60015 370.52\n",
            "- wall_thickness      1      1663  61630 371.84\n",
            "<none>                             59968 372.48\n",
            "- print_speed         1     14538  74505 381.33\n",
            "- bed_temperature     1     36006  95973 393.99\n",
            "- material            1     38246  98213 395.14\n",
            "- nozzle_temperature  1     51832 111800 401.62\n",
            "- layer_height        1    306817 366785 461.03\n",
            "\n",
            "Step:  AIC=370.48\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    nozzle_temperature + bed_temperature + print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- infill_density      1        48  60016 368.52\n",
            "- wall_thickness      1      1697  61665 369.87\n",
            "<none>                             59968 370.48\n",
            "- print_speed         1     14580  74548 379.36\n",
            "- bed_temperature     1     36115  96083 392.05\n",
            "- material            1     38274  98241 393.16\n",
            "- nozzle_temperature  1     51990 111957 399.69\n",
            "- layer_height        1    307226 367193 459.08\n",
            "\n",
            "Step:  AIC=368.52\n",
            "roughness ~ layer_height + wall_thickness + nozzle_temperature + \n",
            "    bed_temperature + print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- wall_thickness      1      1651  61667 367.87\n",
            "<none>                             60016 368.52\n",
            "- print_speed         1     14651  74667 377.44\n",
            "- bed_temperature     1     37456  97472 390.76\n",
            "- material            1     39029  99045 391.57\n",
            "- nozzle_temperature  1     54242 114258 398.71\n",
            "- layer_height        1    307270 367285 457.09\n",
            "\n",
            "Step:  AIC=367.87\n",
            "roughness ~ layer_height + nozzle_temperature + bed_temperature + \n",
            "    print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "<none>                             61667 367.87\n",
            "- print_speed         1     13210  74877 375.58\n",
            "- bed_temperature     1     36693  98360 389.22\n",
            "- material            1     38454 100121 390.11\n",
            "- nozzle_temperature  1     53228 114895 396.99\n",
            "- layer_height        1    314770 376437 456.32\n",
            "Start:  AIC=389.69\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    infill_pattern + nozzle_temperature + bed_temperature + print_speed + \n",
            "    material + fan_speed\n",
            "\n",
            "\n",
            "Step:  AIC=389.69\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    infill_pattern + nozzle_temperature + bed_temperature + print_speed + \n",
            "    material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- infill_pattern      1         0  59968 385.77\n",
            "- infill_density      1        48  60015 385.81\n",
            "- wall_thickness      1      1663  61630 387.14\n",
            "<none>                             59968 389.69\n",
            "- print_speed         1     14538  74505 396.63\n",
            "- bed_temperature     1     36006  95973 409.29\n",
            "- material            1     38246  98213 410.44\n",
            "- nozzle_temperature  1     51832 111800 416.92\n",
            "- layer_height        1    306817 366785 476.32\n",
            "\n",
            "Step:  AIC=385.77\n",
            "roughness ~ layer_height + wall_thickness + infill_density + \n",
            "    nozzle_temperature + bed_temperature + print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- infill_density      1        48  60016 381.90\n",
            "- wall_thickness      1      1697  61665 383.26\n",
            "<none>                             59968 385.77\n",
            "- print_speed         1     14580  74548 392.74\n",
            "- bed_temperature     1     36115  96083 405.43\n",
            "- material            1     38274  98241 406.54\n",
            "- nozzle_temperature  1     51990 111957 413.08\n",
            "- layer_height        1    307226 367193 472.47\n",
            "\n",
            "Step:  AIC=381.9\n",
            "roughness ~ layer_height + wall_thickness + nozzle_temperature + \n",
            "    bed_temperature + print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "- wall_thickness      1      1651  61667 379.35\n",
            "<none>                             60016 381.90\n",
            "- print_speed         1     14651  74667 388.91\n",
            "- bed_temperature     1     37456  97472 402.24\n",
            "- material            1     39029  99045 403.04\n",
            "- nozzle_temperature  1     54242 114258 410.18\n",
            "- layer_height        1    307270 367285 468.57\n",
            "\n",
            "Step:  AIC=379.35\n",
            "roughness ~ layer_height + nozzle_temperature + bed_temperature + \n",
            "    print_speed + material\n",
            "\n",
            "                     Df Sum of Sq    RSS    AIC\n",
            "<none>                             61667 379.35\n",
            "- print_speed         1     13210  74877 385.14\n",
            "- bed_temperature     1     36693  98360 398.78\n",
            "- material            1     38454 100121 399.67\n",
            "- nozzle_temperature  1     53228 114895 406.55\n",
            "- layer_height        1    314770 376437 465.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "50"
            ],
            "text/latex": "50",
            "text/markdown": "50",
            "text/plain": [
              "[1] 50"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f06012fb619e564966ef85199af510d0",
          "grade": false,
          "grade_id": "cell-caca7a76f9a85c09",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "aBWBYjWTj110",
        "outputId": "1096d28b-15be-4e31-f234-10e6f93a1505"
      },
      "source": [
        "mean.sq.error(fit.lm, data.3d)\n",
        "mean.sq.error(fit.lm.aic, data.3d)\n",
        "mean.sq.error(fit.lm.bic, data.3d)\n",
        "\n",
        "mean.abs.error(fit.lm, data.3d)\n",
        "mean.abs.error(fit.lm.aic, data.3d)\n",
        "mean.abs.error(fit.lm.bic, data.3d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(model, df):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1199.35220414104"
            ],
            "text/latex": "1199.35220414104",
            "text/markdown": "1199.35220414104",
            "text/plain": [
              "[1] 1199.352"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1233.34041020578"
            ],
            "text/latex": "1233.34041020578",
            "text/markdown": "1233.34041020578",
            "text/plain": [
              "[1] 1233.34"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1233.34041020578"
            ],
            "text/latex": "1233.34041020578",
            "text/markdown": "1233.34041020578",
            "text/plain": [
              "[1] 1233.34"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(model, df):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "27.6021083464572"
            ],
            "text/latex": "27.6021083464572",
            "text/markdown": "27.6021083464572",
            "text/plain": [
              "[1] 27.60211"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "27.7549872191008"
            ],
            "text/latex": "27.7549872191008",
            "text/markdown": "27.7549872191008",
            "text/plain": [
              "[1] 27.75499"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "27.7549872191008"
            ],
            "text/latex": "27.7549872191008",
            "text/markdown": "27.7549872191008",
            "text/plain": [
              "[1] 27.75499"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "8190e753dc9f828b827c30ffc47160c1",
          "grade": false,
          "grade_id": "cell-a66278ba55f236bb",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "-CQ5vOHhj111"
      },
      "source": [
        "<h2>Question 3.f (4 marks)</h2>\n",
        "\n",
        "Both the AIC and BIC models identify the same formula, and therefore remove the same variables:\n",
        "\n",
        "    roughness ~ layer_height + nozzle_temperature + bed_temperature + print_speed + material\n",
        "   \n",
        "   \n",
        "Use the summary() function on \"fit.lm\", \"fit.lm.aic\", and \"fit.lm.bic\" in the cell block below.\n",
        "\n",
        "In the markdown cell below the code cell, answer the following three questions:\n",
        "\n",
        "1. Do the variables removed by AIC and BIC all have large p-values?\n",
        "2. What does it means to have a large p-value?\n",
        "3. Why it is reasonable that your answer to the first of these three questions (\"Do the variables removed by AIC and BIC all have large p-values?\") would be what we expect to happen? You can answer this using intuition; you don't need to mathematically prove it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "f0b2ae276ee1953d97099e63ce18efd0",
          "grade": true,
          "grade_id": "cell-c4c10b9369fa213f",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "B85PjlsJj111",
        "outputId": "62597ef3-c8db-43dd-e22d-3ee52cfbf470"
      },
      "source": [
        "summary(fit.lm)   \n",
        "summary(fit.lm.aic)   \n",
        "summary(fit.lm.bic)   \n",
        "                                # your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = roughness ~ ., data = data.3d)\n",
              "\n",
              "Residuals:\n",
              "    Min      1Q  Median      3Q     Max \n",
              "-72.746 -24.332  -1.641  20.304  96.552 \n",
              "\n",
              "Coefficients: (1 not defined because of singularities)\n",
              "                          Estimate Std. Error t value Pr(>|t|)    \n",
              "(Intercept)             -2.371e+03  3.716e+02  -6.379 1.25e-07 ***\n",
              "layer_height             1.269e+03  8.765e+01  14.483  < 2e-16 ***\n",
              "wall_thickness           2.334e+00  2.189e+00   1.066  0.29259    \n",
              "infill_density          -4.231e-02  2.341e-01  -0.181  0.85742    \n",
              "infill_patternhoneycomb -1.255e-01  1.128e+01  -0.011  0.99117    \n",
              "nozzle_temperature       1.506e+01  2.529e+00   5.953 5.05e-07 ***\n",
              "bed_temperature         -1.613e+01  3.251e+00  -4.962 1.27e-05 ***\n",
              "print_speed              6.496e-01  2.060e-01   3.153  0.00302 ** \n",
              "materialpla              2.985e+02  5.836e+01   5.114 7.78e-06 ***\n",
              "fan_speed                       NA         NA      NA       NA    \n",
              "---\n",
              "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
              "\n",
              "Residual standard error: 38.24 on 41 degrees of freedom\n",
              "Multiple R-squared:  0.8752,\tAdjusted R-squared:  0.8509 \n",
              "F-statistic: 35.95 on 8 and 41 DF,  p-value: 3.834e-16\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = roughness ~ layer_height + nozzle_temperature + \n",
              "    bed_temperature + print_speed + material, data = df)\n",
              "\n",
              "Residuals:\n",
              "    Min      1Q  Median      3Q     Max \n",
              "-74.084 -26.500  -1.662  22.585  92.356 \n",
              "\n",
              "Coefficients:\n",
              "                     Estimate Std. Error t value Pr(>|t|)    \n",
              "(Intercept)        -2310.7356   353.2009  -6.542 5.38e-08 ***\n",
              "layer_height        1246.5353    83.1780  14.986  < 2e-16 ***\n",
              "nozzle_temperature    14.7774     2.3979   6.163 1.95e-07 ***\n",
              "bed_temperature      -15.8078     3.0895  -5.117 6.55e-06 ***\n",
              "print_speed            0.5538     0.1804   3.070  0.00366 ** \n",
              "materialpla          294.1610    56.1586   5.238 4.38e-06 ***\n",
              "---\n",
              "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
              "\n",
              "Residual standard error: 37.44 on 44 degrees of freedom\n",
              "Multiple R-squared:  0.8717,\tAdjusted R-squared:  0.8571 \n",
              "F-statistic: 59.78 on 5 and 44 DF,  p-value: < 2.2e-16\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = roughness ~ layer_height + nozzle_temperature + \n",
              "    bed_temperature + print_speed + material, data = df)\n",
              "\n",
              "Residuals:\n",
              "    Min      1Q  Median      3Q     Max \n",
              "-74.084 -26.500  -1.662  22.585  92.356 \n",
              "\n",
              "Coefficients:\n",
              "                     Estimate Std. Error t value Pr(>|t|)    \n",
              "(Intercept)        -2310.7356   353.2009  -6.542 5.38e-08 ***\n",
              "layer_height        1246.5353    83.1780  14.986  < 2e-16 ***\n",
              "nozzle_temperature    14.7774     2.3979   6.163 1.95e-07 ***\n",
              "bed_temperature      -15.8078     3.0895  -5.117 6.55e-06 ***\n",
              "print_speed            0.5538     0.1804   3.070  0.00366 ** \n",
              "materialpla          294.1610    56.1586   5.238 4.38e-06 ***\n",
              "---\n",
              "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
              "\n",
              "Residual standard error: 37.44 on 44 degrees of freedom\n",
              "Multiple R-squared:  0.8717,\tAdjusted R-squared:  0.8571 \n",
              "F-statistic: 59.78 on 5 and 44 DF,  p-value: < 2.2e-16\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "40317d43bec03a163afc2f98044f809a",
          "grade": false,
          "grade_id": "cell-2f9136fa48bc24db",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "uT4SCS_kj112"
      },
      "source": [
        "<h2>Question 3.g (3 marks)</h2>\n",
        "\n",
        "It is a problem that we are training and evaluating on the same dataset (in Question 2.g, we asked you to discuss why this is the case).\n",
        "\n",
        "One way to mitigate this problem is by using \"leave-one-out cross validation\", or LOOCV (https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation). In LOOCV, for a dataset of size $N$ we train $N$ different models, where each model trains on all but one of the points in our dataset. We then evaluate the squared error from each model predicting the point we left out (hence the name \"leave-one-out\" CV), and average the final result.\n",
        "\n",
        "In the code cell below, implement a function \"cv.loo\", which takes as argument a dataset \"df\" and returns the average <b>squared prediction error</b> from LOOCV done on that dataset. You may assume that \"roughness\" is the name of the target variable for the dataframe passed into your function (i.e., you can hard-code this into the function).\n",
        "\n",
        "Your function should look something like this:\n",
        "\n",
        "    cv.loo <- function (df) {\n",
        "        # Your code here\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "56210c285c8032ed2a29bc38b733fb67",
          "grade": true,
          "grade_id": "cell-3ab83153d7e46ee1",
          "locked": false,
          "points": 3,
          "schema_version": 1,
          "solution": true
        },
        "id": "9IKGq-F_j112"
      },
      "source": [
        "cv.loo <- function (df){\n",
        "    sq_predct_error_sum<-0\n",
        "    N<-nrow(df)\n",
        "    for( i in 1:N)\n",
        "    {\n",
        "    train_data<-data.3d[-i,]                     # drop the ith column for training data \n",
        "    test_data<-data.3d[i,]                      # consider the dropped row only for traaining set\n",
        "    MODEL<-lm(roughness ~ ., data=train_data)\n",
        "    \n",
        "    predicted_values <- predict(MODEL,test_data)               # predict \n",
        "    sq_pred_er<-(test_data$roughness-predicted_values)^2         # find mse\n",
        "    sq_predct_error_sum<-sq_predct_error_sum + sq_pred_er\n",
        "    }\n",
        "    sq_predct_error_sum<-sq_predct_error_sum/N\n",
        "    return(sq_predct_error_sum)\n",
        "}   \n",
        "                               # your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig7F6RZ9j112"
      },
      "source": [
        "train_data<-data.3d[-1,]\n",
        "test_data<-data.3d[1,]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b2dcc48f92effc7efd470aec2b7cc253",
          "grade": false,
          "grade_id": "cell-051d3d3229acacdf",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Aj0mW83Bj113",
        "outputId": "5c01050b-dede-414b-86a7-8f8615d69207"
      },
      "source": [
        "cv.loo(data.3d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(MODEL, test_data):\n",
            "\"prediction from a rank-deficient fit may be misleading\""
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<strong>1:</strong> 1831.22985575415"
            ],
            "text/latex": "\\textbf{1:} 1831.22985575415",
            "text/markdown": "**1:** 1831.22985575415",
            "text/plain": [
              "      1 \n",
              "1831.23 "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "65253de756da03b8e29ccd9851d85f65",
          "grade": false,
          "grade_id": "cell-a35492d69579aba7",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "FokzWSI9j113"
      },
      "source": [
        "<h2>Question 3.h (2 marks)</h2>\n",
        "\n",
        "The LOOCV error calculated in Question 3.g, if calculated correctly, was significantly higher than the training MSE. Why might this be the case? Describe one approach we might take to reduce this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "e078c87bb4fb6fd1fbd0d00c1856c490",
          "grade": true,
          "grade_id": "cell-00a8f5a5d477d504",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "8RLxVCQsj113"
      },
      "source": [
        "This is  because the training sets in LOOCV have more overlap. This makes the estimates from different folds more dependent THEREFORE MORE VARIANCE \n",
        "\n",
        "Kfold  cross validation can be a better approach\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "e078c87bb4fb6fd1fbd0d00c1856c490",
          "grade": true,
          "grade_id": "cell-00a8f5a5d477d504",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "0YMpsReJ5Oo9"
      },
      "source": [
        "This is  because the training sets in LOOCV have more overlap. This makes the estimates from different folds more dependent THEREFORE MORE VARIANCE \n",
        "\n",
        "Kfold  cross validation can be a better approach\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f3329a00dac8a49f8caa629c8bed81bb",
          "grade": false,
          "grade_id": "cell-ef81d0cf8e72be00",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "yAq8qc01j114"
      },
      "source": [
        "<h2>Question 3.i (1 mark)</h2>\n",
        "In the code above, we use the LOOCV technique to help solve the problem (discussed in Question 2.g) where we evaluate on the trianing set. However, aside from validation techniques like this, it is also common to set aside another <i>testing</i> dataset, which we use at the very end of training our model as a final evaluation. For simplicity, we did not do this here (although perhaps we should have). Why do you think this is considered good practice? You might like to do some research on this (just be careful not to accidentally plaigarise your answer from another source)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "f8efb376e8783dac80fb64c9270d1cd4",
          "grade": true,
          "grade_id": "cell-d8536880c49044d1",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "KUZqUBWIj114"
      },
      "source": [
        "It can a generate a better , more generalised model  without undefitting or overfitting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "44ecdce1807800a14b9f76659b38b158",
          "grade": false,
          "grade_id": "cell-4259cda3f616350e",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "WxP6ze0mj114"
      },
      "source": [
        "<h2>Question 3.j (4 marks)</h2>\n",
        "\n",
        "The engineers are eagerly awaiting your analysis of their situation. Summarize your findings from the previous parts of this question, being sure to address the following points:\n",
        "\n",
        "1. Which variables do not appear to be related to roughness?\n",
        "2. Which variables do appear to be related to roughness?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "29539295941278a5fda59c47c2c36c54",
          "grade": true,
          "grade_id": "cell-fd8ec5c606a4017e",
          "locked": false,
          "points": 4,
          "schema_version": 1,
          "solution": true
        },
        "id": "ogbkiJLsj115"
      },
      "source": [
        "1)The variables influencing the roughness are infill_pattern and material \n",
        "where they show some variation for different infill patterns and materials\n",
        "\n",
        "2)aThe factors such as layer_height,wall_thickness,infill_density , nozzle_temperature\tbed_temperature\t,print_speed ,fan_speed\n",
        "do not affect the roughness \n",
        "\n"
      ]
    }
  ]
}